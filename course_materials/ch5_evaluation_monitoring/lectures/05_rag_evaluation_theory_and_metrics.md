# RAG è©•ä¼°ç†è«–èˆ‡æŒ‡æ¨™é«”ç³»
## å¤§å­¸æ•™ç§‘æ›¸ ç¬¬5ç« ï¼šæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±çš„ç§‘å­¸è©•ä¼°

**èª²ç¨‹ç·¨è™Ÿ**: CS785 - ä¼æ¥­ç´šæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±
**ç« ç¯€**: ç¬¬5ç«  è©•ä¼°èˆ‡ç›£æ§
**å­¸ç¿’æ™‚æ•¸**: 6å°æ™‚
**å…ˆä¿®èª²ç¨‹**: çµ±è¨ˆå­¸åŸºç¤, æ©Ÿå™¨å­¸ç¿’è©•ä¼°, ç¬¬0-4ç« 
**ä½œè€…**: MLè©•ä¼°ç ”ç©¶åœ˜éšŠ & RAGASé–‹ç™¼åœ˜éšŠåˆä½œ
**æœ€å¾Œæ›´æ–°**: 2025-01-06

---

## ğŸ“š å­¸ç¿’ç›®æ¨™ (Learning Objectives)

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œå­¸ç”Ÿæ‡‰èƒ½å¤ :

1. **è©•ä¼°ç†è«–**: æŒæ¡ RAG ç³»çµ±è©•ä¼°çš„ç†è«–æ¡†æ¶å’Œæ•¸å­¸åŸºç¤
2. **æŒ‡æ¨™é«”ç³»**: ç†è§£ä¸¦æ‡‰ç”¨ RAGAS è©•ä¼°æ¡†æ¶çš„æ ¸å¿ƒæŒ‡æ¨™
3. **å¯¦é©—è¨­è¨ˆ**: è¨­è¨ˆç§‘å­¸çš„ RAG ç³»çµ±æ€§èƒ½è©•ä¼°å¯¦é©—
4. **ç›£æ§ç³»çµ±**: å»ºç«‹ç”Ÿç”¢ç’°å¢ƒçš„æŒçºŒç›£æ§å’Œå“è³ªä¿è­‰æ©Ÿåˆ¶

---

## 1. RAG è©•ä¼°çš„ç†è«–æ¡†æ¶

### 1.1 è©•ä¼°è¤‡é›œæ€§çš„æ ¹æºåˆ†æ

#### **å¤šç¶­åº¦è©•ä¼°æŒ‘æˆ°**

RAG ç³»çµ±çš„è©•ä¼°è¤‡é›œæ€§æºæ–¼å…¶**å¤šéšæ®µæµæ°´ç·šç‰¹æ€§**ï¼Œæ¯å€‹éšæ®µéƒ½éœ€è¦ç¨ç‰¹çš„è©•ä¼°æ–¹æ³•ï¼š

**å®šç† 1.1** (RAG è©•ä¼°çš„ä¸å¯åˆ†è§£æ€§): RAG ç³»çµ±çš„æ•´é«”æ€§èƒ½ä¸ç­‰æ–¼å„çµ„ä»¶æ€§èƒ½çš„ç°¡å–®åŠ æ¬Šå’Œï¼Œå­˜åœ¨é¡¯è‘—çš„**äº¤äº’æ•ˆæ‡‰**ï¼š

$$\text{Performance}_{RAG} \neq \sum_{i} w_i \cdot \text{Performance}_i$$

è€Œæ˜¯ï¼š
$$\text{Performance}_{RAG} = f(\text{Retrieval}, \text{Augmentation}, \text{Generation}) + \sum_{i<j} \text{Interaction}_{ij}$$

å…¶ä¸­ $\text{Interaction}_{ij}$ è¡¨ç¤ºçµ„ä»¶é–“çš„äº¤äº’æ•ˆæ‡‰ã€‚

**è­‰æ˜æ€è·¯**: æª¢ç´¢éŒ¯èª¤å¯èƒ½è¢«ç”Ÿæˆæ¨¡å‹çš„å…ˆé©—çŸ¥è­˜è£œå„Ÿï¼Œè€Œæª¢ç´¢å™ªéŸ³å¯èƒ½è¢«ä¸Šä¸‹æ–‡å·¥ç¨‹æŠ€è¡“æ¶ˆé™¤ï¼Œé€™äº›äº¤äº’æ•ˆæ‡‰ä½¿å¾—åˆ†è§£è©•ä¼°ä¸è¶³ä»¥é æ¸¬æ•´é«”æ€§èƒ½ã€‚â–¡

#### **è©•ä¼°ç¶­åº¦çš„æ•¸å­¸å»ºæ¨¡**

åŸºæ–¼ Es et al. (2023)[^18] çš„ RAGAS æ¡†æ¶ï¼ŒRAG ç³»çµ±è©•ä¼°åŒ…å«ä»¥ä¸‹æ ¸å¿ƒç¶­åº¦ï¼š

**ç¶­åº¦ 1.1** (å¿ å¯¦åº¦ Faithfulness): ç”Ÿæˆç­”æ¡ˆèˆ‡æª¢ç´¢ä¸Šä¸‹æ–‡çš„äº‹å¯¦ä¸€è‡´æ€§

$$\text{Faithfulness} = \frac{|\text{æ”¯æŒçš„é™³è¿°}|}{|\text{ç¸½é™³è¿°}|}$$

**ç¶­åº¦ 1.2** (ç­”æ¡ˆç›¸é—œæ€§ Answer Relevancy): ç”Ÿæˆç­”æ¡ˆå°åŸå§‹å•é¡Œçš„ç›¸é—œç¨‹åº¦

$$\text{Answer Relevancy} = \frac{1}{|Q|} \sum_{q_i \in Q} \text{Similarity}(q, q_i)$$

å…¶ä¸­ $Q = \{q_1, q_2, ..., q_n\}$ æ˜¯åŸºæ–¼ç­”æ¡ˆç”Ÿæˆçš„å•é¡Œé›†åˆã€‚

**ç¶­åº¦ 1.3** (ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦ Context Precision): æª¢ç´¢ä¸Šä¸‹æ–‡ä¸­ç›¸é—œä¿¡æ¯çš„æ¯”ä¾‹

$$\text{Context Precision} = \frac{|\text{ç›¸é—œä¸Šä¸‹æ–‡}|}{|\text{ç¸½æª¢ç´¢ä¸Šä¸‹æ–‡}|}$$

**ç¶­åº¦ 1.4** (ä¸Šä¸‹æ–‡å¬å›ç‡ Context Recall): å›ç­”å•é¡Œæ‰€éœ€ä¿¡æ¯åœ¨æª¢ç´¢ä¸Šä¸‹æ–‡ä¸­çš„è¦†è“‹ç‡

$$\text{Context Recall} = \frac{|\text{æª¢ç´¢åˆ°çš„å¿…éœ€ä¿¡æ¯}|}{|\text{å›ç­”æ‰€éœ€çš„ç¸½ä¿¡æ¯}|}$$

### 1.2 è©•ä¼°æŒ‡æ¨™çš„ä¿¡æ¯è«–åˆ†æ

#### **ä¿¡æ¯ç†µè¦–è§’çš„è©•ä¼°**

**å®šç¾© 1.1** (è©•ä¼°ä¿¡æ¯ç†µ): RAG ç³»çµ±è©•ä¼°çš„ä¿¡æ¯ç†µå®šç¾©ç‚ºï¼š

$$H_{eval} = -\sum_{m \in M} P(m) \log P(m)$$

å…¶ä¸­ $M$ ç‚ºè©•ä¼°æŒ‡æ¨™é›†åˆï¼Œ$P(m)$ ç‚ºæŒ‡æ¨™ $m$ çš„é‡è¦æ€§æ¬Šé‡ã€‚

**æ¨è«– 1.1** (æœ€å¤§ç†µè©•ä¼°åŸç†): åœ¨æ²’æœ‰å…ˆé©—çŸ¥è­˜çš„æƒ…æ³ä¸‹ï¼Œæ‡‰é¸æ“‡ä½¿è©•ä¼°ä¿¡æ¯ç†µæœ€å¤§çš„æŒ‡æ¨™çµ„åˆï¼Œä»¥ç²å¾—æœ€å…¨é¢çš„æ€§èƒ½è©•ä¼°ã€‚

---

## 2. RAGAS è©•ä¼°æ¡†æ¶æ·±åº¦è§£æ

### 2.1 å¿ å¯¦åº¦ (Faithfulness) çš„è¨ˆç®—ç†è«–

#### **é™³è¿°åˆ†è§£èˆ‡äº‹å¯¦é©—è­‰**

**ç®—æ³• 2.1** (åŸºæ–¼ LLM çš„é™³è¿°åˆ†è§£):

```python
from typing import List, Dict, Any
import asyncio
from dataclasses import dataclass

@dataclass
class Statement:
    """äº‹å¯¦é™³è¿°æ•¸æ“šçµæ§‹"""
    text: str
    statement_id: str
    confidence: float
    source_span: Optional[Tuple[int, int]]  # åœ¨åŸæ–‡ä¸­çš„ä½ç½®

class FaithfulnessEvaluator:
    """å¿ å¯¦åº¦è©•ä¼°å™¨"""

    def __init__(self, llm_evaluator: Any):
        self.llm_evaluator = llm_evaluator

    async def calculate_faithfulness(self, answer: str,
                                   contexts: List[str]) -> Dict[str, Any]:
        """
        è¨ˆç®—å¿ å¯¦åº¦åˆ†æ•¸

        åŸºæ–¼ Es et al. (2023) RAGAS æ¡†æ¶å¯¦ç¾
        """

        # æ­¥é©Ÿ1: åˆ†è§£ç­”æ¡ˆç‚ºåŸå­é™³è¿°
        statements = await self._decompose_into_statements(answer)

        if not statements:
            return {"faithfulness": 0.0, "details": "No statements found"}

        # æ­¥é©Ÿ2: é©—è­‰æ¯å€‹é™³è¿°
        verification_results = []
        for statement in statements:
            verification = await self._verify_statement(statement, contexts)
            verification_results.append(verification)

        # æ­¥é©Ÿ3: è¨ˆç®—å¿ å¯¦åº¦åˆ†æ•¸
        supported_count = sum(1 for v in verification_results if v["supported"])
        faithfulness_score = supported_count / len(statements)

        return {
            "faithfulness": faithfulness_score,
            "total_statements": len(statements),
            "supported_statements": supported_count,
            "statement_details": verification_results
        }

    async def _decompose_into_statements(self, answer: str) -> List[Statement]:
        """å°‡ç­”æ¡ˆåˆ†è§£ç‚ºåŸå­é™³è¿°"""

        prompt = f"""
        è«‹å°‡ä»¥ä¸‹ç­”æ¡ˆåˆ†è§£ç‚ºç¨ç«‹çš„äº‹å¯¦é™³è¿°ï¼Œæ¯å€‹é™³è¿°æ‡‰è©²æ˜¯ä¸€å€‹å¯ä»¥ç¨ç«‹é©—è­‰çš„åŸå­äº‹å¯¦ã€‚

        ç­”æ¡ˆ: {answer}

        è«‹ä»¥ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
        1. [é™³è¿°1]
        2. [é™³è¿°2]
        ...

        åˆ†è§£çµæœ:
        """

        response = await self.llm_evaluator.generate(prompt, temperature=0.1)

        # è§£æé™³è¿°
        statements = []
        lines = response.strip().split('\n')

        for i, line in enumerate(lines):
            line = line.strip()
            if line and re.match(r'^\d+\.', line):
                statement_text = re.sub(r'^\d+\.\s*', '', line)
                if statement_text:
                    statement = Statement(
                        text=statement_text,
                        statement_id=f"stmt_{i}",
                        confidence=1.0,  # åˆå§‹ç½®ä¿¡åº¦
                        source_span=None
                    )
                    statements.append(statement)

        return statements

    async def _verify_statement(self, statement: Statement,
                               contexts: List[str]) -> Dict[str, Any]:
        """é©—è­‰é™³è¿°æ˜¯å¦è¢«ä¸Šä¸‹æ–‡æ”¯æŒ"""

        # å°‡æ‰€æœ‰ä¸Šä¸‹æ–‡åˆä½µ
        combined_context = "\n\n".join(contexts)

        # æ§‹å»ºé©—è­‰æç¤º
        prompt = f"""
        è«‹åˆ¤æ–·ä»¥ä¸‹é™³è¿°æ˜¯å¦è¢«çµ¦å®šçš„ä¸Šä¸‹æ–‡æ”¯æŒã€‚

        é™³è¿°: {statement.text}

        ä¸Šä¸‹æ–‡:
        {combined_context}

        è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼š
        1. è©²é™³è¿°æ˜¯å¦è¢«ä¸Šä¸‹æ–‡æ˜ç¢ºæ”¯æŒï¼Ÿ (æ˜¯/å¦)
        2. æ”¯æŒè©²é™³è¿°çš„å…·é«”è­‰æ“šæ˜¯ä»€éº¼ï¼Ÿ
        3. æ”¯æŒçš„ä¿¡å¿ƒç¨‹åº¦å¦‚ä½•ï¼Ÿ (0-1åˆ†)

        è«‹ä»¥JSONæ ¼å¼å›ç­”ï¼š
        {{
            "supported": true/false,
            "evidence": "æ”¯æŒè­‰æ“šçš„æ–‡æœ¬",
            "confidence": 0.95
        }}
        """

        response = await self.llm_evaluator.generate(prompt, temperature=0.0)

        try:
            verification_result = self._parse_json_response(response)
            verification_result["statement_text"] = statement.text
            return verification_result
        except Exception as e:
            return {
                "supported": False,
                "evidence": "",
                "confidence": 0.0,
                "error": str(e),
                "statement_text": statement.text
            }

    def _parse_json_response(self, response: str) -> Dict:
        """è§£æ JSON æ ¼å¼çš„å›æ‡‰"""
        import json

        # å˜—è©¦æå– JSON éƒ¨åˆ†
        json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
        if json_match:
            json_text = json_match.group()
            try:
                return json.loads(json_text)
            except json.JSONDecodeError:
                pass

        # å¦‚æœ JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡æå–
        supported = "true" in response.lower() or "æ˜¯" in response
        confidence_match = re.search(r'(\d+\.?\d*)', response)
        confidence = float(confidence_match.group()) if confidence_match else 0.5

        return {
            "supported": supported,
            "evidence": response[:200],
            "confidence": confidence
        }
```

### 2.2 ç­”æ¡ˆç›¸é—œæ€§ (Answer Relevancy) çš„ç†è«–æ¨¡å‹

#### **åå‘å•é¡Œç”Ÿæˆæ–¹æ³•**

**åŸç†**: åŸºæ–¼ç”Ÿæˆçš„ç­”æ¡ˆï¼Œä½¿ç”¨ LLM åå‘ç”Ÿæˆå¯èƒ½çš„å•é¡Œï¼Œé€šéé€™äº›å•é¡Œèˆ‡åŸå§‹å•é¡Œçš„ç›¸ä¼¼åº¦ä¾†è©•ä¼°ç­”æ¡ˆç›¸é—œæ€§ã€‚

**æ•¸å­¸æ¨¡å‹**: è¨­åŸå§‹å•é¡Œç‚º $q$ï¼Œç­”æ¡ˆç‚º $a$ï¼Œåå‘ç”Ÿæˆçš„å•é¡Œé›†åˆç‚º $Q' = \{q'_1, q'_2, ..., q'_n\}$ï¼Œå‰‡ç­”æ¡ˆç›¸é—œæ€§ç‚ºï¼š

$$\text{Answer Relevancy} = \frac{1}{n} \sum_{i=1}^{n} \text{Similarity}(q, q'_i)$$

**ç®—æ³• 2.2** (ç­”æ¡ˆç›¸é—œæ€§è©•ä¼°):

```python
class AnswerRelevancyEvaluator:
    """ç­”æ¡ˆç›¸é—œæ€§è©•ä¼°å™¨"""

    def __init__(self, llm_evaluator: Any, embedding_model: Any):
        self.llm_evaluator = llm_evaluator
        self.embedding_model = embedding_model

    async def calculate_answer_relevancy(self, question: str,
                                       answer: str,
                                       num_questions: int = 3) -> Dict[str, Any]:
        """
        è¨ˆç®—ç­”æ¡ˆç›¸é—œæ€§åˆ†æ•¸

        åŸºæ–¼ Es et al. (2023) çš„åå‘å•é¡Œç”Ÿæˆæ–¹æ³•
        """

        # æ­¥é©Ÿ1: åŸºæ–¼ç­”æ¡ˆç”Ÿæˆå•é¡Œ
        generated_questions = await self._generate_questions_from_answer(
            answer, num_questions
        )

        if not generated_questions:
            return {"answer_relevancy": 0.0, "details": "No questions generated"}

        # æ­¥é©Ÿ2: è¨ˆç®—å•é¡Œç›¸ä¼¼åº¦
        original_embedding = self.embedding_model.encode([question])[0]
        generated_embeddings = self.embedding_model.encode(generated_questions)

        # æ­¥é©Ÿ3: è¨ˆç®—å¹³å‡ç›¸ä¼¼åº¦
        similarities = []
        for gen_embedding in generated_embeddings:
            similarity = cosine_similarity(
                [original_embedding], [gen_embedding]
            )[0][0]
            similarities.append(similarity)

        avg_similarity = sum(similarities) / len(similarities)

        return {
            "answer_relevancy": avg_similarity,
            "generated_questions": generated_questions,
            "individual_similarities": similarities,
            "question_quality": await self._assess_question_quality(generated_questions)
        }

    async def _generate_questions_from_answer(self, answer: str,
                                            num_questions: int) -> List[str]:
        """åŸºæ–¼ç­”æ¡ˆç”Ÿæˆå•é¡Œ"""

        prompt = f"""
        åŸºæ–¼ä»¥ä¸‹ç­”æ¡ˆï¼Œç”Ÿæˆ {num_questions} å€‹å¯èƒ½å°è‡´é€™å€‹ç­”æ¡ˆçš„å•é¡Œã€‚
        å•é¡Œæ‡‰è©²ï¼š
        1. é‚è¼¯åˆç†ä¸”è‡ªç„¶
        2. æ¶µè“‹ç­”æ¡ˆçš„ä¸»è¦ä¿¡æ¯é»
        3. å…·æœ‰ä¸åŒçš„è©¢å•è§’åº¦

        ç­”æ¡ˆ: {answer}

        è«‹ç”Ÿæˆå•é¡Œï¼š
        """

        response = await self.llm_evaluator.generate(prompt, temperature=0.3)

        # è§£æç”Ÿæˆçš„å•é¡Œ
        questions = []
        lines = response.strip().split('\n')

        for line in lines:
            line = line.strip()
            if line and ('?' in line or 'ï¼Ÿ' in line):
                # æ¸…ç†å•é¡Œæ ¼å¼
                question = re.sub(r'^\d+\.?\s*', '', line).strip()
                if len(question) > 10:  # éæ¿¾éçŸ­å•é¡Œ
                    questions.append(question)

        return questions[:num_questions]

    async def _assess_question_quality(self, questions: List[str]) -> Dict[str, float]:
        """è©•ä¼°ç”Ÿæˆå•é¡Œçš„å“è³ª"""

        if not questions:
            return {"diversity": 0.0, "naturalness": 0.0, "complexity": 0.0}

        # 1. å¤šæ¨£æ€§è©•ä¼°
        question_embeddings = self.embedding_model.encode(questions)
        diversity_score = await self._calculate_diversity(question_embeddings)

        # 2. è‡ªç„¶åº¦è©•ä¼°
        naturalness_scores = []
        for question in questions:
            naturalness = await self._assess_question_naturalness(question)
            naturalness_scores.append(naturalness)

        avg_naturalness = sum(naturalness_scores) / len(naturalness_scores)

        # 3. è¤‡é›œåº¦è©•ä¼°
        complexity_scores = []
        for question in questions:
            complexity = await self._assess_question_complexity(question)
            complexity_scores.append(complexity)

        avg_complexity = sum(complexity_scores) / len(complexity_scores)

        return {
            "diversity": diversity_score,
            "naturalness": avg_naturalness,
            "complexity": avg_complexity
        }

    async def _calculate_diversity(self, embeddings: np.ndarray) -> float:
        """è¨ˆç®—å•é¡Œé›†åˆçš„å¤šæ¨£æ€§"""

        if len(embeddings) <= 1:
            return 1.0

        # è¨ˆç®—å…©å…©ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(embeddings)

        # å»é™¤å°è§’ç·šå…ƒç´  (è‡ªç›¸ä¼¼åº¦)
        np.fill_diagonal(similarity_matrix, 0)

        # å¤šæ¨£æ€§ = 1 - å¹³å‡ç›¸ä¼¼åº¦
        avg_similarity = np.mean(similarity_matrix)
        diversity = 1.0 - avg_similarity

        return max(0.0, diversity)
```

### 2.3 ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦èˆ‡å¬å›ç‡

#### **ä¿¡æ¯æª¢ç´¢ç†è«–çš„å»¶ä¼¸**

**å®šç¾© 2.1** (ä¸Šä¸‹æ–‡ç´šç²¾ç¢ºåº¦): åœ¨æª¢ç´¢ä¸Šä¸‹æ–‡ä¸­ï¼Œèˆ‡å›ç­”å•é¡Œç›¸é—œçš„ä¿¡æ¯æ¯”ä¾‹ï¼š

$$\text{Context Precision@k} = \frac{1}{k} \sum_{i=1}^{k} \text{Relevance}(c_i, q)$$

å…¶ä¸­ $c_i$ ç‚ºç¬¬ $i$ å€‹æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µã€‚

**å®šç¾© 2.2** (ä¸Šä¸‹æ–‡ç´šå¬å›ç‡): å›ç­”å•é¡Œæ‰€éœ€çš„ä¿¡æ¯åœ¨æª¢ç´¢ä¸Šä¸‹æ–‡ä¸­çš„è¦†è“‹ç¨‹åº¦ï¼š

$$\text{Context Recall} = \frac{|\text{æª¢ç´¢åˆ°çš„å¿…éœ€ä¿¡æ¯} \cap \text{æ¨™æº–ç­”æ¡ˆä¿¡æ¯}|}{|\text{æ¨™æº–ç­”æ¡ˆä¿¡æ¯}|}$$

#### **å¯¦ç¾ç®—æ³•**

```python
class ContextEvaluator:
    """ä¸Šä¸‹æ–‡å“è³ªè©•ä¼°å™¨"""

    def __init__(self, llm_evaluator: Any):
        self.llm_evaluator = llm_evaluator

    async def calculate_context_precision(self, question: str,
                                        contexts: List[str]) -> Dict[str, Any]:
        """è¨ˆç®—ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦"""

        if not contexts:
            return {"context_precision": 0.0}

        relevance_scores = []
        detailed_assessments = []

        for i, context in enumerate(contexts):
            # è©•ä¼°æ¯å€‹ä¸Šä¸‹æ–‡çš„ç›¸é—œæ€§
            relevance = await self._assess_context_relevance(question, context)
            relevance_scores.append(relevance["score"])

            detailed_assessments.append({
                "context_index": i,
                "context_preview": context[:100] + "..." if len(context) > 100 else context,
                "relevance_score": relevance["score"],
                "relevance_reasoning": relevance.get("reasoning", "")
            })

        # è¨ˆç®—ç²¾ç¢ºåº¦
        precision = sum(relevance_scores) / len(relevance_scores)

        return {
            "context_precision": precision,
            "individual_scores": relevance_scores,
            "detailed_assessments": detailed_assessments,
            "num_contexts": len(contexts)
        }

    async def calculate_context_recall(self, question: str,
                                     contexts: List[str],
                                     ground_truth_answer: str) -> Dict[str, Any]:
        """è¨ˆç®—ä¸Šä¸‹æ–‡å¬å›ç‡"""

        if not contexts or not ground_truth_answer:
            return {"context_recall": 0.0}

        # æ­¥é©Ÿ1: å¾æ¨™æº–ç­”æ¡ˆä¸­æå–é—œéµä¿¡æ¯
        required_info = await self._extract_required_information(
            question, ground_truth_answer
        )

        if not required_info:
            return {"context_recall": 0.0, "details": "No required information identified"}

        # æ­¥é©Ÿ2: æª¢æŸ¥æ¯å€‹é—œéµä¿¡æ¯æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
        coverage_results = []
        for info_item in required_info:
            coverage = await self._check_information_coverage(
                info_item, contexts
            )
            coverage_results.append(coverage)

        # æ­¥é©Ÿ3: è¨ˆç®—å¬å›ç‡
        covered_count = sum(1 for c in coverage_results if c["covered"])
        recall = covered_count / len(required_info)

        return {
            "context_recall": recall,
            "required_information": required_info,
            "coverage_results": coverage_results,
            "covered_items": covered_count,
            "total_required": len(required_info)
        }

    async def _assess_context_relevance(self, question: str,
                                      context: str) -> Dict[str, Any]:
        """è©•ä¼°ä¸Šä¸‹æ–‡èˆ‡å•é¡Œçš„ç›¸é—œæ€§"""

        prompt = f"""
        è©•ä¼°ä»¥ä¸‹ä¸Šä¸‹æ–‡å°æ–¼å›ç­”å•é¡Œçš„ç›¸é—œæ€§ã€‚

        å•é¡Œ: {question}

        ä¸Šä¸‹æ–‡: {context}

        è«‹è©•ä¼°ï¼š
        1. è©²ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«èˆ‡å•é¡Œç›¸é—œçš„ä¿¡æ¯ï¼Ÿ
        2. ç›¸é—œæ€§ç¨‹åº¦å¦‚ä½•ï¼Ÿ(0-1åˆ†ï¼Œ1è¡¨ç¤ºé«˜åº¦ç›¸é—œ)
        3. å…·é«”å“ªéƒ¨åˆ†å…§å®¹æ˜¯ç›¸é—œçš„ï¼Ÿ

        è«‹ä»¥JSONæ ¼å¼å›ç­”ï¼š
        {{
            "relevant": true/false,
            "score": 0.85,
            "reasoning": "ç›¸é—œæ€§åˆ†æ",
            "relevant_parts": "ç›¸é—œå…§å®¹æ‘˜è¦"
        }}
        """

        response = await self.llm_evaluator.generate(prompt, temperature=0.1)

        try:
            result = self._parse_json_response(response)
            result["score"] = float(result.get("score", 0.0))
            return result
        except Exception:
            # å‚™ç”¨è§£æé‚è¼¯
            relevant = "relevant" in response.lower() or "ç›¸é—œ" in response
            return {
                "relevant": relevant,
                "score": 0.7 if relevant else 0.2,
                "reasoning": response[:200],
                "relevant_parts": ""
            }

    async def _extract_required_information(self, question: str,
                                          ground_truth: str) -> List[str]:
        """å¾æ¨™æº–ç­”æ¡ˆä¸­æå–å›ç­”å•é¡Œæ‰€éœ€çš„é—œéµä¿¡æ¯"""

        prompt = f"""
        åˆ†ææ¨™æº–ç­”æ¡ˆï¼Œæå–å›ç­”ä»¥ä¸‹å•é¡Œæ‰€å¿…éœ€çš„é—œéµä¿¡æ¯é»ã€‚

        å•é¡Œ: {question}
        æ¨™æº–ç­”æ¡ˆ: {ground_truth}

        è«‹åˆ—å‡ºå›ç­”è©²å•é¡Œå¿…é ˆåŒ…å«çš„é—œéµä¿¡æ¯é»ï¼š
        1. [ä¿¡æ¯é»1]
        2. [ä¿¡æ¯é»2]
        ...

        é—œéµä¿¡æ¯é»:
        """

        response = await self.llm_evaluator.generate(prompt, temperature=0.1)

        # è§£æä¿¡æ¯é»
        info_items = []
        lines = response.strip().split('\n')

        for line in lines:
            line = line.strip()
            if line and re.match(r'^\d+\.', line):
                info_text = re.sub(r'^\d+\.\s*', '', line).strip()
                if info_text and len(info_text) > 5:
                    info_items.append(info_text)

        return info_items

    async def _check_information_coverage(self, required_info: str,
                                        contexts: List[str]) -> Dict[str, Any]:
        """æª¢æŸ¥å¿…éœ€ä¿¡æ¯æ˜¯å¦è¢«ä¸Šä¸‹æ–‡è¦†è“‹"""

        combined_contexts = "\n\n".join(contexts)

        prompt = f"""
        æª¢æŸ¥ä»¥ä¸‹å¿…éœ€ä¿¡æ¯æ˜¯å¦åœ¨çµ¦å®šçš„ä¸Šä¸‹æ–‡ä¸­è¢«è¦†è“‹ã€‚

        å¿…éœ€ä¿¡æ¯: {required_info}

        ä¸Šä¸‹æ–‡:
        {combined_contexts}

        è«‹åˆ¤æ–·ï¼š
        1. è©²ä¿¡æ¯æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­å‡ºç¾ï¼Ÿ
        2. è¦†è“‹ç¨‹åº¦å¦‚ä½•ï¼Ÿ(0-1åˆ†)
        3. åœ¨å“ªå€‹éƒ¨åˆ†æ‰¾åˆ°è©²ä¿¡æ¯ï¼Ÿ

        è«‹ä»¥JSONæ ¼å¼å›ç­”ï¼š
        {{
            "covered": true/false,
            "coverage_score": 0.9,
            "location": "ä¿¡æ¯åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ä½ç½®æè¿°"
        }}
        """

        response = await self.llm_evaluator.generate(prompt, temperature=0.1)

        try:
            result = self._parse_json_response(response)
            result["required_info"] = required_info
            return result
        except Exception:
            # ç°¡åŒ–çš„è¦†è“‹æª¢æŸ¥
            covered = any(self._text_overlap(required_info, context) > 0.3
                         for context in contexts)
            return {
                "covered": covered,
                "coverage_score": 0.7 if covered else 0.1,
                "location": "automatic_detection",
                "required_info": required_info
            }

    def _text_overlap(self, text1: str, text2: str) -> float:
        """è¨ˆç®—å…©å€‹æ–‡æœ¬çš„é‡ç–Šåº¦"""

        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1:
            return 0.0

        overlap = len(words1.intersection(words2))
        return overlap / len(words1)
```

---

## 3. ä¼æ¥­ç´šè©•ä¼°é«”ç³»è¨­è¨ˆ

### 3.1 åˆ†å±¤è©•ä¼°æ¶æ§‹

#### **ä¸‰å±¤è©•ä¼°æ¨¡å‹**

**å±¤ç´š 3.1** (RAG è©•ä¼°é‡‘å­—å¡”):

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æ¥­å‹™å±¤è©•ä¼°     â”‚  â† ç”¨æˆ¶æ»¿æ„åº¦ã€æ¥­å‹™KPI
                    â”‚  (Business)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ç³»çµ±å±¤è©•ä¼°     â”‚  â† Faithfulnessã€Relevancy
                    â”‚  (System)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   çµ„ä»¶å±¤è©•ä¼°     â”‚  â† Retrievalã€Generation
                    â”‚ (Component)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å¯¦ç¾æ¶æ§‹**:

```python
class EnterpriseRAGEvaluationFramework:
    """ä¼æ¥­ç´š RAG è©•ä¼°æ¡†æ¶"""

    def __init__(self):
        # çµ„ä»¶å±¤è©•ä¼°å™¨
        self.component_evaluators = {
            "retrieval": RetrievalEvaluator(),
            "reranking": RerankingEvaluator(),
            "generation": GenerationEvaluator()
        }

        # ç³»çµ±å±¤è©•ä¼°å™¨
        self.system_evaluators = {
            "faithfulness": FaithfulnessEvaluator(),
            "relevancy": AnswerRelevancyEvaluator(),
            "context_precision": ContextEvaluator(),
            "context_recall": ContextEvaluator()
        }

        # æ¥­å‹™å±¤è©•ä¼°å™¨
        self.business_evaluators = {
            "user_satisfaction": UserSatisfactionEvaluator(),
            "task_success": TaskSuccessEvaluator(),
            "cost_effectiveness": CostEffectivenessEvaluator()
        }

    async def comprehensive_evaluation(self, test_dataset: List[Dict],
                                     rag_system: Any) -> Dict[str, Any]:
        """åŸ·è¡Œå…¨é¢è©•ä¼°"""

        results = {
            "component_level": {},
            "system_level": {},
            "business_level": {},
            "overall_assessment": {}
        }

        # çµ„ä»¶å±¤è©•ä¼°
        for component_name, evaluator in self.component_evaluators.items():
            print(f"è©•ä¼°çµ„ä»¶: {component_name}")
            component_result = await evaluator.evaluate(test_dataset, rag_system)
            results["component_level"][component_name] = component_result

        # ç³»çµ±å±¤è©•ä¼°
        for metric_name, evaluator in self.system_evaluators.items():
            print(f"è©•ä¼°æŒ‡æ¨™: {metric_name}")
            metric_result = await evaluator.evaluate(test_dataset, rag_system)
            results["system_level"][metric_name] = metric_result

        # æ¥­å‹™å±¤è©•ä¼°
        for business_metric, evaluator in self.business_evaluators.items():
            print(f"æ¥­å‹™è©•ä¼°: {business_metric}")
            business_result = await evaluator.evaluate(test_dataset, rag_system)
            results["business_level"][business_metric] = business_result

        # ç¶œåˆè©•ä¼°
        overall_assessment = await self._calculate_overall_assessment(results)
        results["overall_assessment"] = overall_assessment

        return results

    async def _calculate_overall_assessment(self, evaluation_results: Dict) -> Dict:
        """è¨ˆç®—ç¶œåˆè©•ä¼°åˆ†æ•¸"""

        # æ¬Šé‡é…ç½®
        weights = {
            "component_level": 0.2,
            "system_level": 0.5,
            "business_level": 0.3
        }

        weighted_scores = {}

        # çµ„ä»¶å±¤ç¶œåˆåˆ†æ•¸
        component_scores = evaluation_results["component_level"]
        component_avg = sum(score.get("overall_score", 0.0)
                           for score in component_scores.values()) / len(component_scores)
        weighted_scores["component"] = component_avg * weights["component_level"]

        # ç³»çµ±å±¤ç¶œåˆåˆ†æ•¸
        system_scores = evaluation_results["system_level"]
        system_avg = sum(score.get("score", 0.0)
                        for score in system_scores.values()) / len(system_scores)
        weighted_scores["system"] = system_avg * weights["system_level"]

        # æ¥­å‹™å±¤ç¶œåˆåˆ†æ•¸
        business_scores = evaluation_results["business_level"]
        business_avg = sum(score.get("score", 0.0)
                          for score in business_scores.values()) / len(business_scores)
        weighted_scores["business"] = business_avg * weights["business_level"]

        # ç¸½é«”åˆ†æ•¸
        overall_score = sum(weighted_scores.values())

        return {
            "overall_score": overall_score,
            "weighted_scores": weighted_scores,
            "grade": self._assign_performance_grade(overall_score),
            "strengths": self._identify_strengths(evaluation_results),
            "weaknesses": self._identify_weaknesses(evaluation_results),
            "improvement_recommendations": self._generate_recommendations(evaluation_results)
        }

    def _assign_performance_grade(self, score: float) -> str:
        """åˆ†é…æ€§èƒ½ç­‰ç´š"""
        if score >= 0.9:
            return "A+ (å„ªç§€)"
        elif score >= 0.8:
            return "A (è‰¯å¥½)"
        elif score >= 0.7:
            return "B (åˆæ ¼)"
        elif score >= 0.6:
            return "C (éœ€æ”¹é€²)"
        else:
            return "D (ä¸åˆæ ¼)"

    def _identify_strengths(self, results: Dict) -> List[str]:
        """è­˜åˆ¥ç³»çµ±å„ªå‹¢"""

        strengths = []

        # æª¢æŸ¥å„å±¤ç´šçš„é«˜åˆ†é …ç›®
        for level_name, level_results in results.items():
            if level_name == "overall_assessment":
                continue

            for metric, result in level_results.items():
                score = result.get("score", result.get("overall_score", 0.0))
                if score > 0.8:
                    strengths.append(f"{level_name}.{metric}: {score:.2f}")

        return strengths

    def _identify_weaknesses(self, results: Dict) -> List[str]:
        """è­˜åˆ¥ç³»çµ±å¼±é»"""

        weaknesses = []

        for level_name, level_results in results.items():
            if level_name == "overall_assessment":
                continue

            for metric, result in level_results.items():
                score = result.get("score", result.get("overall_score", 0.0))
                if score < 0.6:
                    weaknesses.append(f"{level_name}.{metric}: {score:.2f}")

        return weaknesses
```

---

## 4. ç·šä¸Šç›£æ§èˆ‡å“è³ªä¿è­‰

### 4.1 å¯¦æ™‚è©•ä¼°ç³»çµ±è¨­è¨ˆ

#### **æµå¼è©•ä¼°æ¶æ§‹**

**ç³»çµ± 4.1** (å¯¦æ™‚ RAG å“è³ªç›£æ§):

```python
import asyncio
from collections import deque
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

class RealTimeQualityMonitor:
    """å¯¦æ™‚å“è³ªç›£æ§ç³»çµ±"""

    def __init__(self, evaluation_config: Dict):
        self.config = evaluation_config
        self.metrics_buffer = deque(maxlen=1000)  # æ»¾å‹•çª—å£
        self.alert_thresholds = self._load_alert_thresholds()
        self.quality_trends = QualityTrendAnalyzer()

    async def monitor_query_execution(self, query: str, response: Dict,
                                    user_feedback: Optional[Dict] = None) -> Dict:
        """ç›£æ§æŸ¥è©¢åŸ·è¡Œçš„å“è³ªæŒ‡æ¨™"""

        # å¯¦æ™‚å“è³ªè©•ä¼°
        quality_metrics = await self._quick_quality_assessment(query, response)

        # æ·»åŠ ç”¨æˆ¶åé¥‹ (å¦‚æœæœ‰)
        if user_feedback:
            quality_metrics["user_satisfaction"] = user_feedback.get("rating", 0.0)
            quality_metrics["user_helpful"] = user_feedback.get("helpful", False)

        # è¨˜éŒ„åˆ°ç·©è¡å€
        timestamp = datetime.now()
        self.metrics_buffer.append({
            "timestamp": timestamp,
            "query": query,
            "metrics": quality_metrics,
            "response_metadata": response.get("metadata", {})
        })

        # æª¢æŸ¥å‘Šè­¦æ¢ä»¶
        alerts = await self._check_alert_conditions(quality_metrics, timestamp)

        # æ›´æ–°è¶¨å‹¢åˆ†æ
        await self.quality_trends.update_trends(quality_metrics, timestamp)

        return {
            "quality_metrics": quality_metrics,
            "alerts": alerts,
            "monitoring_status": "active"
        }

    async def _quick_quality_assessment(self, query: str,
                                      response: Dict) -> Dict[str, float]:
        """å¿«é€Ÿå“è³ªè©•ä¼° (é©ç”¨æ–¼å¯¦æ™‚ç›£æ§)"""

        metrics = {}

        # 1. éŸ¿æ‡‰æ™‚é–“æŒ‡æ¨™
        processing_time = response.get("processing_time_ms", 0)
        latency_score = self._calculate_latency_score(processing_time)
        metrics["latency_score"] = latency_score

        # 2. ä¾†æºå“è³ªæŒ‡æ¨™
        sources = response.get("sources", [])
        source_quality = await self._assess_source_quality(sources)
        metrics["source_quality"] = source_quality

        # 3. å›ç­”å®Œæ•´æ€§ (ç°¡åŒ–ç‰ˆæœ¬)
        answer = response.get("answer", "")
        answer_completeness = await self._estimate_answer_completeness(query, answer)
        metrics["answer_completeness"] = answer_completeness

        # 4. å¼•ç”¨è¦†è“‹ç‡
        citation_coverage = self._calculate_citation_coverage(answer, sources)
        metrics["citation_coverage"] = citation_coverage

        return metrics

    def _calculate_latency_score(self, processing_time_ms: float) -> float:
        """è¨ˆç®—å»¶é²åˆ†æ•¸ (è¶Šä½è¶Šå¥½)"""

        # SLO ç›®æ¨™: p95 < 500ms
        if processing_time_ms <= 200:
            return 1.0
        elif processing_time_ms <= 500:
            return 1.0 - (processing_time_ms - 200) / 300 * 0.3
        elif processing_time_ms <= 1000:
            return 0.7 - (processing_time_ms - 500) / 500 * 0.4
        else:
            return max(0.0, 0.3 - (processing_time_ms - 1000) / 2000 * 0.3)

    async def _assess_source_quality(self, sources: List[Dict]) -> float:
        """è©•ä¼°æª¢ç´¢ä¾†æºçš„å“è³ª"""

        if not sources:
            return 0.0

        quality_scores = []

        for source in sources:
            score = 0.0

            # ä¾†æºå¯ä¿¡åº¦
            if source.get("confidence", 0) > 0.8:
                score += 0.3

            # å…§å®¹é•·åº¦åˆç†æ€§
            content_length = len(source.get("content", ""))
            if 50 <= content_length <= 2000:
                score += 0.2
            elif content_length > 2000:
                score += 0.1

            # å…ƒæ•¸æ“šå®Œæ•´æ€§
            metadata = source.get("metadata", {})
            if metadata.get("title") and metadata.get("timestamp"):
                score += 0.3

            # ç›¸é—œæ€§åˆ†æ•¸
            relevance = source.get("score", 0.0)
            score += 0.2 * min(1.0, relevance)

            quality_scores.append(score)

        return sum(quality_scores) / len(quality_scores)

    async def _check_alert_conditions(self, metrics: Dict[str, float],
                                    timestamp: datetime) -> List[Dict]:
        """æª¢æŸ¥å‘Šè­¦æ¢ä»¶"""

        alerts = []

        # æª¢æŸ¥å„é …æŒ‡æ¨™
        for metric_name, value in metrics.items():
            if metric_name in self.alert_thresholds:
                threshold = self.alert_thresholds[metric_name]

                if (threshold.get("type") == "min" and
                    value < threshold["value"]):
                    alerts.append({
                        "type": "quality_degradation",
                        "metric": metric_name,
                        "current_value": value,
                        "threshold": threshold["value"],
                        "severity": threshold.get("severity", "warning"),
                        "timestamp": timestamp
                    })

                elif (threshold.get("type") == "max" and
                      value > threshold["value"]):
                    alerts.append({
                        "type": "performance_degradation",
                        "metric": metric_name,
                        "current_value": value,
                        "threshold": threshold["value"],
                        "severity": threshold.get("severity", "warning"),
                        "timestamp": timestamp
                    })

        # æª¢æŸ¥è¶¨å‹¢å‘Šè­¦
        trend_alerts = await self.quality_trends.check_trend_alerts(timestamp)
        alerts.extend(trend_alerts)

        return alerts

    def _load_alert_thresholds(self) -> Dict[str, Dict]:
        """è¼‰å…¥å‘Šè­¦é–¾å€¼é…ç½®"""

        return {
            "faithfulness": {
                "type": "min",
                "value": 0.8,
                "severity": "warning"
            },
            "answer_relevancy": {
                "type": "min",
                "value": 0.7,
                "severity": "warning"
            },
            "context_precision": {
                "type": "min",
                "value": 0.6,
                "severity": "info"
            },
            "latency_score": {
                "type": "min",
                "value": 0.7,
                "severity": "critical"
            },
            "source_quality": {
                "type": "min",
                "value": 0.5,
                "severity": "warning"
            }
        }

class QualityTrendAnalyzer:
    """å“è³ªè¶¨å‹¢åˆ†æå™¨"""

    def __init__(self):
        self.trend_window = timedelta(hours=24)  # 24å°æ™‚è¶¨å‹¢çª—å£
        self.metrics_history = {}

    async def update_trends(self, metrics: Dict[str, float],
                           timestamp: datetime):
        """æ›´æ–°å“è³ªè¶¨å‹¢"""

        for metric_name, value in metrics.items():
            if metric_name not in self.metrics_history:
                self.metrics_history[metric_name] = deque(maxlen=1000)

            self.metrics_history[metric_name].append({
                "timestamp": timestamp,
                "value": value
            })

    async def check_trend_alerts(self, current_time: datetime) -> List[Dict]:
        """æª¢æŸ¥è¶¨å‹¢å‘Šè­¦"""

        alerts = []

        for metric_name, history in self.metrics_history.items():
            if len(history) < 10:  # éœ€è¦è¶³å¤ çš„æ­·å²æ•¸æ“š
                continue

            # è¨ˆç®—æœ€è¿‘è¶¨å‹¢
            recent_data = [
                record for record in history
                if current_time - record["timestamp"] <= self.trend_window
            ]

            if len(recent_data) < 5:
                continue

            # è¨ˆç®—è¶¨å‹¢æ–œç‡
            timestamps = [(r["timestamp"] - current_time).total_seconds()
                         for r in recent_data]
            values = [r["value"] for r in recent_data]

            trend_slope = self._calculate_trend_slope(timestamps, values)

            # æª¢æŸ¥ä¸‹é™è¶¨å‹¢
            if trend_slope < -0.1:  # é¡¯è‘—ä¸‹é™è¶¨å‹¢
                alerts.append({
                    "type": "negative_trend",
                    "metric": metric_name,
                    "trend_slope": trend_slope,
                    "severity": "warning",
                    "timestamp": current_time,
                    "description": f"{metric_name} å‘ˆç¾ä¸‹é™è¶¨å‹¢ (æ–œç‡: {trend_slope:.3f})"
                })

        return alerts

    def _calculate_trend_slope(self, x: List[float], y: List[float]) -> float:
        """è¨ˆç®—ç·šæ€§è¶¨å‹¢æ–œç‡"""

        if len(x) != len(y) or len(x) < 2:
            return 0.0

        n = len(x)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(xi * yi for xi, yi in zip(x, y))
        sum_x2 = sum(xi * xi for xi in x)

        # ç·šæ€§å›æ­¸æ–œç‡
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)

        return slope
```

---

## 5. A/B æ¸¬è©¦èˆ‡æŒçºŒæ”¹é€²

### 5.1 RAG ç³»çµ± A/B æ¸¬è©¦è¨­è¨ˆ

#### **å¯¦é©—è¨­è¨ˆåŸç†**

**å®šç¾© 5.1** (RAG A/B æ¸¬è©¦): å°ç…§å¯¦é©—è¨­è¨ˆï¼Œæ¯”è¼ƒä¸åŒ RAG é…ç½®æˆ–ç®—æ³•åœ¨ç›¸åŒè©•ä¼°æŒ‡æ¨™ä¸Šçš„æ€§èƒ½å·®ç•°ã€‚

**çµ±è¨ˆå‡è¨­æª¢é©—**:
- **é›¶å‡è¨­ $H_0$**: $\mu_A = \mu_B$ (å…©å€‹ç‰ˆæœ¬æ€§èƒ½ç„¡å·®ç•°)
- **å°ç«‹å‡è¨­ $H_1$**: $\mu_A \neq \mu_B$ (å­˜åœ¨é¡¯è‘—å·®ç•°)

**åŠŸæ•ˆåˆ†æ**: æ‰€éœ€æ¨£æœ¬é‡è¨ˆç®—ï¼š

$$n = \frac{2(z_{\alpha/2} + z_{\beta})^2 \sigma^2}{(\mu_A - \mu_B)^2}$$

å…¶ä¸­ï¼š
- $\alpha$: ç¬¬ä¸€é¡éŒ¯èª¤æ¦‚ç‡ (é€šå¸¸ 0.05)
- $\beta$: ç¬¬äºŒé¡éŒ¯èª¤æ¦‚ç‡ (é€šå¸¸ 0.2)
- $\sigma$: ç¸½é«”æ¨™æº–å·®
- $\mu_A - \mu_B$: æœ€å°å¯æª¢æ¸¬å·®ç•°

#### **A/B æ¸¬è©¦æ¡†æ¶å¯¦ç¾**

```python
import numpy as np
from scipy import stats
from typing import Dict, List, Any, Optional
import random
from datetime import datetime, timedelta

class RAGABTestFramework:
    """RAG ç³»çµ± A/B æ¸¬è©¦æ¡†æ¶"""

    def __init__(self):
        self.experiments = {}  # æ´»èºå¯¦é©—
        self.results_store = ExperimentResultStore()
        self.statistical_analyzer = StatisticalAnalyzer()

    async def create_experiment(self, experiment_config: Dict) -> str:
        """å‰µå»ºæ–°çš„ A/B æ¸¬è©¦å¯¦é©—"""

        experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # é©—è­‰å¯¦é©—é…ç½®
        validation_result = await self._validate_experiment_config(experiment_config)
        if not validation_result["valid"]:
            raise ValueError(f"Invalid experiment config: {validation_result['errors']}")

        # è¨ˆç®—æ‰€éœ€æ¨£æœ¬é‡
        required_sample_size = await self._calculate_required_sample_size(
            experiment_config
        )

        experiment = {
            "id": experiment_id,
            "name": experiment_config["name"],
            "description": experiment_config["description"],
            "variants": experiment_config["variants"],
            "success_metrics": experiment_config["success_metrics"],
            "traffic_allocation": experiment_config["traffic_allocation"],
            "required_sample_size": required_sample_size,
            "start_date": datetime.now(),
            "status": "active",
            "current_samples": {variant: 0 for variant in experiment_config["variants"]}
        }

        self.experiments[experiment_id] = experiment

        return experiment_id

    async def assign_user_to_variant(self, experiment_id: str,
                                   user_id: str) -> str:
        """åˆ†é…ç”¨æˆ¶åˆ°å¯¦é©—è®Šé«”"""

        if experiment_id not in self.experiments:
            return "control"

        experiment = self.experiments[experiment_id]

        # ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œç¢ºä¿ç”¨æˆ¶ç¸½æ˜¯åˆ†é…åˆ°åŒä¸€è®Šé«”
        hash_input = f"{experiment_id}:{user_id}"
        hash_value = hash(hash_input) % 10000
        allocation_value = hash_value / 10000.0

        # æ ¹æ“šæµé‡åˆ†é…ç¢ºå®šè®Šé«”
        cumulative_allocation = 0.0
        for variant, allocation in experiment["traffic_allocation"].items():
            cumulative_allocation += allocation
            if allocation_value <= cumulative_allocation:
                return variant

        return "control"  # å‚™ç”¨

    async def record_experiment_result(self, experiment_id: str,
                                     user_id: str, variant: str,
                                     query: str, result: Dict,
                                     user_feedback: Optional[Dict] = None):
        """è¨˜éŒ„å¯¦é©—çµæœ"""

        if experiment_id not in self.experiments:
            return

        # è¨˜éŒ„å¯¦é©—æ•¸æ“šé»
        data_point = {
            "experiment_id": experiment_id,
            "user_id": user_id,
            "variant": variant,
            "timestamp": datetime.now(),
            "query": query,
            "result": result,
            "user_feedback": user_feedback,
            "session_metadata": {
                "processing_time": result.get("processing_time_ms", 0),
                "num_sources": len(result.get("sources", [])),
                "answer_length": len(result.get("answer", ""))
            }
        }

        await self.results_store.save_data_point(data_point)

        # æ›´æ–°å¯¦é©—æ¨£æœ¬è¨ˆæ•¸
        experiment = self.experiments[experiment_id]
        experiment["current_samples"][variant] += 1

        # æª¢æŸ¥æ˜¯å¦é”åˆ°çµ±è¨ˆé¡¯è‘—æ€§
        if sum(experiment["current_samples"].values()) >= experiment["required_sample_size"]:
            await self._check_statistical_significance(experiment_id)

    async def analyze_experiment_results(self, experiment_id: str) -> Dict:
        """åˆ†æå¯¦é©—çµæœ"""

        if experiment_id not in self.experiments:
            return {"error": "Experiment not found"}

        experiment = self.experiments[experiment_id]

        # ç²å–å¯¦é©—æ•¸æ“š
        experiment_data = await self.results_store.get_experiment_data(experiment_id)

        # æŒ‰è®Šé«”åˆ†çµ„æ•¸æ“š
        variant_data = {}
        for data_point in experiment_data:
            variant = data_point["variant"]
            if variant not in variant_data:
                variant_data[variant] = []
            variant_data[variant].append(data_point)

        # åˆ†ææ¯å€‹æˆåŠŸæŒ‡æ¨™
        analysis_results = {}
        for metric in experiment["success_metrics"]:
            metric_analysis = await self._analyze_metric(variant_data, metric)
            analysis_results[metric] = metric_analysis

        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—
        significance_results = await self._perform_significance_tests(
            variant_data, experiment["success_metrics"]
        )

        # ç”Ÿæˆå¯¦é©—å ±å‘Š
        experiment_report = {
            "experiment_id": experiment_id,
            "experiment_name": experiment["name"],
            "analysis_timestamp": datetime.now(),
            "sample_sizes": {variant: len(data) for variant, data in variant_data.items()},
            "metric_analysis": analysis_results,
            "significance_tests": significance_results,
            "recommendation": await self._generate_experiment_recommendation(
                analysis_results, significance_results
            )
        }

        return experiment_report

    async def _analyze_metric(self, variant_data: Dict[str, List],
                            metric: str) -> Dict:
        """åˆ†æç‰¹å®šæŒ‡æ¨™åœ¨ä¸åŒè®Šé«”é–“çš„è¡¨ç¾"""

        metric_results = {}

        for variant, data_points in variant_data.items():
            metric_values = []

            for data_point in data_points:
                # æ ¹æ“šæŒ‡æ¨™é¡å‹æå–å€¼
                if metric == "user_satisfaction":
                    feedback = data_point.get("user_feedback", {})
                    if feedback and "rating" in feedback:
                        metric_values.append(feedback["rating"])

                elif metric == "response_time":
                    time_ms = data_point["result"].get("processing_time_ms", 0)
                    metric_values.append(time_ms)

                elif metric == "answer_quality":
                    # é€™è£¡éœ€è¦å¯¦æ™‚å“è³ªè©•ä¼°
                    quality_score = await self._estimate_answer_quality(
                        data_point["query"],
                        data_point["result"].get("answer", "")
                    )
                    metric_values.append(quality_score)

            if metric_values:
                metric_results[variant] = {
                    "mean": np.mean(metric_values),
                    "std": np.std(metric_values),
                    "count": len(metric_values),
                    "median": np.median(metric_values),
                    "values": metric_values
                }

        return metric_results

    async def _perform_significance_tests(self, variant_data: Dict,
                                        metrics: List[str]) -> Dict:
        """åŸ·è¡Œçµ±è¨ˆé¡¯è‘—æ€§æª¢é©—"""

        significance_results = {}

        for metric in metrics:
            metric_analysis = await self._analyze_metric(variant_data, metric)

            if len(metric_analysis) >= 2:
                # å‡è¨­æœ‰ control å’Œ treatment å…©å€‹è®Šé«”
                variants = list(metric_analysis.keys())
                control_data = metric_analysis[variants[0]]["values"]
                treatment_data = metric_analysis[variants[1]]["values"]

                # åŸ·è¡Œ t æª¢é©—
                t_stat, p_value = stats.ttest_ind(control_data, treatment_data)

                # è¨ˆç®—æ•ˆæ‡‰å¤§å° (Cohen's d)
                pooled_std = np.sqrt(
                    ((len(control_data) - 1) * np.var(control_data) +
                     (len(treatment_data) - 1) * np.var(treatment_data)) /
                    (len(control_data) + len(treatment_data) - 2)
                )

                effect_size = (np.mean(treatment_data) - np.mean(control_data)) / pooled_std

                significance_results[metric] = {
                    "t_statistic": t_stat,
                    "p_value": p_value,
                    "effect_size": effect_size,
                    "significant": p_value < 0.05,
                    "control_mean": np.mean(control_data),
                    "treatment_mean": np.mean(treatment_data),
                    "practical_significance": abs(effect_size) > 0.2  # Cohen's convention
                }

        return significance_results
```

---

## 6. è©•ä¼°æ•¸æ“šé›†æ§‹å»º

### 6.1 é»ƒé‡‘æ¨™æº–æ•¸æ“šé›†è¨­è¨ˆ

#### **ä¼æ¥­è©•ä¼°æ•¸æ“šé›†çš„æ§‹å»ºåŸå‰‡**

**åŸå‰‡ 6.1** (ä»£è¡¨æ€§åŸå‰‡): è©•ä¼°æ•¸æ“šé›†æ‡‰è¦†è“‹ä¼æ¥­å¯¦éš›ä½¿ç”¨ä¸­çš„å„ç¨®æŸ¥è©¢é¡å‹å’Œé›£åº¦åˆ†ä½ˆã€‚

**æ•¸æ“šé›†æ§‹å»ºæµç¨‹**:

```python
class EnterpriseEvaluationDatasetBuilder:
    """ä¼æ¥­è©•ä¼°æ•¸æ“šé›†æ§‹å»ºå™¨"""

    def __init__(self):
        self.query_categories = [
            "factual_lookup",      # äº‹å¯¦æŸ¥è©¢
            "procedural_guide",    # ç¨‹åºæŒ‡å—
            "analytical_complex",  # åˆ†æå‹è¤‡é›œæŸ¥è©¢
            "troubleshooting",     # æ•…éšœæ’é™¤
            "policy_compliance",   # æ”¿ç­–åˆè¦
            "multi_hop_reasoning"  # å¤šè·³æ¨ç†
        ]

        self.difficulty_levels = ["easy", "medium", "hard", "expert"]

    async def build_balanced_dataset(self, source_queries: List[Dict],
                                   target_size: int = 500) -> List[Dict]:
        """æ§‹å»ºå¹³è¡¡çš„è©•ä¼°æ•¸æ“šé›†"""

        # ç›®æ¨™åˆ†ä½ˆ: æ¯å€‹é¡åˆ¥-é›£åº¦çµ„åˆçš„æ¨£æœ¬æ•¸
        categories = len(self.query_categories)
        difficulties = len(self.difficulty_levels)
        samples_per_cell = target_size // (categories * difficulties)

        balanced_dataset = []

        # å°æ¯å€‹é¡åˆ¥-é›£åº¦çµ„åˆæ¡æ¨£
        for category in self.query_categories:
            for difficulty in self.difficulty_levels:
                # éæ¿¾ç¬¦åˆæ¢ä»¶çš„æŸ¥è©¢
                matching_queries = [
                    q for q in source_queries
                    if (q.get("category") == category and
                        q.get("difficulty") == difficulty)
                ]

                # æ¡æ¨£
                if len(matching_queries) >= samples_per_cell:
                    sampled = random.sample(matching_queries, samples_per_cell)
                else:
                    sampled = matching_queries
                    # å¦‚æœæ¨£æœ¬ä¸è¶³ï¼Œè¨˜éŒ„è­¦å‘Š
                    print(f"Warning: ä¸è¶³æ¨£æœ¬ {category}-{difficulty}: {len(matching_queries)}")

                balanced_dataset.extend(sampled)

        # è£œå……åˆ°ç›®æ¨™å¤§å°
        remaining = target_size - len(balanced_dataset)
        if remaining > 0:
            unused_queries = [q for q in source_queries if q not in balanced_dataset]
            if unused_queries:
                additional_samples = random.sample(
                    unused_queries, min(remaining, len(unused_queries))
                )
                balanced_dataset.extend(additional_samples)

        # æ‰“äº‚æ•¸æ“šé›†é †åº
        random.shuffle(balanced_dataset)

        return balanced_dataset

    async def validate_dataset_quality(self, dataset: List[Dict]) -> Dict:
        """é©—è­‰æ•¸æ“šé›†å“è³ª"""

        quality_metrics = {}

        # 1. åˆ†ä½ˆå¹³è¡¡æ€§æª¢æŸ¥
        category_distribution = {}
        difficulty_distribution = {}

        for item in dataset:
            category = item.get("category", "unknown")
            difficulty = item.get("difficulty", "unknown")

            category_distribution[category] = category_distribution.get(category, 0) + 1
            difficulty_distribution[difficulty] = difficulty_distribution.get(difficulty, 0) + 1

        # è¨ˆç®—åˆ†ä½ˆç†µ (è¶Šé«˜è¶Šå¹³è¡¡)
        category_entropy = self._calculate_distribution_entropy(category_distribution)
        difficulty_entropy = self._calculate_distribution_entropy(difficulty_distribution)

        quality_metrics["category_balance"] = category_entropy / np.log(len(self.query_categories))
        quality_metrics["difficulty_balance"] = difficulty_entropy / np.log(len(self.difficulty_levels))

        # 2. æŸ¥è©¢å“è³ªæª¢æŸ¥
        query_quality_scores = []
        for item in dataset:
            quality = await self._assess_query_quality(item)
            query_quality_scores.append(quality)

        quality_metrics["average_query_quality"] = np.mean(query_quality_scores)

        # 3. ç­”æ¡ˆå“è³ªæª¢æŸ¥
        if all("expected_answer" in item for item in dataset):
            answer_quality_scores = []
            for item in dataset:
                answer_quality = await self._assess_answer_quality(
                    item["query"], item["expected_answer"]
                )
                answer_quality_scores.append(answer_quality)

            quality_metrics["average_answer_quality"] = np.mean(answer_quality_scores)

        return {
            "overall_quality_score": np.mean(list(quality_metrics.values())),
            "detailed_metrics": quality_metrics,
            "distributions": {
                "category": category_distribution,
                "difficulty": difficulty_distribution
            },
            "dataset_size": len(dataset),
            "quality_grade": self._assign_dataset_grade(np.mean(list(quality_metrics.values())))
        }

    def _calculate_distribution_entropy(self, distribution: Dict[str, int]) -> float:
        """è¨ˆç®—åˆ†ä½ˆçš„ä¿¡æ¯ç†µ"""

        total = sum(distribution.values())
        if total == 0:
            return 0.0

        entropy = 0.0
        for count in distribution.values():
            if count > 0:
                probability = count / total
                entropy -= probability * np.log(probability)

        return entropy

    async def _assess_query_quality(self, query_item: Dict) -> float:
        """è©•ä¼°å–®å€‹æŸ¥è©¢çš„å“è³ª"""

        quality_score = 0.0

        query = query_item.get("query", "")

        # 1. é•·åº¦åˆç†æ€§
        query_length = len(query.split())
        if 3 <= query_length <= 50:
            quality_score += 0.2
        elif query_length > 50:
            quality_score += 0.1

        # 2. èªæ³•æ­£ç¢ºæ€§
        if await self._check_grammar(query):
            quality_score += 0.2

        # 3. æ˜ç¢ºæ€§
        ambiguity_score = await self._calculate_ambiguity(query)
        quality_score += 0.2 * (1.0 - ambiguity_score)

        # 4. å¯å›ç­”æ€§
        answerability = await self._assess_answerability(query)
        quality_score += 0.4 * answerability

        return quality_score
```

---

## 7. æœ¬ç« ç¸½çµèˆ‡å¯¦è¸æŒ‡å—

### 7.1 è©•ä¼°æœ€ä½³å¯¦è¸

#### **è©•ä¼°ç­–ç•¥é¸æ“‡æŒ‡å—**

| è©•ä¼°ç›®æ¨™ | æ¨è–¦æŒ‡æ¨™ | è©•ä¼°é »ç‡ | è‡ªå‹•åŒ–ç¨‹åº¦ |
|---------|---------|---------|-----------|
| **ç³»çµ±èª¿è©¦** | Context Precision/Recall | æ¯æ¬¡æ›´æ–° | å®Œå…¨è‡ªå‹•åŒ– |
| **è³ªé‡ä¿è­‰** | Faithfulness, Answer Relevancy | æ¯æ—¥ | å®Œå…¨è‡ªå‹•åŒ– |
| **ç”¨æˆ¶æ»¿æ„åº¦** | User Satisfaction, Task Success | æ¯é€± | åŠè‡ªå‹•åŒ– |
| **æ¥­å‹™å½±éŸ¿** | Cost per Query, ROI | æ¯æœˆ | äººå·¥åˆ†æ |

#### **è©•ä¼°å·¥å…·éˆæ¨è–¦**

**ç”Ÿç”¢ç’°å¢ƒé…ç½®**:
```yaml
evaluation_stack:
  primary_framework: "RAGAS"
  monitoring_platform: "Opik + LangFuse"
  experimentation: "Custom A/B Testing"
  business_intelligence: "Streamlit + Plotly"

automation_level:
  component_testing: 100%
  system_testing: 90%
  business_evaluation: 60%
  strategic_assessment: 30%
```

### 7.2 æŒçºŒæ”¹é€²å¾ªç’°

**æ”¹é€²å¾ªç’°æ¨¡å‹**:

```
è©•ä¼° â†’ åˆ†æ â†’ å‡è¨­ â†’ å¯¦é©— â†’ é©—è­‰ â†’ éƒ¨ç½² â†’ è©•ä¼°
```

æ¯å€‹å¾ªç’°é€±æœŸå»ºè­°ç‚º2-4é€±ï¼Œç¢ºä¿å¿«é€Ÿè¿­ä»£å’ŒæŒçºŒå„ªåŒ–ã€‚

---

**èª²ç¨‹è©•ä¼°**: æœ¬ç« å…§å®¹åœ¨æœŸæœ«è€ƒè©¦ä¸­å 20%æ¬Šé‡ï¼Œé‡é»è€ƒæŸ¥è©•ä¼°æ¡†æ¶è¨­è¨ˆå’Œçµ±è¨ˆåˆ†æèƒ½åŠ›ã€‚

**é …ç›®è¦æ±‚**: å­¸ç”Ÿéœ€å®Œæˆä¸€å€‹å®Œæ•´çš„ RAG ç³»çµ±è©•ä¼°é …ç›®ï¼ŒåŒ…æ‹¬æ•¸æ“šé›†æ§‹å»ºã€å¯¦é©—è¨­è¨ˆå’Œçµæœåˆ†æã€‚