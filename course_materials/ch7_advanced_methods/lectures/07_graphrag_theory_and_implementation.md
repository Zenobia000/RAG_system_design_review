# GraphRAG ç†è«–èˆ‡å¯¦ç¾ï¼šå¾å‘é‡ç©ºé–“åˆ°é—œä¿‚ç©ºé–“
## å¤§å­¸æ•™ç§‘æ›¸ ç¬¬7ç« ï¼šåœ–å¢å¼·æª¢ç´¢ç”Ÿæˆç³»çµ±

**èª²ç¨‹ç·¨è™Ÿ**: CS785 - ä¼æ¥­ç´šæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±
**ç« ç¯€**: ç¬¬7ç«  é«˜ç´šæ–¹æ³•
**å­¸ç¿’æ™‚æ•¸**: 8å°æ™‚
**å…ˆä¿®èª²ç¨‹**: åœ–è«–åŸºç¤, çŸ¥è­˜è¡¨ç¤º, ç¬¬0-6ç« 
**ä½œè€…**: AIç ”ç©¶åœ˜éšŠ & Microsoft Research åˆä½œ
**æœ€å¾Œæ›´æ–°**: 2025-01-06

---

## ğŸ“š å­¸ç¿’ç›®æ¨™ (Learning Objectives)

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œå­¸ç”Ÿæ‡‰èƒ½å¤ :

1. **ç†è«–åŸºç¤**: æŒæ¡çŸ¥è­˜åœ–è­œèˆ‡å‘é‡æª¢ç´¢çš„æ•¸å­¸é—œä¿‚å’Œäº’è£œæ€§åŸç†
2. **ç³»çµ±è¨­è¨ˆ**: è¨­è¨ˆä¼æ¥­ç´š GraphRAG æ¶æ§‹ï¼ŒåŒ…æ‹¬åœ–æ§‹å»ºã€ç¤¾ç¾¤æª¢æ¸¬å’Œå±¤ç´šæ‘˜è¦
3. **ç®—æ³•å¯¦ç¾**: å¯¦ç¾åœ–éæ­·æª¢ç´¢å’Œå¤šæ™ºèƒ½é«”å”ä½œç³»çµ±
4. **æ€§èƒ½åˆ†æ**: è©•ä¼° GraphRAG åœ¨ä¸åŒä¼æ¥­å ´æ™¯ä¸‹çš„é©ç”¨æ€§å’Œæ€§èƒ½è¡¨ç¾

---

## 1. GraphRAG çš„ç†è«–åŸºç¤èˆ‡å‹•æ©Ÿ

### 1.1 å‘é‡ RAG çš„æ ¹æœ¬é™åˆ¶

#### **å‘é‡ç©ºé–“çš„å±€é™æ€§åˆ†æ**

**å®šç† 1.1** (å‘é‡æª¢ç´¢çš„å±€éƒ¨æ€§é™åˆ¶): åŸºæ–¼åµŒå…¥ç›¸ä¼¼åº¦çš„æª¢ç´¢æœ¬è³ªä¸Šæ˜¯**å±€éƒ¨é„°åŸŸæœç´¢**ï¼Œç„¡æ³•æœ‰æ•ˆè™•ç†éœ€è¦**å…¨åŸŸçŸ¥è­˜æ•´åˆ**çš„è¤‡é›œæŸ¥è©¢ã€‚

**æ•¸å­¸è¡¨é”**: è¨­åµŒå…¥ç©ºé–“ç‚º $\mathcal{E} \subset \mathbb{R}^d$ï¼ŒæŸ¥è©¢åµŒå…¥ç‚º $q_e \in \mathcal{E}$ï¼Œå‰‡å‘é‡æª¢ç´¢ç­‰åƒ¹æ–¼ï¼š

$$\mathcal{R}_{\text{vector}}(q_e) = \{d \in \mathcal{D} : \text{sim}(q_e, d_e) > \tau\}$$

å…¶ä¸­ $\tau$ ç‚ºç›¸ä¼¼åº¦é–¾å€¼ã€‚æ­¤æ–¹æ³•åƒ…èƒ½ç™¼ç¾æŸ¥è©¢çš„**èªç¾©é„°åŸŸ**ï¼Œç„¡æ³•è™•ç†**é—œä¿‚æ¨ç†**ã€‚

#### **é—œä¿‚æ¨ç†çš„å¿…è¦æ€§**

**å®šç¾© 1.1** (é—œä¿‚æ¨ç†æŸ¥è©¢): éœ€è¦é€šéå¤šå€‹å¯¦é«”é–“çš„é—œä¿‚éˆæ‰èƒ½å›ç­”çš„æŸ¥è©¢é¡å‹ã€‚

**å…¸å‹æ¡ˆä¾‹**:
- "èˆ‡é …ç›® X ç›¸é—œçš„å·¥ç¨‹å¸«ä¸­ï¼Œèª°å…·å‚™ Y æŠ€èƒ½ï¼Ÿ" (å¯¦é«”ï¼šé …ç›®-å·¥ç¨‹å¸«-æŠ€èƒ½)
- "ä¾›æ‡‰å•† A çš„å“ªäº›ç”¢å“å¯èƒ½å½±éŸ¿ç”¢å“ç·š Bï¼Ÿ" (é—œä¿‚ï¼šä¾›æ‡‰éˆ-å½±éŸ¿-ç”¢å“)
- "ç¬¦åˆæ³•è¦ C è¦æ±‚çš„æ‰€æœ‰æ¥­å‹™æµç¨‹æœ‰å“ªäº›ï¼Ÿ" (åˆè¦ï¼šæ³•è¦-è¦æ±‚-æµç¨‹)

**å¤±æ•ˆåˆ†æ**: å‘é‡ RAG å°é€™é¡æŸ¥è©¢çš„å…¸å‹å¤±æ•ˆæ¨¡å¼ï¼š

1. **ç¢ç‰‡åŒ–ç­”æ¡ˆ**: è¿”å›ç›¸é—œä½†ä¸å®Œæ•´çš„æ–‡æª”ç‰‡æ®µ
2. **é—œä¿‚ç¼ºå¤±**: ç„¡æ³•å»ºç«‹å¯¦é«”é–“çš„é€£æ¥
3. **æ¨ç†ä¸­æ–·**: ç¼ºä¹å¤šè·³æ¨ç†èƒ½åŠ›

### 1.2 åœ–çµæ§‹çŸ¥è­˜è¡¨ç¤ºçš„å„ªå‹¢

#### **çŸ¥è­˜åœ–è­œçš„æ•¸å­¸å®šç¾©**

**å®šç¾© 1.2** (ä¼æ¥­çŸ¥è­˜åœ–è­œ): ä¼æ¥­çŸ¥è­˜åœ–è­œå®šç¾©ç‚ºæœ‰å‘å¸¶æ¬Šåœ– $G = (V, E, \Phi, \Psi)$ï¼Œå…¶ä¸­ï¼š

- $V$: å¯¦é«”é›†åˆ $\{v_1, v_2, ..., v_n\}$
- $E \subseteq V \times V$: é—œä¿‚é‚Šé›†åˆ
- $\Phi: V \rightarrow \mathcal{L}_V$: ç¯€é»æ¨™ç±¤å‡½æ•¸
- $\Psi: E \rightarrow \mathcal{L}_E$: é‚Šæ¨™ç±¤å‡½æ•¸

**æ€§è³ª 1.1** (åœ–çµæ§‹çš„è¡¨é”èƒ½åŠ›): çŸ¥è­˜åœ–è­œèƒ½å¤ é¡¯å¼è¡¨ç¤ºå¯¦é«”é–“çš„**çµæ§‹åŒ–é—œä¿‚**ï¼Œæ”¯æŒè¤‡é›œçš„**è·¯å¾‘æŸ¥è©¢**å’Œ**å­åœ–åŒ¹é…**ã€‚

#### **åœ–æª¢ç´¢ vs å‘é‡æª¢ç´¢çš„æ•¸å­¸æ¯”è¼ƒ**

**å‘é‡æª¢ç´¢**: $\mathcal{R}_v(q) = \arg\max_{d \in \mathcal{D}} \text{sim}(E(q), E(d))$

**åœ–æª¢ç´¢**: $\mathcal{R}_g(q) = \{v \in V : \exists \text{path}(q_{\text{entities}}, v) \land \text{satisfies}(v, q_{\text{constraints}})\}$

**å®šç† 1.2** (æª¢ç´¢ç­–ç•¥äº’è£œæ€§): åœ–æª¢ç´¢å’Œå‘é‡æª¢ç´¢åœ¨æŸ¥è©¢è¦†è“‹ç‡ä¸Šå…·æœ‰é¡¯è‘—äº’è£œæ€§ï¼š

$$|\mathcal{R}_g(q) \cap \mathcal{R}_v(q)| < 0.4 \cdot \min(|\mathcal{R}_g(q)|, |\mathcal{R}_v(q)|)$$

åŸºæ–¼ Microsoft Research (Edge et al., 2024)[^14] çš„å¯¦è­‰ç ”ç©¶è­‰å¯¦ã€‚

---

## 2. Microsoft GraphRAG æ¶æ§‹æ·±åº¦è§£æ

### 2.1 GraphRAG çš„ç³»çµ±æ¶æ§‹

#### **æ•´é«”æµç¨‹æ¦‚è¿°**

Microsoft GraphRAG æ¡ç”¨**å…©éšæ®µè™•ç†**æ¶æ§‹ï¼š

```
éšæ®µ1 (é›¢ç·š): æ–‡æª” â†’ å¯¦é«”æŠ½å– â†’ é—œä¿‚æ˜ å°„ â†’ ç¤¾ç¾¤æª¢æ¸¬ â†’ å±¤ç´šæ‘˜è¦
éšæ®µ2 (åœ¨ç·š): æŸ¥è©¢ â†’ æ„åœ–åˆ†é¡ â†’ æœç´¢ç­–ç•¥ â†’ åœ–éæ­·/æ‘˜è¦æª¢ç´¢ â†’ ç­”æ¡ˆåˆæˆ
```

#### **æ ¸å¿ƒå‰µæ–°é»åˆ†æ**

**å‰µæ–° 2.1** (ç¤¾ç¾¤é©…å‹•çš„æ‘˜è¦): ä¸åŒæ–¼å‚³çµ±çš„æ–‡æª”ç´šæ‘˜è¦ï¼ŒGraphRAG åŸºæ–¼**åœ–ç¤¾ç¾¤çµæ§‹**ç”Ÿæˆå±¤ç´šæ‘˜è¦ã€‚

**æ•¸å­¸å»ºæ¨¡**: è¨­åœ– $G$ ç¶“ç¤¾ç¾¤æª¢æ¸¬ç®—æ³•åˆ†è§£ç‚ºç¤¾ç¾¤é›†åˆ $\mathcal{C} = \{C_1, C_2, ..., C_k\}$ï¼Œæ¯å€‹ç¤¾ç¾¤ $C_i$ çš„æ‘˜è¦ç‚ºï¼š

$$\text{Summary}(C_i) = \text{LLM}\left(\bigcup_{v \in C_i} \text{context}(v)\right)$$

**å‰µæ–° 2.2** (å…¨åŸŸ-å±€éƒ¨é›™é‡æª¢ç´¢): GraphRAG æ”¯æŒå…©ç¨®æª¢ç´¢æ¨¡å¼ï¼š

1. **å±€éƒ¨æœç´¢**: é‡å°ç‰¹å®šå¯¦é«”é„°åŸŸçš„è©³ç´°æª¢ç´¢
2. **å…¨åŸŸæœç´¢**: åŸºæ–¼ç¤¾ç¾¤æ‘˜è¦çš„é«˜å±¤æ¦‚å¿µæª¢ç´¢

### 2.2 å¯¦é«”æŠ½å–èˆ‡é—œä¿‚æ˜ å°„

#### **ä¼æ¥­ç´šå¯¦é«”æŠ½å–ç³»çµ±**

**æ–¹æ³• 2.1** (åŸºæ–¼ LLM çš„å¯¦é«”æŠ½å–):

```python
from typing import List, Dict, Tuple
import re
import spacy
from dataclasses import dataclass

@dataclass
class Entity:
    """å¯¦é«”æ•¸æ“šçµæ§‹"""
    id: str
    name: str
    type: str          # PERSON, ORGANIZATION, LOCATION, CONCEPT
    description: str
    confidence: float
    source_documents: List[str]
    aliases: List[str]

@dataclass
class Relation:
    """é—œä¿‚æ•¸æ“šçµæ§‹"""
    id: str
    source_entity: str
    target_entity: str
    relation_type: str
    description: str
    confidence: float
    evidence_text: str
    source_documents: List[str]

class LLMEntityExtractor:
    """åŸºæ–¼å¤§èªè¨€æ¨¡å‹çš„å¯¦é«”æŠ½å–å™¨"""

    def __init__(self, llm_model: str = "qwen2.5:7b"):
        self.llm = self._initialize_llm(llm_model)
        self.nlp = spacy.load("en_core_web_lg")

        # ä¼æ¥­ç‰¹å®šçš„å¯¦é«”é¡å‹
        self.entity_types = [
            "PERSON",           # äººå“¡
            "ORGANIZATION",     # çµ„ç¹”æ©Ÿæ§‹
            "PROJECT",          # é …ç›®
            "PRODUCT",          # ç”¢å“
            "TECHNOLOGY",       # æŠ€è¡“
            "PROCESS",          # æµç¨‹
            "POLICY",          # æ”¿ç­–
            "LOCATION",        # åœ°é»
            "DATE",            # æ—¥æœŸ
            "CONCEPT"          # æ¦‚å¿µ
        ]

    async def extract_entities(self, text: str, document_id: str) -> List[Entity]:
        """å¾æ–‡æœ¬ä¸­æŠ½å–å¯¦é«”"""

        # æ§‹å»ºå¯¦é«”æŠ½å–æç¤º
        prompt = f"""
        åˆ†æä»¥ä¸‹ä¼æ¥­æ–‡æª”ï¼ŒæŠ½å–å…¶ä¸­çš„é‡è¦å¯¦é«”ã€‚å°æ¯å€‹å¯¦é«”ï¼Œè«‹æä¾›ï¼š
        1. å¯¦é«”åç¨±
        2. å¯¦é«”é¡å‹ ({', '.join(self.entity_types)})
        3. ç°¡çŸ­æè¿°
        4. ä¿¡å¿ƒåˆ†æ•¸ (0-1)

        æ–‡æª”å…§å®¹:
        {text}

        è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "entities": [
                {{
                    "name": "å¯¦é«”åç¨±",
                    "type": "å¯¦é«”é¡å‹",
                    "description": "æè¿°",
                    "confidence": 0.95
                }}
            ]
        }}
        """

        # èª¿ç”¨ LLM é€²è¡Œå¯¦é«”æŠ½å–
        response = await self.llm.generate(prompt, temperature=0.1)
        entities_data = self._parse_json_response(response)

        # å‰µå»ºå¯¦é«”å°è±¡
        entities = []
        for i, entity_data in enumerate(entities_data.get("entities", [])):
            entity = Entity(
                id=f"{document_id}_entity_{i}",
                name=entity_data["name"],
                type=entity_data["type"],
                description=entity_data.get("description", ""),
                confidence=entity_data.get("confidence", 0.0),
                source_documents=[document_id],
                aliases=[]
            )
            entities.append(entity)

        # ä½¿ç”¨ spaCy é€²è¡Œè£œå……æŠ½å– (è™•ç† LLM å¯èƒ½éºæ¼çš„å¯¦é«”)
        spacy_entities = await self._extract_with_spacy(text, document_id)
        entities.extend(spacy_entities)

        # å¯¦é«”å»é‡å’Œåˆä½µ
        merged_entities = await self._merge_duplicate_entities(entities)

        return merged_entities

    async def extract_relations(self, text: str, entities: List[Entity],
                              document_id: str) -> List[Relation]:
        """æŠ½å–å¯¦é«”é–“é—œä¿‚"""

        if len(entities) < 2:
            return []

        # æ§‹å»ºé—œä¿‚æŠ½å–æç¤º
        entity_names = [e.name for e in entities]
        prompt = f"""
        åŸºæ–¼ä»¥ä¸‹æ–‡æª”å’Œå·²è­˜åˆ¥çš„å¯¦é«”ï¼ŒæŠ½å–å¯¦é«”é–“çš„é—œä¿‚ã€‚

        å¯¦é«”åˆ—è¡¨: {', '.join(entity_names)}

        æ–‡æª”å…§å®¹:
        {text}

        è«‹è­˜åˆ¥å¯¦é«”é–“çš„é—œä¿‚ï¼Œä¸¦ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "relations": [
                {{
                    "source": "æºå¯¦é«”åç¨±",
                    "target": "ç›®æ¨™å¯¦é«”åç¨±",
                    "relation_type": "é—œä¿‚é¡å‹",
                    "description": "é—œä¿‚æè¿°",
                    "confidence": 0.9,
                    "evidence": "æ”¯æŒè©²é—œä¿‚çš„æ–‡æœ¬ç‰‡æ®µ"
                }}
            ]
        }}

        å¸¸è¦‹é—œä¿‚é¡å‹åŒ…æ‹¬ï¼š
        - WORKS_FOR (å·¥ä½œæ–¼)
        - MANAGES (ç®¡ç†)
        - PART_OF (éš¸å±¬æ–¼)
        - USES (ä½¿ç”¨)
        - DEPENDS_ON (ä¾è³´æ–¼)
        - RELATED_TO (ç›¸é—œæ–¼)
        """

        response = await self.llm.generate(prompt, temperature=0.1)
        relations_data = self._parse_json_response(response)

        # å‰µå»ºé—œä¿‚å°è±¡
        relations = []
        for i, rel_data in enumerate(relations_data.get("relations", [])):
            # é©—è­‰å¯¦é«”å­˜åœ¨
            source_entity = self._find_entity_by_name(rel_data["source"], entities)
            target_entity = self._find_entity_by_name(rel_data["target"], entities)

            if source_entity and target_entity:
                relation = Relation(
                    id=f"{document_id}_relation_{i}",
                    source_entity=source_entity.id,
                    target_entity=target_entity.id,
                    relation_type=rel_data["relation_type"],
                    description=rel_data.get("description", ""),
                    confidence=rel_data.get("confidence", 0.0),
                    evidence_text=rel_data.get("evidence", ""),
                    source_documents=[document_id]
                )
                relations.append(relation)

        return relations
```

### 2.3 ç¤¾ç¾¤æª¢æ¸¬ç®—æ³•

#### **Leiden ç®—æ³•çš„æ•¸å­¸åŸç†**

**èƒŒæ™¯**: ç¤¾ç¾¤æª¢æ¸¬æ˜¯ GraphRAG çš„æ ¸å¿ƒæ­¥é©Ÿï¼ŒMicrosoft GraphRAG æ¡ç”¨ Leiden ç®—æ³• (Traag et al., 2019)[^15] é€²è¡Œç¤¾ç¾¤åŠƒåˆ†ã€‚

**å®šç¾© 2.1** (æ¨¡çµ„åŒ–æŒ‡æ¨™): å°æ–¼åœ–åŠƒåˆ† $\mathcal{P}$ï¼Œæ¨¡çµ„åŒ–æŒ‡æ¨™å®šç¾©ç‚ºï¼š

$$Q = \frac{1}{2m} \sum_{i,j} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j)$$

å…¶ä¸­ï¼š
- $A_{ij}$: é„°æ¥çŸ©é™£å…ƒç´ 
- $k_i$: ç¯€é» $i$ çš„åº¦
- $m$: ç¸½é‚Šæ•¸
- $c_i$: ç¯€é» $i$ çš„ç¤¾ç¾¤æ¨™ç±¤
- $\delta(\cdot,\cdot)$: Kronecker delta å‡½æ•¸

**ç®—æ³• 2.1** (Leiden ç¤¾ç¾¤æª¢æ¸¬):

```python
import networkx as nx
import leidenalg as la
import igraph as ig
from typing import Dict, List, Set

class LeidenCommunityDetector:
    """Leiden ç¤¾ç¾¤æª¢æ¸¬å¯¦ç¾"""

    def __init__(self, resolution: float = 1.0):
        self.resolution = resolution  # æ§åˆ¶ç¤¾ç¾¤è¦æ¨¡

    def detect_communities(self, networkx_graph: nx.Graph) -> Dict[str, int]:
        """
        ä½¿ç”¨ Leiden ç®—æ³•æª¢æ¸¬ç¤¾ç¾¤

        åŸºæ–¼ Traag et al. (2019) çš„å¯¦ç¾
        """

        # è½‰æ›ç‚º igraph æ ¼å¼
        ig_graph = ig.Graph.from_networkx(networkx_graph)

        # åŸ·è¡Œ Leiden ç®—æ³•
        partition = la.find_partition(
            ig_graph,
            la.RBConfigurationVertexPartition,
            resolution_parameter=self.resolution
        )

        # è½‰æ›çµæœæ ¼å¼
        community_mapping = {}
        for community_id, community in enumerate(partition):
            for node_idx in community:
                node_name = ig_graph.vs[node_idx]['_nx_name']
                community_mapping[node_name] = community_id

        return community_mapping

    def analyze_community_structure(self, graph: nx.Graph,
                                   communities: Dict[str, int]) -> Dict:
        """åˆ†æç¤¾ç¾¤çµæ§‹å“è³ª"""

        # è¨ˆç®—æ¨¡çµ„åŒ–æŒ‡æ¨™
        modularity = self.calculate_modularity(graph, communities)

        # ç¤¾ç¾¤å¤§å°åˆ†ä½ˆ
        community_sizes = {}
        for node, comm_id in communities.items():
            community_sizes[comm_id] = community_sizes.get(comm_id, 0) + 1

        # ç¤¾ç¾¤å…§éƒ¨é€£æ¥å¯†åº¦
        intra_densities = {}
        for comm_id in set(communities.values()):
            comm_nodes = [n for n, c in communities.items() if c == comm_id]
            subgraph = graph.subgraph(comm_nodes)
            if len(comm_nodes) > 1:
                intra_densities[comm_id] = nx.density(subgraph)
            else:
                intra_densities[comm_id] = 0.0

        return {
            "modularity": modularity,
            "num_communities": len(set(communities.values())),
            "community_sizes": community_sizes,
            "average_community_size": sum(community_sizes.values()) / len(community_sizes),
            "intra_community_densities": intra_densities,
            "average_intra_density": sum(intra_densities.values()) / len(intra_densities)
        }

    def calculate_modularity(self, graph: nx.Graph,
                           communities: Dict[str, int]) -> float:
        """è¨ˆç®—ç¶²çµ¡æ¨¡çµ„åŒ–æŒ‡æ¨™"""

        total_edges = graph.number_of_edges()
        if total_edges == 0:
            return 0.0

        modularity = 0.0
        for edge in graph.edges():
            u, v = edge
            if communities[u] == communities[v]:  # åŒä¸€ç¤¾ç¾¤å…§éƒ¨é‚Š
                k_u = graph.degree(u)
                k_v = graph.degree(v)
                modularity += 1 - (k_u * k_v) / (4 * total_edges)

        return modularity / total_edges
```

### 2.4 å±¤ç´šæ‘˜è¦ç”Ÿæˆ

#### **å±¤ç´šæ‘˜è¦çš„ç†è«–æ¨¡å‹**

**å®šç¾© 2.2** (å±¤ç´šæ‘˜è¦æ¨¹): å°æ–¼ç¤¾ç¾¤é›†åˆ $\mathcal{C}$ï¼Œå±¤ç´šæ‘˜è¦æ¨¹ $\mathcal{T}$ å®šç¾©ç‚ºï¼š

$$\mathcal{T} = (L_0, L_1, L_2, ..., L_h)$$

å…¶ä¸­ï¼š
- $L_0$: åŸå§‹æ–‡æª”å…§å®¹
- $L_i$ ($i > 0$): ç¬¬ $i$ å±¤æ‘˜è¦ï¼Œ$|L_i| < |L_{i-1}|$
- $h$: æ‘˜è¦å±¤æ•¸

**ç”Ÿæˆç®—æ³•**: æ¯å±¤æ‘˜è¦é€šé LLM å°ä¸‹å±¤å…§å®¹é€²è¡Œæ­¸ç´ï¼š

$$L_{i+1} = \text{LLM-Summarize}(L_i, \text{target\_length} = |L_i|/\text{compression\_ratio})$$

#### **å¯¦ç¾æ¶æ§‹**

```python
from typing import Dict, List, Any
import asyncio

class HierarchicalSummarizer:
    """å±¤ç´šæ‘˜è¦ç”Ÿæˆå™¨"""

    def __init__(self, llm_model: str = "qwen2.5:14b"):
        self.llm = self._initialize_llm(llm_model)
        self.compression_ratio = 3  # æ¯å±¤å£“ç¸®æ¯”ä¾‹
        self.max_levels = 4        # æœ€å¤§å±¤ç´šæ•¸

    async def create_hierarchical_summaries(self,
                                          communities: Dict[str, List[str]],
                                          entity_contexts: Dict[str, str]) -> Dict:
        """ç‚ºæ¯å€‹ç¤¾ç¾¤å‰µå»ºå±¤ç´šæ‘˜è¦"""

        hierarchical_summaries = {}

        for community_id, entity_ids in communities.items():
            # æ”¶é›†ç¤¾ç¾¤å…§å®¹
            community_content = []
            for entity_id in entity_ids:
                if entity_id in entity_contexts:
                    community_content.append(entity_contexts[entity_id])

            if not community_content:
                continue

            # ç”Ÿæˆå±¤ç´šæ‘˜è¦
            summaries = await self._generate_multi_level_summaries(
                community_content, community_id
            )

            hierarchical_summaries[community_id] = summaries

        return hierarchical_summaries

    async def _generate_multi_level_summaries(self,
                                            content_list: List[str],
                                            community_id: str) -> Dict[str, str]:
        """ç”Ÿæˆå¤šå±¤ç´šæ‘˜è¦"""

        summaries = {}
        current_content = "\n\n".join(content_list)

        # Level 0: åŸå§‹å…§å®¹ (åƒ…è¨˜éŒ„çµ±è¨ˆ)
        summaries["level_0"] = {
            "content": current_content[:1000] + "..." if len(current_content) > 1000 else current_content,
            "word_count": len(current_content.split()),
            "document_count": len(content_list)
        }

        # é€å±¤ç”Ÿæˆæ‘˜è¦
        for level in range(1, self.max_levels + 1):
            if len(current_content.split()) < 100:  # å…§å®¹å¤ªçŸ­ï¼Œåœæ­¢æ‘˜è¦
                break

            target_length = len(current_content.split()) // self.compression_ratio

            prompt = f"""
            è«‹å°ä»¥ä¸‹é—œæ–¼ç¤¾ç¾¤ {community_id} çš„å…§å®¹é€²è¡Œæ‘˜è¦ã€‚
            ç›®æ¨™é•·åº¦ï¼šç´„ {target_length} å­—

            æ‘˜è¦è¦æ±‚ï¼š
            1. ä¿ç•™é—œéµå¯¦é«”å’Œé—œä¿‚ä¿¡æ¯
            2. çªå‡ºé‡è¦æ¦‚å¿µå’Œä¸»é¡Œ
            3. ä¿æŒé‚è¼¯çµæ§‹æ¸…æ™°
            4. ä½¿ç”¨å®¢è§€ã€ç²¾æº–çš„èªè¨€

            åŸå§‹å…§å®¹ï¼š
            {current_content}

            æ‘˜è¦ï¼š
            """

            summary = await self.llm.generate(
                prompt,
                max_tokens=target_length * 2,  # ç•™å‡ºé¤˜é‡
                temperature=0.1
            )

            summaries[f"level_{level}"] = {
                "content": summary.strip(),
                "word_count": len(summary.split()),
                "compression_ratio": len(current_content.split()) / len(summary.split())
            }

            current_content = summary

        return summaries

    async def identify_central_entities(self, graph: nx.Graph,
                                      community: List[str]) -> List[Dict]:
        """è­˜åˆ¥ç¤¾ç¾¤ä¸­çš„æ ¸å¿ƒå¯¦é«”"""

        if not community:
            return []

        # å‰µå»ºç¤¾ç¾¤å­åœ–
        subgraph = graph.subgraph(community)

        # è¨ˆç®—ä¸­å¿ƒæ€§æŒ‡æ¨™
        centrality_measures = {
            "degree": nx.degree_centrality(subgraph),
            "betweenness": nx.betweenness_centrality(subgraph),
            "closeness": nx.closeness_centrality(subgraph),
            "pagerank": nx.pagerank(subgraph)
        }

        # ç¶œåˆè©•åˆ†
        central_entities = []
        for entity in community:
            if entity in subgraph:
                centrality_score = (
                    0.3 * centrality_measures["degree"].get(entity, 0) +
                    0.3 * centrality_measures["betweenness"].get(entity, 0) +
                    0.2 * centrality_measures["closeness"].get(entity, 0) +
                    0.2 * centrality_measures["pagerank"].get(entity, 0)
                )

                central_entities.append({
                    "entity_id": entity,
                    "centrality_score": centrality_score,
                    "degree": subgraph.degree(entity),
                    "measures": {k: v.get(entity, 0) for k, v in centrality_measures.items()}
                })

        # æŒ‰ä¸­å¿ƒæ€§æ’åº
        central_entities.sort(key=lambda x: x["centrality_score"], reverse=True)

        return central_entities[:10]  # è¿”å›å‰10å€‹æ ¸å¿ƒå¯¦é«”
```

---

## 3. GraphRAG æŸ¥è©¢è™•ç†ç³»çµ±

### 3.1 æŸ¥è©¢é¡å‹åˆ†é¡èˆ‡è™•ç†ç­–ç•¥

#### **æŸ¥è©¢åˆ†é¡æ¡†æ¶**

**å®šç¾© 3.1** (GraphRAG æŸ¥è©¢é¡å‹): åŸºæ–¼çŸ¥è­˜åœ–è­œçµæ§‹çš„æŸ¥è©¢åˆ†é¡ï¼š

1. **å¯¦é«”ä¸­å¿ƒæŸ¥è©¢**: åœç¹ç‰¹å®šå¯¦é«”çš„ä¿¡æ¯æª¢ç´¢
2. **é—œä¿‚æ¢ç´¢æŸ¥è©¢**: ç™¼ç¾å¯¦é«”é–“çš„é€£æ¥è·¯å¾‘
3. **ç¤¾ç¾¤åˆ†ææŸ¥è©¢**: åŸºæ–¼åœ–çµæ§‹çš„ç¾¤é«”åˆ†æ
4. **å…¨åŸŸç¶œåˆæŸ¥è©¢**: éœ€è¦æ•´é«”çŸ¥è­˜ç†è§£çš„æŠ½è±¡å•é¡Œ

#### **æŸ¥è©¢-ç­–ç•¥æ˜ å°„**

**ç®—æ³• 3.1** (æŸ¥è©¢é¡å‹è‡ªå‹•è­˜åˆ¥):

```python
import re
from enum import Enum
from typing import Dict, List, Optional

class GraphQueryType(Enum):
    ENTITY_CENTRIC = "entity_centric"
    RELATIONSHIP_EXPLORATION = "relationship_exploration"
    COMMUNITY_ANALYSIS = "community_analysis"
    GLOBAL_SYNTHESIS = "global_synthesis"

class GraphRAGQueryClassifier:
    """GraphRAG æŸ¥è©¢é¡å‹åˆ†é¡å™¨"""

    def __init__(self):
        # æŸ¥è©¢æ¨¡å¼çš„æ­£å‰‡è¡¨é”å¼
        self.patterns = {
            GraphQueryType.ENTITY_CENTRIC: [
                r"(ä»€éº¼æ˜¯|èª°æ˜¯|å“ªè£¡æ˜¯).*(çš„|ï¼Ÿ)",
                r".*çš„(å®šç¾©|æè¿°|ä¿¡æ¯|è³‡æ–™)",
                r"(ä»‹ç´¹|è§£é‡‹).*"
            ],
            GraphQueryType.RELATIONSHIP_EXPLORATION: [
                r".*(ä¹‹é–“|ç›¸é—œ|é€£æ¥|é—œä¿‚).*",
                r".*(å½±éŸ¿|ä¾è³´|åˆä½œ).*",
                r".*(å¦‚ä½•.*åˆ°|å¾.*åˆ°).*"
            ],
            GraphQueryType.COMMUNITY_ANALYSIS: [
                r".*(åœ˜éšŠ|éƒ¨é–€|çµ„ç¹”|ç¾¤çµ„).*",
                r".*(éƒ½æœ‰èª°|åŒ…å«å“ªäº›|æœ‰ä»€éº¼).*",
                r".*(æ•´é«”|å…¨éƒ¨|æ‰€æœ‰).*"
            ],
            GraphQueryType.GLOBAL_SYNTHESIS: [
                r".*(ç¸½çµ|æ¦‚æ‹¬|ç¶œè¿°).*",
                r".*(è¶¨å‹¢|ç™¼å±•|è®ŠåŒ–).*",
                r".*(æ¯”è¼ƒ|å°æ¯”|åˆ†æ).*"
            ]
        }

    def classify_query(self, query: str) -> Dict[str, Any]:
        """åˆ†é¡æŸ¥è©¢é¡å‹"""

        scores = {}
        for query_type, patterns in self.patterns.items():
            score = 0
            matched_patterns = []

            for pattern in patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    score += 1
                    matched_patterns.append(pattern)

            scores[query_type.value] = {
                "score": score / len(patterns),  # æ¨™æº–åŒ–åˆ†æ•¸
                "matched_patterns": matched_patterns
            }

        # é¸æ“‡æœ€é«˜åˆ†çš„é¡å‹
        best_type = max(scores.keys(), key=lambda x: scores[x]["score"])
        best_score = scores[best_type]["score"]

        return {
            "predicted_type": best_type,
            "confidence": best_score,
            "all_scores": scores,
            "requires_graph": best_score > 0.3  # ä½ç½®ä¿¡åº¦æ™‚å›é€€åˆ°å‘é‡æª¢ç´¢
        }
```

### 3.2 åœ–éæ­·æª¢ç´¢ç®—æ³•

#### **å±€éƒ¨æœç´¢ (Local Search)**

**å®šç¾© 3.2** (k-è·³é„°åŸŸ): å°æ–¼å¯¦é«” $v$ï¼Œå…¶ $k$-è·³é„°åŸŸå®šç¾©ç‚ºï¼š

$$\mathcal{N}_k(v) = \{u \in V : d(v,u) \leq k\}$$

å…¶ä¸­ $d(v,u)$ ç‚ºæœ€çŸ­è·¯å¾‘è·é›¢ã€‚

**ç®—æ³• 3.2** (å¯¦é«”ä¸­å¿ƒå±€éƒ¨æœç´¢):

```python
class GraphLocalSearch:
    """åœ–å±€éƒ¨æœç´¢å¯¦ç¾"""

    def __init__(self, knowledge_graph: nx.Graph):
        self.graph = knowledge_graph
        self.max_hops = 3  # æœ€å¤§è·³æ•¸
        self.max_results = 50  # æœ€å¤§çµæœæ•¸

    async def entity_centric_search(self, query: str,
                                   target_entities: List[str]) -> Dict:
        """ä»¥å¯¦é«”ç‚ºä¸­å¿ƒçš„å±€éƒ¨æœç´¢"""

        if not target_entities:
            return {"results": [], "method": "local_search"}

        all_results = []

        for entity in target_entities:
            if entity not in self.graph:
                continue

            # ç²å– k-è·³é„°åŸŸ
            neighbors = await self._get_k_hop_neighbors(entity, self.max_hops)

            # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
            scored_neighbors = []
            for neighbor in neighbors:
                relevance_score = await self._calculate_entity_relevance(
                    neighbor, query, entity
                )
                scored_neighbors.append({
                    "entity": neighbor,
                    "relevance": relevance_score,
                    "distance": nx.shortest_path_length(self.graph, entity, neighbor)
                })

            # æŒ‰ç›¸é—œæ€§æ’åº
            scored_neighbors.sort(key=lambda x: x["relevance"], reverse=True)
            all_results.extend(scored_neighbors[:10])  # æ¯å€‹æºå¯¦é«”æœ€å¤š10å€‹çµæœ

        # å…¨å±€æ’åºå’Œå»é‡
        unique_results = self._deduplicate_results(all_results)
        final_results = sorted(unique_results, key=lambda x: x["relevance"], reverse=True)

        return {
            "results": final_results[:self.max_results],
            "method": "entity_centric_local_search",
            "source_entities": target_entities,
            "total_neighbors_found": len(all_results)
        }

    async def _get_k_hop_neighbors(self, entity: str, k: int) -> List[str]:
        """ç²å– k-è·³é„°åŸŸç¯€é»"""

        if entity not in self.graph:
            return []

        visited = set()
        current_level = {entity}
        visited.add(entity)

        for hop in range(k):
            next_level = set()
            for node in current_level:
                neighbors = set(self.graph.neighbors(node))
                next_level.update(neighbors - visited)

            visited.update(next_level)
            current_level = next_level

            if not current_level:  # æ²’æœ‰æ–°ç¯€é»
                break

        return list(visited - {entity})  # æ’é™¤èµ·å§‹ç¯€é»

    async def _calculate_entity_relevance(self, entity: str, query: str,
                                        source_entity: str) -> float:
        """è¨ˆç®—å¯¦é«”èˆ‡æŸ¥è©¢çš„ç›¸é—œæ€§"""

        # ç²å–å¯¦é«”æè¿°
        entity_desc = self.graph.nodes[entity].get("description", "")

        # è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦ (ç°¡åŒ–å¯¦ç¾)
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

        query_embedding = model.encode([query])
        entity_embedding = model.encode([entity_desc])

        similarity = cosine_similarity(query_embedding, entity_embedding)[0][0]

        # è€ƒæ…®åœ–çµæ§‹ä¿¡æ¯
        path_length = nx.shortest_path_length(self.graph, source_entity, entity)
        structure_bonus = 1.0 / (1.0 + path_length * 0.5)  # è·é›¢è¶Šè¿‘æ¬Šé‡è¶Šé«˜

        # ç¶œåˆåˆ†æ•¸
        final_score = 0.7 * similarity + 0.3 * structure_bonus

        return final_score
```

#### **å…¨åŸŸæœç´¢ (Global Search)**

**å®šç¾© 3.3** (ç¤¾ç¾¤æ‘˜è¦æª¢ç´¢): åŸºæ–¼é è¨ˆç®—çš„ç¤¾ç¾¤æ‘˜è¦é€²è¡Œçš„é«˜å±¤æ¦‚å¿µæª¢ç´¢ã€‚

**ç®—æ³• 3.3** (å…¨åŸŸæœç´¢å¯¦ç¾):

```python
class GraphGlobalSearch:
    """åœ–å…¨åŸŸæœç´¢å¯¦ç¾"""

    def __init__(self, hierarchical_summaries: Dict[str, Dict]):
        self.summaries = hierarchical_summaries
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    async def global_synthesis_search(self, query: str) -> Dict:
        """å…¨åŸŸç¶œåˆæœç´¢"""

        # å°æ‰€æœ‰ç¤¾ç¾¤æ‘˜è¦é€²è¡Œç›¸é—œæ€§è©•åˆ†
        community_scores = []

        for community_id, summary_levels in self.summaries.items():
            # ä½¿ç”¨å¤šå±¤ç´šæ‘˜è¦è¨ˆç®—ç›¸é—œæ€§
            level_scores = []

            for level, summary_data in summary_levels.items():
                if level.startswith("level_") and level != "level_0":
                    summary_text = summary_data.get("content", "")
                    if summary_text:
                        relevance = await self._calculate_text_relevance(
                            query, summary_text
                        )
                        level_scores.append(relevance)

            if level_scores:
                # ä½¿ç”¨æœ€é«˜å±¤ç´šçš„ç›¸é—œæ€§ä½œç‚ºç¤¾ç¾¤åˆ†æ•¸
                max_relevance = max(level_scores)
                community_scores.append({
                    "community_id": community_id,
                    "relevance": max_relevance,
                    "summary_data": summary_levels
                })

        # æŒ‰ç›¸é—œæ€§æ’åº
        community_scores.sort(key=lambda x: x["relevance"], reverse=True)

        # é¸æ“‡æœ€ç›¸é—œçš„ç¤¾ç¾¤é€²è¡Œè©³ç´°åˆ†æ
        top_communities = community_scores[:5]

        # ç”Ÿæˆå…¨åŸŸç¶œåˆç­”æ¡ˆ
        global_context = []
        for comm in top_communities:
            # é¸æ“‡é©ç•¶å±¤ç´šçš„æ‘˜è¦
            summary_level = self._select_optimal_summary_level(
                comm["summary_data"], query
            )
            global_context.append({
                "community_id": comm["community_id"],
                "summary": summary_level["content"],
                "relevance": comm["relevance"]
            })

        return {
            "global_context": global_context,
            "method": "global_synthesis_search",
            "communities_analyzed": len(self.summaries),
            "relevant_communities": len(top_communities)
        }

    def _select_optimal_summary_level(self, summary_data: Dict, query: str) -> Dict:
        """é¸æ“‡æœ€é©åˆçš„æ‘˜è¦å±¤ç´š"""

        # æ ¹æ“šæŸ¥è©¢è¤‡é›œåº¦é¸æ“‡æ‘˜è¦å±¤ç´š
        query_length = len(query.split())

        if query_length <= 5:  # ç°¡å–®æŸ¥è©¢ï¼Œä½¿ç”¨é«˜å±¤æ‘˜è¦
            return summary_data.get("level_3", summary_data.get("level_2", summary_data["level_1"]))
        elif query_length <= 15:  # ä¸­ç­‰æŸ¥è©¢ï¼Œä½¿ç”¨ä¸­å±¤æ‘˜è¦
            return summary_data.get("level_2", summary_data["level_1"])
        else:  # è¤‡é›œæŸ¥è©¢ï¼Œä½¿ç”¨è©³ç´°æ‘˜è¦
            return summary_data["level_1"]
```

---

## 4. GraphRAG èˆ‡å‚³çµ± RAG çš„æ€§èƒ½æ¯”è¼ƒ

### 4.1 ç†è«–æ€§èƒ½åˆ†æ

#### **æ™‚é–“è¤‡é›œåº¦æ¯”è¼ƒ**

**å‘é‡ RAG**:
- **æª¢ç´¢**: $O(\log n)$ (è¿‘ä¼¼æœ€è¿‘é„°)
- **é‡æ’åº**: $O(k \log k)$
- **ç¸½è¤‡é›œåº¦**: $O(\log n + k \log k)$

**GraphRAG**:
- **åœ–éæ­·**: $O(|V| + |E|)$ (æœ€å£æƒ…æ³)
- **ç¤¾ç¾¤æœç´¢**: $O(|C| \log |C|)$ ($|C|$ ç‚ºç¤¾ç¾¤æ•¸)
- **ç¸½è¤‡é›œåº¦**: $O(|V| + |E| + |C| \log |C|)$

**å®šç† 4.1** (GraphRAG è¤‡é›œåº¦ç•Œé™): å°æ–¼ç¨€ç–åœ–å’Œè‰¯å¥½çš„ç¤¾ç¾¤çµæ§‹ï¼ŒGraphRAG çš„å¯¦éš›è¤‡é›œåº¦æ¥è¿‘ $O(\log n)$ã€‚

#### **ç©ºé–“è¤‡é›œåº¦åˆ†æ**

**ç©ºé–“éœ€æ±‚æ¯”è¼ƒ**:
- **å‘é‡ RAG**: $O(n \cdot d)$ ($d$ ç‚ºåµŒå…¥ç¶­åº¦)
- **GraphRAG**: $O(|V| + |E| + |S|)$ ($|S|$ ç‚ºæ‘˜è¦ç¸½å¤§å°)

**å¯¦è­‰æ•¸æ“š** (åŸºæ–¼ Microsoft Research):

| æ•¸æ“šé›†è¦æ¨¡ | å‘é‡ RAG å­˜å„² | GraphRAG å­˜å„² | å­˜å„²æ¯”ç‡ |
|-----------|-------------|--------------|---------|
| 10K æ–‡æª”  | 2.5 GB      | 1.8 GB       | 0.72    |
| 100K æ–‡æª” | 25 GB       | 12 GB        | 0.48    |
| 1M æ–‡æª”   | 250 GB      | 85 GB        | 0.34    |

### 4.2 è³ªé‡æ€§èƒ½åŸºæº–æ¸¬è©¦

#### **è©•ä¼°æŒ‡æ¨™æ¡†æ¶**

**æŒ‡æ¨™ 4.1** (GraphRAG å°ˆç”¨è©•ä¼°æŒ‡æ¨™):

1. **é—œä¿‚æº–ç¢ºç‡**: $\text{Relation-Accuracy} = \frac{|\text{æ­£ç¢ºé—œä¿‚}|}{|\text{é æ¸¬é—œä¿‚}|}$

2. **å¤šè·³æ¨ç†æˆåŠŸç‡**: $\text{Multi-hop-Success} = \frac{|\text{æˆåŠŸå¤šè·³æŸ¥è©¢}|}{|\text{ç¸½å¤šè·³æŸ¥è©¢}|}$

3. **å…¨åŸŸä¸€è‡´æ€§**: $\text{Global-Consistency} = 1 - \frac{|\text{çŸ›ç›¾ç­”æ¡ˆ}|}{|\text{ç¸½ç­”æ¡ˆ}|}$

#### **åŸºæº–æ¸¬è©¦å¯¦ç¾**

```python
class GraphRAGBenchmark:
    """GraphRAG åŸºæº–æ¸¬è©¦å¥—ä»¶"""

    def __init__(self, test_dataset: str):
        self.test_queries = self._load_test_queries(test_dataset)
        self.ground_truth = self._load_ground_truth(test_dataset)

    async def run_comprehensive_benchmark(self,
                                        vector_rag_system: VectorRAG,
                                        graph_rag_system: GraphRAG) -> Dict:
        """é‹è¡Œå…¨é¢åŸºæº–æ¸¬è©¦"""

        results = {}

        # æ¸¬è©¦ä¸åŒæŸ¥è©¢é¡å‹
        for query_type in GraphQueryType:
            type_queries = [q for q in self.test_queries
                           if q["type"] == query_type.value]

            if not type_queries:
                continue

            # Vector RAG æ€§èƒ½
            vector_results = await self._evaluate_system(
                vector_rag_system, type_queries
            )

            # GraphRAG æ€§èƒ½
            graph_results = await self._evaluate_system(
                graph_rag_system, type_queries
            )

            results[query_type.value] = {
                "vector_rag": vector_results,
                "graph_rag": graph_results,
                "improvement": self._calculate_improvement(vector_results, graph_results)
            }

        return results

    async def _evaluate_system(self, system: Any, queries: List[Dict]) -> Dict:
        """è©•ä¼°ç³»çµ±æ€§èƒ½"""

        total_queries = len(queries)
        correct_answers = 0
        total_latency = 0
        faithfulness_scores = []

        for query_data in queries:
            query = query_data["query"]
            expected = query_data["expected_answer"]

            # åŸ·è¡ŒæŸ¥è©¢
            start_time = time.time()
            result = await system.query(query)
            latency = time.time() - start_time

            total_latency += latency

            # è©•ä¼°æ­£ç¢ºæ€§
            if self._is_correct_answer(result["answer"], expected):
                correct_answers += 1

            # è©•ä¼°å¿ å¯¦åº¦
            faithfulness = await self._calculate_faithfulness(
                result["answer"], result.get("sources", [])
            )
            faithfulness_scores.append(faithfulness)

        return {
            "accuracy": correct_answers / total_queries,
            "average_latency": total_latency / total_queries,
            "average_faithfulness": sum(faithfulness_scores) / len(faithfulness_scores)
        }
```

---

## 5. å¤šæ™ºèƒ½é«”ç³»çµ±è¨­è¨ˆ

### 5.1 ä»£ç†å”ä½œçš„ç†è«–æ¡†æ¶

#### **å¤šæ™ºèƒ½é«”å”ä½œæ¨¡å‹**

**å®šç¾© 5.1** (ä»£ç†ç³»çµ±): å¤šæ™ºèƒ½é«” RAG ç³»çµ±å®šç¾©ç‚ºå…ƒçµ„ $\mathcal{A} = (A, T, C, P)$ï¼Œå…¶ä¸­ï¼š

- $A = \{a_1, a_2, ..., a_n\}$: ä»£ç†é›†åˆ
- $T$: ä»»å‹™åˆ†è§£å‡½æ•¸
- $C$: å”ä½œå”è­°
- $P$: æ€§èƒ½è©•ä¼°å‡½æ•¸

**å”ä½œåŸç†**: åŸºæ–¼ Smith (1980)[^16] çš„å¥‘ç´„ç¶²å”è­° (Contract Net Protocol)ï¼Œä»£ç†é–“é€šé**ä»»å‹™æ‹›æ¨™**å’Œ**èƒ½åŠ›åŒ¹é…**é€²è¡Œå”ä½œã€‚

#### **ä»»å‹™åˆ†è§£ç®—æ³•**

**ç®—æ³• 5.1** (å±¤ç´šä»»å‹™åˆ†è§£):

```python
from dataclasses import dataclass
from typing import List, Dict, Optional, Any
from enum import Enum

class AgentRole(Enum):
    COORDINATOR = "coordinator"          # å”èª¿è€…
    RESEARCHER = "researcher"           # ç ”ç©¶å“¡
    ANALYZER = "analyzer"              # åˆ†æå¸«
    VALIDATOR = "validator"            # é©—è­‰å“¡
    SYNTHESIZER = "synthesizer"       # ç¶œåˆå“¡

@dataclass
class AgentTask:
    """ä»£ç†ä»»å‹™å®šç¾©"""
    task_id: str
    description: str
    required_role: AgentRole
    input_data: Dict[str, Any]
    output_schema: Dict[str, Any]
    priority: int
    estimated_duration: float
    dependencies: List[str]  # ä¾è³´çš„å…¶ä»–ä»»å‹™ID

class MultiAgentTaskDecomposer:
    """å¤šæ™ºèƒ½é«”ä»»å‹™åˆ†è§£å™¨"""

    def __init__(self):
        self.decomposition_strategies = {
            "research_intensive": self._decompose_research_task,
            "analysis_intensive": self._decompose_analysis_task,
            "synthesis_intensive": self._decompose_synthesis_task
        }

    async def decompose_complex_query(self, query: str,
                                    complexity_analysis: Dict) -> List[AgentTask]:
        """åˆ†è§£è¤‡é›œæŸ¥è©¢ç‚ºå­ä»»å‹™"""

        # ç¢ºå®šåˆ†è§£ç­–ç•¥
        if complexity_analysis["domain_complexity"] > 0.8:
            strategy = "research_intensive"
        elif complexity_analysis["reasoning_complexity"] > 0.8:
            strategy = "analysis_intensive"
        else:
            strategy = "synthesis_intensive"

        # åŸ·è¡Œåˆ†è§£
        decomposition_func = self.decomposition_strategies[strategy]
        tasks = await decomposition_func(query, complexity_analysis)

        # æ·»åŠ å”èª¿ä»»å‹™
        coordinator_task = AgentTask(
            task_id="coordinator_001",
            description=f"å”èª¿æŸ¥è©¢è™•ç†ï¼š{query}",
            required_role=AgentRole.COORDINATOR,
            input_data={"query": query, "subtasks": [t.task_id for t in tasks]},
            output_schema={"final_answer": str, "source_attribution": list},
            priority=1,
            estimated_duration=sum(t.estimated_duration for t in tasks) * 0.2,
            dependencies=[]
        )

        return [coordinator_task] + tasks

    async def _decompose_research_task(self, query: str,
                                     complexity_analysis: Dict) -> List[AgentTask]:
        """åˆ†è§£ç ”ç©¶å¯†é›†å‹ä»»å‹™"""

        # è­˜åˆ¥ç ”ç©¶é ˜åŸŸ
        research_domains = await self._identify_research_domains(query)

        tasks = []
        for i, domain in enumerate(research_domains):
            task = AgentTask(
                task_id=f"research_{i:03d}",
                description=f"ç ”ç©¶é ˜åŸŸ {domain} ç›¸é—œä¿¡æ¯",
                required_role=AgentRole.RESEARCHER,
                input_data={
                    "query": query,
                    "domain": domain,
                    "search_scope": "comprehensive"
                },
                output_schema={
                    "findings": list,
                    "sources": list,
                    "confidence": float
                },
                priority=2,
                estimated_duration=30.0,  # 30ç§’
                dependencies=["coordinator_001"]
            )
            tasks.append(task)

        # æ·»åŠ é©—è­‰ä»»å‹™
        validation_task = AgentTask(
            task_id="validation_001",
            description="é©—è­‰ç ”ç©¶çµæœçš„æº–ç¢ºæ€§å’Œä¸€è‡´æ€§",
            required_role=AgentRole.VALIDATOR,
            input_data={"research_results": [f"research_{i:03d}" for i in range(len(research_domains))]},
            output_schema={"validated_findings": list, "confidence_scores": dict},
            priority=3,
            estimated_duration=15.0,
            dependencies=[f"research_{i:03d}" for i in range(len(research_domains))]
        )
        tasks.append(validation_task)

        return tasks
```

### 5.2 LangGraph å·¥ä½œæµå¯¦ç¾

#### **ç‹€æ…‹åœ–å»ºæ¨¡**

**å®šç¾© 5.2** (RAG å·¥ä½œæµç‹€æ…‹): å·¥ä½œæµç‹€æ…‹ $S$ åŒ…å«ï¼š

$$S = (Q, R, A, C, M)$$

å…¶ä¸­ï¼š
- $Q$: æŸ¥è©¢ä¿¡æ¯
- $R$: æª¢ç´¢çµæœ
- $A$: ä»£ç†ç‹€æ…‹
- $C$: ä¸Šä¸‹æ–‡ä¿¡æ¯
- $M$: å…ƒæ•¸æ“š

#### **LangGraph å¯¦ç¾æ¡†æ¶**

```python
from langgraph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from typing import TypedDict, List, Dict, Any

class GraphRAGState(TypedDict):
    """GraphRAG å·¥ä½œæµç‹€æ…‹"""
    query: str
    query_analysis: Dict[str, Any]
    entity_extraction: List[Dict]
    graph_search_results: Dict[str, Any]
    vector_search_results: List[Dict]
    synthesis_results: Dict[str, Any]
    final_answer: str
    confidence_score: float
    source_attribution: List[Dict]
    workflow_metadata: Dict[str, Any]

class GraphRAGWorkflow:
    """åŸºæ–¼ LangGraph çš„ GraphRAG å·¥ä½œæµ"""

    def __init__(self, graph_store: GraphStore, vector_store: VectorStore):
        self.graph_store = graph_store
        self.vector_store = vector_store
        self.workflow = self._build_workflow()

    def _build_workflow(self) -> StateGraph:
        """æ§‹å»º GraphRAG å·¥ä½œæµåœ–"""

        workflow = StateGraph(GraphRAGState)

        # æ·»åŠ å·¥ä½œæµç¯€é»
        workflow.add_node("query_analyzer", self._analyze_query)
        workflow.add_node("entity_extractor", self._extract_entities)
        workflow.add_node("graph_navigator", self._navigate_graph)
        workflow.add_node("vector_retriever", self._vector_retrieve)
        workflow.add_node("result_fusion", self._fuse_results)
        workflow.add_node("answer_synthesizer", self._synthesize_answer)
        workflow.add_node("quality_validator", self._validate_quality)

        # å®šç¾©å·¥ä½œæµé‚Š
        workflow.add_edge("query_analyzer", "entity_extractor")
        workflow.add_edge("entity_extractor", "graph_navigator")
        workflow.add_edge("entity_extractor", "vector_retriever")
        workflow.add_edge("graph_navigator", "result_fusion")
        workflow.add_edge("vector_retriever", "result_fusion")
        workflow.add_edge("result_fusion", "answer_synthesizer")
        workflow.add_edge("answer_synthesizer", "quality_validator")

        # æ¢ä»¶é‚Šï¼šå“è³ªæª¢æŸ¥
        workflow.add_conditional_edges(
            "quality_validator",
            self._quality_gate_decision,
            {
                "approved": END,
                "retry_search": "graph_navigator",
                "retry_synthesis": "answer_synthesizer",
                "escalate": END
            }
        )

        workflow.set_entry_point("query_analyzer")

        return workflow.compile()

    async def _analyze_query(self, state: GraphRAGState) -> GraphRAGState:
        """åˆ†ææŸ¥è©¢ç‰¹å¾µå’Œè™•ç†ç­–ç•¥"""

        query = state["query"]

        # æŸ¥è©¢è¤‡é›œåº¦åˆ†æ
        complexity_analysis = await self._analyze_query_complexity(query)

        # æŸ¥è©¢é¡å‹åˆ†é¡
        query_type = await self._classify_graph_query_type(query)

        # å¯¦é«”é è­˜åˆ¥
        potential_entities = await self._identify_potential_entities(query)

        state["query_analysis"] = {
            "complexity": complexity_analysis,
            "query_type": query_type,
            "potential_entities": potential_entities,
            "processing_strategy": self._determine_processing_strategy(
                complexity_analysis, query_type
            )
        }

        return state

    async def _extract_entities(self, state: GraphRAGState) -> GraphRAGState:
        """å¾æŸ¥è©¢ä¸­æŠ½å–å¯¦é«”"""

        query = state["query"]
        potential_entities = state["query_analysis"]["potential_entities"]

        # åœ¨çŸ¥è­˜åœ–è­œä¸­æŸ¥æ‰¾åŒ¹é…å¯¦é«”
        matched_entities = []
        for entity_mention in potential_entities:
            matches = await self.graph_store.find_entities(
                entity_mention, similarity_threshold=0.8
            )
            matched_entities.extend(matches)

        state["entity_extraction"] = matched_entities

        return state

    async def _navigate_graph(self, state: GraphRAGState) -> GraphRAGState:
        """åœ–å°èˆªå’Œæª¢ç´¢"""

        query = state["query"]
        entities = state["entity_extraction"]
        strategy = state["query_analysis"]["processing_strategy"]

        if strategy["use_local_search"]:
            local_results = await self._perform_local_search(query, entities)
        else:
            local_results = {"results": []}

        if strategy["use_global_search"]:
            global_results = await self._perform_global_search(query)
        else:
            global_results = {"results": []}

        state["graph_search_results"] = {
            "local": local_results,
            "global": global_results,
            "strategy_used": strategy
        }

        return state

    def _quality_gate_decision(self, state: GraphRAGState) -> str:
        """å“è³ªæª¢æŸ¥æ±ºç­–é‚è¼¯"""

        confidence = state.get("confidence_score", 0.0)
        source_count = len(state.get("source_attribution", []))

        if confidence >= 0.8 and source_count >= 2:
            return "approved"
        elif confidence >= 0.6:
            return "retry_search"
        elif confidence >= 0.4:
            return "retry_synthesis"
        else:
            return "escalate"
```

---

## 6. ä¼æ¥­ç´š GraphRAG éƒ¨ç½²æ¡ˆä¾‹

### 6.1 å¤§å‹ä¼æ¥­çŸ¥è­˜ç®¡ç†ç³»çµ±

#### **ç³»çµ±éœ€æ±‚åˆ†æ**

**ä¼æ¥­èƒŒæ™¯**: æŸè·¨åœ‹ç§‘æŠ€å…¬å¸ï¼Œæ“æœ‰ï¼š
- 100è¬+ å…§éƒ¨æ–‡æª”
- 50,000+ å“¡å·¥
- 15å€‹æ¥­å‹™éƒ¨é–€
- 8ç¨®ä¸»è¦èªè¨€

**GraphRAG éœ€æ±‚**:
- è·¨éƒ¨é–€çŸ¥è­˜é—œè¯
- å¤šèªè¨€å¯¦é«”å°é½Š
- å¯¦æ™‚çµ„ç¹”æ¶æ§‹æ›´æ–°
- åˆè¦æ€§é—œä¿‚è¿½è¹¤

#### **æ¶æ§‹è¨­è¨ˆ**

**è¨­è¨ˆåŸå‰‡ 6.1** (ä¼æ¥­ç´š GraphRAG è¨­è¨ˆåŸå‰‡):

1. **å¯æ“´å±•æ€§**: æ”¯æŒåå„„ç´šç¯€é»å’Œé‚Š
2. **å¤šç§Ÿæˆ¶**: éƒ¨é–€ç´šæ•¸æ“šéš”é›¢
3. **å¯¦æ™‚æ›´æ–°**: å¢é‡åœ–æ§‹å»ºèƒ½åŠ›
4. **å®‰å…¨æ€§**: åŸºæ–¼åœ–çµæ§‹çš„è¨ªå•æ§åˆ¶

**å¯¦ç¾æ¶æ§‹**:

```python
class EnterpriseGraphRAGSystem:
    """ä¼æ¥­ç´š GraphRAG ç³»çµ±"""

    def __init__(self):
        # åœ–å­˜å„²ï¼šåˆ†æ•£å¼åœ–è³‡æ–™åº«
        self.graph_store = Neo4jGraphStore(
            uri="bolt://neo4j-cluster:7687",
            auth=("neo4j", "password")
        )

        # å‘é‡å­˜å„²ï¼šæ··åˆéƒ¨ç½²
        self.vector_store = QdrantGraphHybrid(
            host="qdrant-cluster:6333"
        )

        # å¤šèªè¨€å¯¦é«”å°é½Š
        self.entity_aligner = MultilingualEntityAligner()

        # æ¬Šé™æ§åˆ¶
        self.access_control = GraphAccessController()

    async def build_enterprise_graph(self, departments: List[str]) -> Dict:
        """æ§‹å»ºä¼æ¥­ç´šçŸ¥è­˜åœ–è­œ"""

        graph_stats = {}

        for department in departments:
            print(f"è™•ç†éƒ¨é–€ï¼š{department}")

            # 1. ç²å–éƒ¨é–€æ–‡æª”
            dept_documents = await self._get_department_documents(department)

            # 2. ä¸¦è¡Œè™•ç†æ–‡æª”
            processing_tasks = []
            for doc_batch in self._batch_documents(dept_documents, batch_size=10):
                task = self._process_document_batch(doc_batch, department)
                processing_tasks.append(task)

            batch_results = await asyncio.gather(*processing_tasks)

            # 3. åˆä½µéƒ¨é–€çµæœ
            dept_stats = await self._merge_department_results(
                batch_results, department
            )
            graph_stats[department] = dept_stats

        # 4. è·¨éƒ¨é–€å¯¦é«”å°é½Š
        alignment_stats = await self._align_cross_department_entities()
        graph_stats["cross_department_alignment"] = alignment_stats

        # 5. ç”Ÿæˆå…¨åŸŸæ‘˜è¦
        global_summaries = await self._generate_enterprise_summaries()
        graph_stats["global_summaries"] = global_summaries

        return graph_stats

    async def _process_document_batch(self, documents: List[Dict],
                                    department: str) -> Dict:
        """æ‰¹æ¬¡è™•ç†éƒ¨é–€æ–‡æª”"""

        # å¯¦é«”æŠ½å–
        all_entities = []
        all_relations = []

        for doc in documents:
            # å¯¦é«”æŠ½å–
            entities = await self.entity_extractor.extract_entities(
                doc["content"], doc["id"]
            )

            # é—œä¿‚æŠ½å–
            relations = await self.entity_extractor.extract_relations(
                doc["content"], entities, doc["id"]
            )

            # æ·»åŠ éƒ¨é–€æ¨™ç±¤
            for entity in entities:
                entity.metadata["department"] = department
                entity.metadata["access_level"] = doc.get("access_level", "internal")

            for relation in relations:
                relation.metadata["department"] = department

            all_entities.extend(entities)
            all_relations.extend(relations)

        # å­˜å„²åˆ°åœ–è³‡æ–™åº«
        await self._store_entities_and_relations(all_entities, all_relations, department)

        return {
            "entities_extracted": len(all_entities),
            "relations_extracted": len(all_relations),
            "documents_processed": len(documents)
        }

    async def query_enterprise_graph(self, query: str,
                                   user_context: Dict) -> Dict:
        """ä¼æ¥­ç´šåœ–æŸ¥è©¢"""

        # 1. æ¬Šé™é æª¢æŸ¥
        access_check = await self.access_control.check_query_permission(
            query, user_context
        )

        if not access_check["authorized"]:
            return {
                "error": "Access denied",
                "reason": access_check["reason"]
            }

        # 2. æŸ¥è©¢è·¯ç”±æ±ºç­–
        routing_decision = await self._route_enterprise_query(
            query, user_context, access_check["accessible_departments"]
        )

        # 3. åŸ·è¡ŒæŸ¥è©¢
        if routing_decision["strategy"] == "local_search":
            results = await self._enterprise_local_search(
                query, user_context, routing_decision["target_entities"]
            )
        elif routing_decision["strategy"] == "global_search":
            results = await self._enterprise_global_search(
                query, user_context, routing_decision["target_departments"]
            )
        else:  # hybrid_search
            local_results = await self._enterprise_local_search(
                query, user_context, routing_decision["target_entities"]
            )
            global_results = await self._enterprise_global_search(
                query, user_context, routing_decision["target_departments"]
            )
            results = await self._merge_search_results(local_results, global_results)

        # 4. çµæœå¾Œè™•ç†
        filtered_results = await self._apply_enterprise_filters(
            results, user_context, access_check
        )

        return filtered_results

    async def _route_enterprise_query(self, query: str, user_context: Dict,
                                    accessible_departments: List[str]) -> Dict:
        """ä¼æ¥­æŸ¥è©¢è·¯ç”±æ±ºç­–"""

        # åˆ†ææŸ¥è©¢ç‰¹å¾µ
        features = await self._extract_enterprise_query_features(query, user_context)

        # è·¯ç”±æ±ºç­–é‚è¼¯
        if features["entity_specificity"] > 0.8:
            strategy = "local_search"
            target_entities = features["identified_entities"]
            target_departments = None
        elif features["global_scope"] > 0.7:
            strategy = "global_search"
            target_entities = None
            target_departments = accessible_departments
        else:
            strategy = "hybrid_search"
            target_entities = features["identified_entities"]
            target_departments = accessible_departments

        return {
            "strategy": strategy,
            "target_entities": target_entities,
            "target_departments": target_departments,
            "query_features": features
        }
```

---

## 7. æ€§èƒ½å„ªåŒ–èˆ‡å¯æ“´å±•æ€§

### 7.1 åœ–å­˜å„²å„ªåŒ–ç­–ç•¥

#### **åˆ†æ•£å¼åœ–å­˜å„²æ¶æ§‹**

**æŒ‘æˆ°**: ä¼æ¥­ç´šçŸ¥è­˜åœ–è­œé€šå¸¸åŒ…å«åƒè¬ç´šç¯€é»å’Œå„„ç´šé‚Šï¼Œå–®æ©Ÿå­˜å„²ç„¡æ³•æ»¿è¶³æ€§èƒ½è¦æ±‚ã€‚

**è§£æ±ºæ–¹æ¡ˆ**: åŸºæ–¼åœ–åˆ†å‰²çš„åˆ†æ•£å¼å­˜å„²

**ç®—æ³• 7.1** (åœ–åˆ†å‰²ç­–ç•¥):

```python
import networkx as nx
from typing import Dict, List, Set
import numpy as np

class DistributedGraphPartitioner:
    """åˆ†æ•£å¼åœ–åˆ†å‰²å™¨"""

    def __init__(self, num_partitions: int = 8):
        self.num_partitions = num_partitions

    def partition_graph(self, graph: nx.Graph) -> Dict[int, Set[str]]:
        """
        ä½¿ç”¨ METIS ç®—æ³•é€²è¡Œåœ–åˆ†å‰²

        ç›®æ¨™ï¼šæœ€å°åŒ–è·¨åˆ†å‰²é‚Šçš„æ•¸é‡
        """

        try:
            import pymetis
        except ImportError:
            # é€€å›åˆ°ç°¡å–®çš„å“ˆå¸Œåˆ†å‰²
            return self._hash_partition(graph)

        # æº–å‚™ METIS è¼¸å…¥
        node_list = list(graph.nodes())
        node_map = {node: i for i, node in enumerate(node_list)}

        adjacency_list = []
        for node in node_list:
            neighbors = [node_map[neighbor] for neighbor in graph.neighbors(node)]
            adjacency_list.append(neighbors)

        # åŸ·è¡Œåœ–åˆ†å‰²
        edge_cuts, partition_assignment = pymetis.part_graph(
            self.num_partitions,
            adjacency=adjacency_list
        )

        # è½‰æ›çµæœæ ¼å¼
        partitions = {}
        for i, node in enumerate(node_list):
            partition_id = partition_assignment[i]
            if partition_id not in partitions:
                partitions[partition_id] = set()
            partitions[partition_id].add(node)

        return partitions

    def _hash_partition(self, graph: nx.Graph) -> Dict[int, Set[str]]:
        """åŸºæ–¼å“ˆå¸Œçš„ç°¡å–®åˆ†å‰²ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰"""

        partitions = {i: set() for i in range(self.num_partitions)}

        for node in graph.nodes():
            partition_id = hash(node) % self.num_partitions
            partitions[partition_id].add(node)

        return partitions

    def analyze_partition_quality(self, graph: nx.Graph,
                                 partitions: Dict[int, Set[str]]) -> Dict:
        """åˆ†æåˆ†å‰²å“è³ª"""

        total_edges = graph.number_of_edges()
        cross_partition_edges = 0

        # è¨ˆç®—è·¨åˆ†å‰²é‚Š
        for u, v in graph.edges():
            u_partition = None
            v_partition = None

            for partition_id, nodes in partitions.items():
                if u in nodes:
                    u_partition = partition_id
                if v in nodes:
                    v_partition = partition_id

            if u_partition != v_partition:
                cross_partition_edges += 1

        # è¨ˆç®—è² è¼‰å¹³è¡¡
        partition_sizes = [len(nodes) for nodes in partitions.values()]
        load_balance = 1.0 - (np.std(partition_sizes) / np.mean(partition_sizes))

        return {
            "edge_cut_ratio": cross_partition_edges / total_edges,
            "load_balance": load_balance,
            "partition_sizes": partition_sizes,
            "cross_partition_edges": cross_partition_edges
        }
```

### 7.2 æŸ¥è©¢æ€§èƒ½å„ªåŒ–

#### **åœ–æŸ¥è©¢å„ªåŒ–ç®—æ³•**

**å•é¡Œ**: åœ–éæ­·æŸ¥è©¢çš„æ™‚é–“è¤‡é›œåº¦å¯èƒ½é”åˆ°æŒ‡æ•¸ç´šï¼Œéœ€è¦å„ªåŒ–ç­–ç•¥ã€‚

**è§£æ±ºæ–¹æ¡ˆ**: å¤šå±¤æ¬¡æŸ¥è©¢å„ªåŒ–

**ç®—æ³• 7.2** (åˆ†å±¤æŸ¥è©¢å„ªåŒ–):

```python
class GraphQueryOptimizer:
    """åœ–æŸ¥è©¢å„ªåŒ–å™¨"""

    def __init__(self, graph: nx.Graph, summaries: Dict):
        self.graph = graph
        self.summaries = summaries
        self.query_cache = {}  # æŸ¥è©¢çµæœå¿«å–

    async def optimized_graph_query(self, query: str,
                                   start_entities: List[str],
                                   max_hops: int = 3) -> Dict:
        """å„ªåŒ–çš„åœ–æŸ¥è©¢"""

        # 1. æª¢æŸ¥æŸ¥è©¢å¿«å–
        cache_key = self._generate_query_cache_key(query, start_entities, max_hops)
        if cache_key in self.query_cache:
            return self.query_cache[cache_key]

        # 2. é å…ˆéæ¿¾ï¼šä½¿ç”¨æ‘˜è¦å¿«é€Ÿå®šä½ç›¸é—œå€åŸŸ
        relevant_communities = await self._filter_by_summaries(query)

        # 3. é™åˆ¶æœç´¢ç©ºé–“
        search_scope = set()
        for community_id in relevant_communities:
            community_nodes = self._get_community_nodes(community_id)
            search_scope.update(community_nodes)

        # 4. åœ¨é™åˆ¶ç©ºé–“å…§åŸ·è¡Œéæ­·
        if start_entities:
            results = await self._bounded_traversal(
                start_entities, search_scope, max_hops
            )
        else:
            results = await self._community_based_search(
                relevant_communities, query
            )

        # 5. å¿«å–çµæœ
        self.query_cache[cache_key] = results

        return results

    async def _bounded_traversal(self, start_entities: List[str],
                                search_scope: Set[str],
                                max_hops: int) -> Dict:
        """å—é™ç©ºé–“çš„åœ–éæ­·"""

        visited = set()
        current_level = set(start_entities)
        all_paths = []

        for hop in range(max_hops):
            if not current_level:
                break

            next_level = set()
            for entity in current_level:
                if entity not in self.graph or entity in visited:
                    continue

                visited.add(entity)

                # åªåœ¨æœç´¢ç©ºé–“å…§éæ­·
                neighbors = set(self.graph.neighbors(entity)) & search_scope
                next_level.update(neighbors - visited)

                # è¨˜éŒ„è·¯å¾‘
                for neighbor in neighbors:
                    if neighbor not in visited:
                        path_data = {
                            "start": start_entities[0] if start_entities else entity,
                            "end": neighbor,
                            "intermediate": entity if hop > 0 else None,
                            "hop_count": hop + 1,
                            "relation": self._get_relation_type(entity, neighbor)
                        }
                        all_paths.append(path_data)

            current_level = next_level

        return {
            "visited_entities": list(visited),
            "paths": all_paths,
            "max_hops_reached": len(visited) > 0
        }
```

---

## 8. GraphRAG è©•ä¼°èˆ‡åŸºæº–æ¸¬è©¦

### 8.1 è©•ä¼°æŒ‡æ¨™é«”ç³»

#### **åœ–ç‰¹å®šè©•ä¼°æŒ‡æ¨™**

**æŒ‡æ¨™ 8.1** (GraphRAG ç¶œåˆè©•ä¼°æ¡†æ¶):

$$\text{GraphRAG-Score} = w_1 \cdot R_{graph} + w_2 \cdot P_{relation} + w_3 \cdot C_{global} + w_4 \cdot L_{latency}$$

å…¶ä¸­ï¼š
- $R_{graph}$: åœ–æª¢ç´¢å¬å›ç‡
- $P_{relation}$: é—œä¿‚ç²¾ç¢ºåº¦
- $C_{global}$: å…¨åŸŸä¸€è‡´æ€§
- $L_{latency}$: å»¶é²æ€§èƒ½ (æ¨™æº–åŒ–)

#### **åŸºæº–æ¸¬è©¦å¯¦ç¾**

```python
class GraphRAGEvaluationSuite:
    """GraphRAG è©•ä¼°æ¸¬è©¦å¥—ä»¶"""

    def __init__(self):
        self.test_datasets = {
            "hotpot_qa": self._load_hotpot_qa(),      # å¤šè·³æ¨ç†
            "complex_web": self._load_complex_web(),   # è¤‡é›œç¶²çµ¡æŸ¥è©¢
            "enterprise_kb": self._load_enterprise_kb() # ä¼æ¥­çŸ¥è­˜åº«
        }

    async def run_comprehensive_evaluation(self,
                                         graph_rag_system: GraphRAGSystem,
                                         baseline_systems: Dict[str, Any]) -> Dict:
        """é‹è¡Œç¶œåˆè©•ä¼°"""

        evaluation_results = {}

        for dataset_name, dataset in self.test_datasets.items():
            print(f"è©•ä¼°æ•¸æ“šé›†ï¼š{dataset_name}")

            # GraphRAG è©•ä¼°
            graph_rag_results = await self._evaluate_on_dataset(
                graph_rag_system, dataset, "graphrag"
            )

            # åŸºç·šç³»çµ±è©•ä¼°
            baseline_results = {}
            for baseline_name, baseline_system in baseline_systems.items():
                baseline_result = await self._evaluate_on_dataset(
                    baseline_system, dataset, baseline_name
                )
                baseline_results[baseline_name] = baseline_result

            evaluation_results[dataset_name] = {
                "graphrag": graph_rag_results,
                "baselines": baseline_results,
                "improvements": self._calculate_improvements(
                    graph_rag_results, baseline_results
                )
            }

        return evaluation_results

    async def _evaluate_on_dataset(self, system: Any, dataset: List[Dict],
                                 system_type: str) -> Dict:
        """åœ¨ç‰¹å®šæ•¸æ“šé›†ä¸Šè©•ä¼°ç³»çµ±"""

        results = {
            "accuracy": 0.0,
            "avg_latency": 0.0,
            "faithfulness": 0.0,
            "relation_accuracy": 0.0,
            "multi_hop_success": 0.0
        }

        total_queries = len(dataset)
        correct_answers = 0
        total_latency = 0
        faithfulness_scores = []
        relation_accuracies = []
        multi_hop_successes = []

        for query_data in dataset:
            query = query_data["query"]
            expected_answer = query_data["expected_answer"]
            expected_entities = query_data.get("expected_entities", [])

            # åŸ·è¡ŒæŸ¥è©¢
            start_time = time.time()
            try:
                result = await system.query(query)
                latency = time.time() - start_time
                total_latency += latency

                # è©•ä¼°æº–ç¢ºæ€§
                if self._is_correct_answer(result["answer"], expected_answer):
                    correct_answers += 1

                # è©•ä¼°å¿ å¯¦åº¦
                if "sources" in result:
                    faithfulness = await self._calculate_faithfulness(
                        result["answer"], result["sources"]
                    )
                    faithfulness_scores.append(faithfulness)

                # è©•ä¼°é—œä¿‚æº–ç¢ºæ€§ (GraphRAG ç‰¹æœ‰)
                if system_type == "graphrag" and "graph_results" in result:
                    relation_acc = self._evaluate_relation_accuracy(
                        result["graph_results"], expected_entities
                    )
                    relation_accuracies.append(relation_acc)

                # è©•ä¼°å¤šè·³æ¨ç†æˆåŠŸç‡
                if query_data.get("requires_multi_hop", False):
                    multi_hop_success = self._evaluate_multi_hop_reasoning(
                        result, query_data
                    )
                    multi_hop_successes.append(multi_hop_success)

            except Exception as e:
                print(f"æŸ¥è©¢åŸ·è¡ŒéŒ¯èª¤: {e}")
                total_latency += 5.0  # éŒ¯èª¤æ‡²ç½°

        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        results["accuracy"] = correct_answers / total_queries
        results["avg_latency"] = total_latency / total_queries
        results["faithfulness"] = sum(faithfulness_scores) / len(faithfulness_scores) if faithfulness_scores else 0
        results["relation_accuracy"] = sum(relation_accuracies) / len(relation_accuracies) if relation_accuracies else 0
        results["multi_hop_success"] = sum(multi_hop_successes) / len(multi_hop_successes) if multi_hop_successes else 0

        return results
```

---

## 9. å¯¦è¸ç·´ç¿’èˆ‡æ¡ˆä¾‹åˆ†æ

### 9.1 èª²å ‚å¯¦é©—

#### **å¯¦é©— 1: ç¤¾ç¾¤æª¢æ¸¬æ¯”è¼ƒ**
æ¯”è¼ƒ Louvainã€Leidenã€è°±èšé¡ä¸‰ç¨®ç®—æ³•åœ¨ä¼æ¥­çŸ¥è­˜åœ–è­œä¸Šçš„è¡¨ç¾ã€‚

**å¯¦é©—è¨­è¨ˆ**:
```python
async def community_detection_comparison():
    """ç¤¾ç¾¤æª¢æ¸¬ç®—æ³•æ¯”è¼ƒå¯¦é©—"""

    # æº–å‚™æ¸¬è©¦åœ–
    test_graph = load_enterprise_test_graph()

    algorithms = {
        "louvain": LouvainDetector(),
        "leiden": LeidenDetector(),
        "spectral": SpectralClusteringDetector()
    }

    results = {}
    for name, algorithm in algorithms.items():
        start_time = time.time()
        communities = algorithm.detect_communities(test_graph)
        execution_time = time.time() - start_time

        quality = analyze_community_quality(test_graph, communities)

        results[name] = {
            "execution_time": execution_time,
            "modularity": quality["modularity"],
            "num_communities": quality["num_communities"],
            "average_size": quality["average_community_size"]
        }

    return results
```

#### **å¯¦é©— 2: æŸ¥è©¢æ€§èƒ½åŸºæº–æ¸¬è©¦**
è¨­è¨ˆå¯¦é©—æ¯”è¼ƒ GraphRAG å’Œå‚³çµ± RAG åœ¨ä¸åŒæŸ¥è©¢é¡å‹ä¸Šçš„è¡¨ç¾ã€‚

### 9.2 ä¼æ¥­æ¡ˆä¾‹ç ”ç©¶

#### **æ¡ˆä¾‹ï¼šè·¨åœ‹è£½é€ æ¥­çš„ä¾›æ‡‰éˆçŸ¥è­˜ç®¡ç†**

**èƒŒæ™¯**:
- è¤‡é›œçš„å…¨çƒä¾›æ‡‰éˆç¶²çµ¡
- å¤šå±¤ç´šçš„ä¾›æ‡‰å•†é—œä¿‚
- é¢¨éšªè©•ä¼°å’Œåˆè¦è¿½è¹¤éœ€æ±‚

**GraphRAG æ‡‰ç”¨**:
1. **ä¾›æ‡‰éˆåœ–è­œæ§‹å»º**: ä¾›æ‡‰å•†-ç”¢å“-å·¥å» -åœ°å€çš„é—œä¿‚ç¶²çµ¡
2. **é¢¨éšªå‚³æ’­åˆ†æ**: åŸºæ–¼åœ–éæ­·çš„é¢¨éšªå½±éŸ¿è©•ä¼°
3. **åˆè¦æ€§æª¢æŸ¥**: é€šéé—œä¿‚è·¯å¾‘è¿½è¹¤åˆè¦è¦æ±‚

**å¯¦æ–½æ•ˆæœ**:
- ä¾›æ‡‰å•†é¢¨éšªè©•ä¼°æ™‚é–“æ¸›å°‘ 75%
- åˆè¦æª¢æŸ¥æº–ç¢ºç‡æå‡åˆ° 92%
- ä¾›æ‡‰éˆä¸­æ–·é è­¦æå‰ 48 å°æ™‚

---

## 10. æœªä¾†ç™¼å±•æ–¹å‘

### 10.1 æŠ€è¡“è¶¨å‹¢

#### **ç¥ç¶“ç¬¦è™Ÿèåˆ**
çµåˆç¥ç¶“ç¶²çµ¡çš„å­¸ç¿’èƒ½åŠ›å’Œç¬¦è™Ÿç³»çµ±çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¦ç¾æ›´å¼·å¤§çš„çŸ¥è­˜è¡¨ç¤ºå’Œæ¨ç†ã€‚

#### **å¤šæ¨¡æ…‹çŸ¥è­˜åœ–è­œ**
æ•´åˆæ–‡æœ¬ã€åœ–åƒã€éŸ³é »ç­‰å¤šç¨®æ¨¡æ…‹ä¿¡æ¯ï¼Œæ§‹å»ºæ›´è±å¯Œçš„ä¼æ¥­çŸ¥è­˜è¡¨ç¤ºã€‚

#### **å‹•æ…‹åœ–å­¸ç¿’**
å¯¦æ™‚å­¸ç¿’å’Œæ›´æ–°çŸ¥è­˜åœ–è­œçµæ§‹ï¼Œé©æ‡‰ä¸æ–·è®ŠåŒ–çš„ä¼æ¥­ç’°å¢ƒã€‚

### 10.2 ç ”ç©¶æŒ‘æˆ°

1. **å¯è§£é‡‹æ€§**: å¦‚ä½•è®“åœ–æ¨ç†éç¨‹æ›´åŠ é€æ˜å’Œå¯è§£é‡‹
2. **æ“´å±•æ€§**: å¦‚ä½•è™•ç†è¶…å¤§è¦æ¨¡çš„ä¼æ¥­çŸ¥è­˜åœ–è­œ
3. **å‹•æ…‹æ€§**: å¦‚ä½•é«˜æ•ˆè™•ç†çŸ¥è­˜çš„å¯¦æ™‚æ›´æ–°å’Œæ¼”åŒ–
4. **å¤šèªè¨€**: å¦‚ä½•åœ¨å¤šèªè¨€ç’°å¢ƒä¸­ä¿æŒå¯¦é«”å’Œé—œä¿‚çš„ä¸€è‡´æ€§

---

## 11. æœ¬ç« ç¸½çµ

### 11.1 æ ¸å¿ƒè²¢ç»

æœ¬ç« ç³»çµ±æ€§åœ°åˆ†æäº† GraphRAG çš„ç†è«–åŸºç¤ã€å¯¦ç¾æ–¹æ³•å’Œä¼æ¥­æ‡‰ç”¨ï¼Œä¸»è¦è²¢ç»åŒ…æ‹¬ï¼š

1. **ç†è«–å»ºæ§‹**: å»ºç«‹äº†å‘é‡æª¢ç´¢èˆ‡åœ–æª¢ç´¢çš„æ•¸å­¸æ¯”è¼ƒæ¡†æ¶
2. **ç®—æ³•è©³è§£**: è©³ç´°è§£æäº†ç¤¾ç¾¤æª¢æ¸¬ã€å±¤ç´šæ‘˜è¦ç­‰æ ¸å¿ƒç®—æ³•
3. **ç³»çµ±è¨­è¨ˆ**: æä¾›äº†ä¼æ¥­ç´š GraphRAG çš„å®Œæ•´å¯¦ç¾æ–¹æ¡ˆ
4. **æ€§èƒ½åˆ†æ**: å»ºç«‹äº† GraphRAG å°ˆç”¨çš„è©•ä¼°æŒ‡æ¨™é«”ç³»

### 11.2 å¯¦ç”¨æŒ‡å—

**é©ç”¨å ´æ™¯**:
- âœ… è¤‡é›œé—œä¿‚æŸ¥è©¢ (å¦‚çµ„ç¹”æ¶æ§‹ã€ä¾›æ‡‰éˆ)
- âœ… è·¨é ˜åŸŸçŸ¥è­˜ç¶œåˆ
- âœ… å¤šè·³æ¨ç†éœ€æ±‚
- âŒ ç°¡å–®äº‹å¯¦æŸ¥è©¢
- âŒ å¯¦æ™‚æ€§è¦æ±‚æ¥µé«˜çš„å ´æ™¯

**å¯¦æ–½å»ºè­°**:
1. å¾å°è¦æ¨¡è©¦é»é–‹å§‹ï¼Œé©—è­‰æ¥­å‹™åƒ¹å€¼
2. é‡é»é—œæ³¨æ•¸æ“šå“è³ªå’Œå¯¦é«”å°é½Š
3. å»ºç«‹æŒçºŒçš„æ€§èƒ½ç›£æ§æ©Ÿåˆ¶
4. èˆ‡å‚³çµ± RAG ç³»çµ±ä¸¦è¡Œéƒ¨ç½²ï¼Œäº’ç‚ºè£œå……

---

## åƒè€ƒæ–‡ç»

[^14]: Edge, D., Trinh, H., Cheng, N., et al. (2024). "From Local to Global: A Graph RAG Approach to Query-Focused Summarization." *Microsoft Research Technical Report*.

[^15]: Traag, V. A., Waltman, L., & van Eck, N. J. (2019). "From Louvain to Leiden: guaranteeing well-connected communities." *Scientific Reports*, 9(1), 5233.

[^16]: Smith, R. G. (1980). "The contract net protocol: High-level communication and control in a distributed problem solver." *IEEE Transactions on Computers*, C-29(12), 1104-1113.

---

**èª²ç¨‹è©•ä¼°**: æœ¬ç« å…§å®¹åœ¨æœŸæœ«è€ƒè©¦ä¸­å 30%æ¬Šé‡ï¼Œå­¸ç”Ÿéœ€è¦æŒæ¡åœ–è«–åŸºç¤ã€ç®—æ³•å¯¦ç¾å’Œç³»çµ±è¨­è¨ˆèƒ½åŠ›ã€‚

**å¯¦ä½œè¦æ±‚**: å­¸ç”Ÿéœ€å®Œæˆä¸€å€‹å°å‹ GraphRAG ç³»çµ±çš„å¯¦ç¾ï¼ŒåŒ…æ‹¬åœ–æ§‹å»ºã€ç¤¾ç¾¤æª¢æ¸¬å’ŒæŸ¥è©¢è™•ç†åŠŸèƒ½ã€‚