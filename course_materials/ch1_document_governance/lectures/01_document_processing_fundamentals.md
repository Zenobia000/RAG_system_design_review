# æ–‡æª”è™•ç†åŸºç¤ï¼šåˆ¥è®“åƒåœ¾æ•¸æ“šæ¯€äº†ä½ çš„ RAG
## ç¬¬1ç« ï¼šæŠŠä¼æ¥­æ–‡æª”è®Šæˆå¯ç”¨çš„çŸ¥è­˜

**å­¸ç¿’æ™‚é–“**: 2-3 å°æ™‚
**å‰ç½®çŸ¥è­˜**: æœƒç”¨ Pythonï¼ŒçŸ¥é“ä»€éº¼æ˜¯ PDF
**ç›®æ¨™**: å­¸æœƒè™•ç†ä¼æ¥­æ–‡æª”ï¼Œåˆ¥è¢«åƒåœ¾æ•¸æ“šå‘æ­»

---

## ğŸ¯ æ ¸å¿ƒå•é¡Œ

**ä¼æ¥­æ–‡æª” = ç½é›£ç¾å ´**

å¤§éƒ¨åˆ†ä¼æ¥­çš„æ–‡æª”ç‹€æ³ï¼š
- ğŸ“„ **æ ¼å¼æ··äº‚**: PDF/Word/PPT/Confluence åˆ°è™•éƒ½æ˜¯
- ğŸ—“ï¸ **ç‰ˆæœ¬æ··äº‚**: 2019å¹´çš„æ–‡ä»¶é‚„åœ¨ç”¨ï¼Œæ²’äººçŸ¥é“æ˜¯ä¸æ˜¯æœ€æ–°ç‰ˆ
- ğŸ” **æ‰¾ä¸åˆ°**: é—œéµä¿¡æ¯è—åœ¨æŸå€‹æ·±åº¦ç›®éŒ„çš„ Excel è¡¨æ ¼è£¡
- ğŸš« **æ¬Šé™æ··äº‚**: èª°èƒ½çœ‹ä»€éº¼ï¼Œé€£ IT éƒ½æä¸æ¸…æ¥š

**åº•ç·š**: åƒåœ¾é€²ï¼Œåƒåœ¾å‡º (GIGO)ã€‚RAG ç³»çµ±å†è°æ˜ï¼Œä¹Ÿæ•‘ä¸äº†çˆ›æ•¸æ“šã€‚

---

## ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼šå¯¦ç”¨çš„æ–‡æª”è™•ç†æµæ°´ç·š

### 1.1 æ–‡æª”è™•ç†çš„ç¾å¯¦é¸æ“‡

#### **å·¥å…·é¸å‹ï¼šç°¡å–®æœ‰æ•ˆ**

```python
# 2025å¹´å¯¦ç”¨çµ„åˆ
pip install docling              # IBMå‡ºå“ï¼ŒPDFè™•ç†æœ€å¼·
pip install unstructured         # å‚™ç”¨æ–¹æ¡ˆï¼Œæ ¼å¼æ”¯æ´å»£
pip install pypdf               # ç°¡å–®PDFï¼Œé€Ÿåº¦å¿«
```

**é¸æ“‡é‚è¼¯**:
- **ä¸»åŠ›**: Docling (æº–ç¢ºç‡95%+ï¼Œå€¼å¾—å­¸ç¿’æˆæœ¬)
- **å‚™ç”¨**: Unstructured (æ ¼å¼å…¨ï¼Œä½†æº–ç¢ºç‡å·®é»)
- **ç°¡å–®**: PyPDF (ç´”PDFå ´æ™¯ï¼Œæ€§èƒ½å¥½)

#### **å¯¦éš›ä»£ç¢¼ï¼š30è¡Œæå®šåŸºæœ¬è™•ç†**

```python
from docling.document_converter import DocumentConverter
from pathlib import Path

def process_enterprise_doc(file_path: str) -> dict:
    """è™•ç†ä¼æ¥­æ–‡æª”çš„æœ€ç°¡å¯¦ç¾"""

    converter = DocumentConverter()

    try:
        # è½‰æ›æ–‡æª”
        result = converter.convert(file_path)

        # æå–ç´”æ–‡æœ¬ (Markdownæ ¼å¼ï¼Œä¿ç•™çµæ§‹)
        content = result.document.export_to_markdown()

        # åŸºæœ¬çµ±è¨ˆ
        stats = {
            "char_count": len(content),
            "word_count": len(content.split()),
            "has_tables": "|" in content,
            "has_headers": "#" in content
        }

        return {
            "success": True,
            "content": content,
            "stats": stats,
            "file_path": file_path
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "file_path": file_path
        }

# æ‰¹é‡è™•ç†
def process_document_folder(folder_path: str) -> list:
    """æ‰¹é‡è™•ç†æ–‡æª”æ–‡ä»¶å¤¾"""

    results = []
    folder = Path(folder_path)

    # æ”¯æ´çš„æ ¼å¼
    supported_formats = {'.pdf', '.docx', '.pptx', '.md', '.txt'}

    for file_path in folder.rglob('*'):
        if file_path.suffix.lower() in supported_formats:
            result = process_enterprise_doc(str(file_path))
            results.append(result)

            # ç°¡å–®é€²åº¦é¡¯ç¤º
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{status} {file_path.name}")

    return results
```

### 1.2 æ–‡æª”åˆ†å¡Šï¼šåˆ¥æƒ³å¤ªè¤‡é›œ

#### **åˆ†å¡Šç­–ç•¥ï¼šå¯¦ç”¨ä¸»ç¾©**

å­¸è¡“ç•Œå–œæ­¡æè¤‡é›œçš„"èªç¾©åˆ†å¡Š"ã€‚ç¾å¯¦ä¸­ï¼Œç°¡å–®çš„è¦å‰‡åˆ†å¡Šå°±å¤ ç”¨ï¼š

```python
from langchain.text_splitters import RecursiveCharacterTextSplitter

def smart_chunk_document(content: str, doc_type: str = "general") -> list:
    """å¯¦ç”¨çš„æ–‡æª”åˆ†å¡Šç­–ç•¥"""

    # ä¸åŒé¡å‹æ–‡æª”çš„åˆ†å¡Šåƒæ•¸
    chunk_configs = {
        "technical": {"size": 800, "overlap": 100},   # æŠ€è¡“æ–‡æª”è¦ç²¾ç¢º
        "policy": {"size": 1200, "overlap": 200},     # æ”¿ç­–æ–‡æª”è¦å®Œæ•´
        "general": {"size": 1000, "overlap": 150}     # ä¸€èˆ¬æ–‡æª”å¹³è¡¡
    }

    config = chunk_configs.get(doc_type, chunk_configs["general"])

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=config["size"],
        chunk_overlap=config["overlap"],
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", ".", " "]  # ä¸­è‹±æ–‡éƒ½è€ƒæ…®
    )

    chunks = splitter.split_text(content)

    # æ·»åŠ åŸºæœ¬å…ƒæ•¸æ“š
    chunk_data = []
    for i, chunk in enumerate(chunks):
        chunk_data.append({
            "id": f"chunk_{i}",
            "text": chunk,
            "char_count": len(chunk),
            "word_count": len(chunk.split()),
            "chunk_index": i
        })

    return chunk_data

# æ¸¬è©¦æ•ˆæœ
def test_chunking():
    """æ¸¬è©¦åˆ†å¡Šæ•ˆæœ"""

    sample_text = "ä½ çš„æ¸¬è©¦æ–‡æª”å…§å®¹..."
    chunks = smart_chunk_document(sample_text, "technical")

    print(f"åŸæ–‡é•·åº¦: {len(sample_text)}")
    print(f"åˆ†å¡Šæ•¸é‡: {len(chunks)}")
    print(f"å¹³å‡åˆ†å¡Šé•·åº¦: {sum(len(c['text']) for c in chunks) / len(chunks):.0f}")

    return chunks
```

#### **ç‚ºä»€éº¼ä¸ç”¨è¤‡é›œçš„èªç¾©åˆ†å¡Šï¼Ÿ**

**Linus è§€é»**:
> "è¤‡é›œçš„ç®—æ³•é€šå¸¸æ˜¯ç‚ºäº†æ©è“‹è¨­è¨ˆå•é¡Œã€‚å¥½çš„è¨­è¨ˆæ‡‰è©²è®“ç°¡å–®çš„ç®—æ³•å°±èƒ½å·¥ä½œã€‚"

**ç¾å¯¦æª¢é©—**:
- âœ… **ç°¡å–®åˆ†å¡Š** + **å¥½çš„æª¢ç´¢** = 95% å ´æ™¯å¤ ç”¨
- âŒ **è¤‡é›œåˆ†å¡Š** + **æ™®é€šæª¢ç´¢** = éåº¦å·¥ç¨‹ï¼Œæ€§èƒ½é‚„å¯èƒ½æ›´å·®
- ğŸ¯ **å…ˆåšç°¡å–®ç‰ˆæœ¬ï¼Œæ¸¬é‡æ•ˆæœï¼Œç¢ºå¯¦ä¸å¤ å†å„ªåŒ–**

---

## ğŸ’¾ å…ƒæ•¸æ“šï¼šåªè¦æœ‰ç”¨çš„

### 2.1 æœ€å°å¯è¡Œå…ƒæ•¸æ“š

**åˆ¥æè¤‡é›œçš„æœ¬é«”è«–ï¼** ä¼æ¥­éœ€è¦çš„å…ƒæ•¸æ“šå¾ˆç°¡å–®ï¼š

```python
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List
import hashlib

@dataclass
class SimpleDocumentMetadata:
    """ç°¡å–®å¯¦ç”¨çš„æ–‡æª”å…ƒæ•¸æ“š"""

    # åŸºæœ¬æ¨™è­˜ (å¿…éœ€)
    doc_id: str
    title: str
    file_path: str
    content_hash: str  # ç”¨æ–¼æª¢æ¸¬è®Šæ›´

    # åˆ†é¡ä¿¡æ¯ (é‡è¦)
    document_type: str  # "manual", "policy", "tech_spec", "general"
    department: str     # "engineering", "legal", "hr", "general"

    # æ™‚é–“ä¿¡æ¯ (é—œéµ)
    created_at: datetime
    modified_at: datetime
    processed_at: datetime

    # æ¬Šé™ä¿¡æ¯ (å®‰å…¨)
    access_level: str = "internal"  # "public", "internal", "confidential"
    owner: str = "unknown"

    # å…§å®¹çµ±è¨ˆ (æœ‰ç”¨)
    word_count: int = 0
    chunk_count: int = 0

    # å¯é¸ä¿¡æ¯
    keywords: List[str] = None
    related_docs: List[str] = None

def extract_simple_metadata(file_path: str, content: str) -> SimpleDocumentMetadata:
    """æå–ç°¡å–®å¯¦ç”¨çš„å…ƒæ•¸æ“š"""

    from pathlib import Path
    import os

    file_info = Path(file_path)

    # å¾æ–‡ä»¶è·¯å¾‘æ¨æ–·ä¿¡æ¯
    department = "general"
    doc_type = "general"

    # ç°¡å–®çš„è·¯å¾‘åˆ†æ
    path_parts = str(file_info).lower().split('/')

    if any(dept in path_parts for dept in ["engineering", "tech", "dev"]):
        department = "engineering"
    elif any(dept in path_parts for dept in ["legal", "compliance"]):
        department = "legal"
    elif any(dept in path_parts for dept in ["hr", "people"]):
        department = "hr"

    if any(type_hint in path_parts for type_hint in ["manual", "guide"]):
        doc_type = "manual"
    elif any(type_hint in path_parts for type_hint in ["policy", "procedure"]):
        doc_type = "policy"
    elif any(type_hint in path_parts for type_hint in ["spec", "design"]):
        doc_type = "tech_spec"

    # å¾æ–‡ä»¶åæå–æ¨™é¡Œ
    title = file_info.stem.replace('_', ' ').replace('-', ' ').title()

    # æ™‚é–“ä¿¡æ¯
    try:
        file_stat = os.stat(file_path)
        created_at = datetime.fromtimestamp(file_stat.st_ctime)
        modified_at = datetime.fromtimestamp(file_stat.st_mtime)
    except:
        created_at = modified_at = datetime.now()

    return SimpleDocumentMetadata(
        doc_id=hashlib.md5(file_path.encode()).hexdigest()[:16],
        title=title,
        file_path=file_path,
        content_hash=hashlib.md5(content.encode()).hexdigest(),
        document_type=doc_type,
        department=department,
        created_at=created_at,
        modified_at=modified_at,
        processed_at=datetime.now(),
        word_count=len(content.split()),
        keywords=extract_simple_keywords(content)  # ç°¡å–®é—œéµè©æå–
    )

def extract_simple_keywords(content: str, max_keywords: int = 10) -> List[str]:
    """ç°¡å–®çš„é—œéµè©æå–"""

    # ç§»é™¤å¸¸ç”¨è©
    stopwords = {
        "çš„", "å’Œ", "åœ¨", "æ˜¯", "æœ‰", "ä¸", "äº†", "å¯ä»¥", "é€™å€‹", "é‚£å€‹",
        "the", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with"
    }

    # ç°¡å–®è©é »çµ±è¨ˆ
    words = content.lower().split()
    word_freq = {}

    for word in words:
        if len(word) > 2 and word not in stopwords:
            word_freq[word] = word_freq.get(word, 0) + 1

    # è¿”å›é«˜é »è©
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    return [word for word, freq in top_words[:max_keywords]]
```

### 2.2 æ–‡æª”å“è³ªï¼šåªæª¢æŸ¥é‡è¦çš„

**å¿˜æ‰è¤‡é›œçš„å“è³ªæ¨¡å‹ï¼** å¯¦éš›ä¸Šåªéœ€è¦æª¢æŸ¥å¹¾å€‹é—œéµé»ï¼š

```python
def simple_quality_check(content: str, metadata: SimpleDocumentMetadata) -> dict:
    """ç°¡å–®å¯¦ç”¨çš„å“è³ªæª¢æŸ¥"""

    issues = []
    score = 1.0  # å¾æ»¿åˆ†é–‹å§‹æ‰£åˆ†

    # 1. å…§å®¹é•·åº¦æª¢æŸ¥
    if len(content) < 100:
        issues.append("å…§å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯ç©ºæ–‡æª”")
        score -= 0.3
    elif len(content) > 100000:
        issues.append("å…§å®¹å¤ªé•·ï¼Œå¯èƒ½éœ€è¦æ‹†åˆ†")
        score -= 0.1

    # 2. äº‚ç¢¼æª¢æŸ¥
    non_printable_ratio = sum(1 for c in content if not c.isprintable()) / len(content)
    if non_printable_ratio > 0.1:
        issues.append("å¯èƒ½åŒ…å«äº‚ç¢¼æˆ–äºŒé€²ä½æ•¸æ“š")
        score -= 0.4

    # 3. é‡è¤‡å…§å®¹æª¢æŸ¥
    lines = content.split('\n')
    unique_lines = set(line.strip() for line in lines if line.strip())
    if len(unique_lines) < len(lines) * 0.5:
        issues.append("é‡è¤‡å…§å®¹éå¤š")
        score -= 0.2

    # 4. çµæ§‹å®Œæ•´æ€§æª¢æŸ¥
    if metadata.document_type == "manual" and not any(word in content.lower()
                                                     for word in ["æ­¥é©Ÿ", "æ“ä½œ", "step", "procedure"]):
        issues.append("æ‰‹å†Šé¡æ–‡æª”ç¼ºå°‘æ“ä½œæ­¥é©Ÿ")
        score -= 0.2

    # 5. æ™‚æ•ˆæ€§æª¢æŸ¥ (è¶…é2å¹´çš„æ–‡æª”è¦å°å¿ƒ)
    doc_age_days = (datetime.now() - metadata.modified_at).days
    if doc_age_days > 730:  # 2å¹´
        issues.append(f"æ–‡æª”å·²æœ‰ {doc_age_days} å¤©æœªæ›´æ–°ï¼Œå¯èƒ½éæ™‚")
        score -= min(0.3, (doc_age_days - 730) / 365 * 0.1)

    return {
        "quality_score": max(0.0, score),
        "grade": "A" if score >= 0.9 else "B" if score >= 0.7 else "C" if score >= 0.5 else "F",
        "issues": issues,
        "usable": score >= 0.5  # ä½æ–¼50%å°±åˆ¥ç”¨äº†
    }
```

---

## ğŸ“ å¯¦éš›çš„æ–‡æª”è™•ç†æµæ°´ç·š

### 3.1 å®Œæ•´çš„è™•ç†æµç¨‹

```python
import os
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor
import time

class DocumentProcessor:
    """å¯¦ç”¨çš„æ–‡æª”è™•ç†å™¨"""

    def __init__(self):
        self.converter = DocumentConverter()

        # ç°¡å–®é…ç½®ï¼Œåˆ¥æå¤ªè¤‡é›œ
        self.config = {
            "max_file_size_mb": 50,  # 50MBä»¥ä¸Šçš„æ–‡ä»¶åˆ¥è™•ç†äº†
            "timeout_seconds": 60,   # 1åˆ†é˜è™•ç†ä¸å®Œå°±æ”¾æ£„
            "supported_formats": {".pdf", ".docx", ".pptx", ".md", ".txt"}
        }

    def process_folder(self, folder_path: str) -> Dict:
        """è™•ç†æ–‡æª”æ–‡ä»¶å¤¾"""

        print(f"ğŸš€ é–‹å§‹è™•ç†æ–‡ä»¶å¤¾: {folder_path}")
        start_time = time.time()

        # æ‰¾åˆ°æ‰€æœ‰æ”¯æ´çš„æ–‡æª”
        all_files = []
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                if Path(file_path).suffix.lower() in self.config["supported_formats"]:
                    # æª¢æŸ¥æ–‡ä»¶å¤§å°
                    size_mb = os.path.getsize(file_path) / (1024 * 1024)
                    if size_mb <= self.config["max_file_size_mb"]:
                        all_files.append(file_path)
                    else:
                        print(f"âš ï¸  è·³éå¤§æ–‡ä»¶ ({size_mb:.1f}MB): {file_path}")

        print(f"ğŸ“„ æ‰¾åˆ° {len(all_files)} å€‹å¯è™•ç†æ–‡ä»¶")

        # ä¸¦è¡Œè™•ç† (ä½†åˆ¥é–‹å¤ªå¤šç·šç¨‹)
        max_workers = min(4, len(all_files))
        processed_docs = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(self._process_single_file, all_files))

        # çµ±è¨ˆçµæœ
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]

        processing_time = time.time() - start_time

        print(f"âœ… è™•ç†å®Œæˆ: {len(successful)} æˆåŠŸ, {len(failed)} å¤±æ•—")
        print(f"â±ï¸  ç¸½è€—æ™‚: {processing_time:.1f} ç§’")

        return {
            "total_files": len(all_files),
            "successful": len(successful),
            "failed": len(failed),
            "processing_time": processing_time,
            "successful_docs": successful,
            "failed_docs": failed
        }

    def _process_single_file(self, file_path: str) -> Dict:
        """è™•ç†å–®å€‹æ–‡ä»¶"""

        try:
            # 1. è½‰æ›æ–‡æª”
            result = process_enterprise_doc(file_path)

            if not result["success"]:
                return result

            content = result["content"]

            # 2. æå–å…ƒæ•¸æ“š
            metadata = extract_simple_metadata(file_path, content)

            # 3. å“è³ªæª¢æŸ¥
            quality = simple_quality_check(content, metadata)

            # 4. åˆ†å¡Šè™•ç†
            chunks = smart_chunk_document(content, metadata.document_type)

            # 5. çµ„è£æœ€çµ‚çµæœ
            return {
                "success": True,
                "file_path": file_path,
                "metadata": metadata.__dict__,
                "quality": quality,
                "chunks": chunks,
                "usable": quality["usable"]
            }

        except Exception as e:
            return {
                "success": False,
                "file_path": file_path,
                "error": str(e)
            }
```

---

## ğŸš¨ PII æª¢æ¸¬ï¼šä¸èƒ½é¦¬è™çš„å®‰å…¨æª¢æŸ¥

### 4.1 å¯¦ç”¨çš„ PII æª¢æ¸¬

```python
from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine

def setup_simple_pii_detector():
    """è¨­ç½®ç°¡å–®å¯¦ç”¨çš„ PII æª¢æ¸¬å™¨"""

    analyzer = AnalyzerEngine()
    anonymizer = AnonymizerEngine()

    # ä¼æ¥­å¸¸è¦‹çš„æ•æ„Ÿä¿¡æ¯é¡å‹
    pii_types = [
        "PERSON",           # äººå
        "EMAIL_ADDRESS",    # éƒµç®±
        "PHONE_NUMBER",     # é›»è©±
        "CREDIT_CARD",      # ä¿¡ç”¨å¡
        "US_SSN",          # èº«ä»½è­‰è™Ÿ
        "IP_ADDRESS"        # IPåœ°å€
    ]

    return analyzer, anonymizer, pii_types

def check_document_pii(content: str) -> Dict:
    """æª¢æŸ¥æ–‡æª”ä¸­çš„å€‹äººä¿¡æ¯"""

    analyzer, anonymizer, pii_types = setup_simple_pii_detector()

    # æª¢æ¸¬ PII
    results = analyzer.analyze(
        text=content,
        language="en",  # ä¸»è¦æ”¯æ´è‹±æ–‡ï¼Œä¸­æ–‡æ”¯æ´æœ‰é™
        entities=pii_types
    )

    if not results:
        return {
            "has_pii": False,
            "risk_level": "safe",
            "detected_types": []
        }

    # é¢¨éšªè©•ç´šï¼šç°¡å–®ç²—æš´
    high_risk_types = {"CREDIT_CARD", "US_SSN"}
    detected_types = [r.entity_type for r in results]

    if any(pii_type in high_risk_types for pii_type in detected_types):
        risk_level = "high"
    elif len(detected_types) >= 3:
        risk_level = "medium"
    else:
        risk_level = "low"

    return {
        "has_pii": True,
        "risk_level": risk_level,
        "detected_types": detected_types,
        "detection_count": len(results),
        "needs_anonymization": risk_level in ["high", "medium"]
    }

def anonymize_if_needed(content: str, pii_check: Dict) -> str:
    """å¿…è¦æ™‚é€²è¡ŒåŒ¿ååŒ–"""

    if not pii_check["needs_anonymization"]:
        return content

    analyzer, anonymizer, _ = setup_simple_pii_detector()

    # é‡æ–°åˆ†æ (ç²å–ä½ç½®ä¿¡æ¯)
    pii_results = analyzer.analyze(text=content, language="en")

    # ç°¡å–®æ›¿æ›ç­–ç•¥
    anonymized_result = anonymizer.anonymize(
        text=content,
        analyzer_results=pii_results,
        operators={
            "PERSON": {"type": "replace", "new_value": "[PERSON]"},
            "EMAIL_ADDRESS": {"type": "replace", "new_value": "[EMAIL]"},
            "PHONE_NUMBER": {"type": "replace", "new_value": "[PHONE]"},
            "CREDIT_CARD": {"type": "replace", "new_value": "[CARD]"},
            "US_SSN": {"type": "replace", "new_value": "[SSN]"}
        }
    )

    return anonymized_result.text
```

---

## ğŸƒâ€â™‚ï¸ å¿«é€Ÿé–‹å§‹æŒ‡å—

### 5.1 30åˆ†é˜æ­å»ºæ–‡æª”è™•ç†ç³»çµ±

```python
# main.py - å®Œæ•´çš„æ–‡æª”è™•ç†è…³æœ¬
import sys
from pathlib import Path

def main():
    """ä¸»è™•ç†å‡½æ•¸"""

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python main.py <æ–‡æª”æ–‡ä»¶å¤¾è·¯å¾‘>")
        return

    folder_path = sys.argv[1]

    if not os.path.exists(folder_path):
        print(f"âŒ æ–‡ä»¶å¤¾ä¸å­˜åœ¨: {folder_path}")
        return

    # 1. åˆå§‹åŒ–è™•ç†å™¨
    processor = DocumentProcessor()

    # 2. æ‰¹é‡è™•ç†
    results = processor.process_folder(folder_path)

    # 3. éæ¿¾å¯ç”¨æ–‡æª”
    usable_docs = [doc for doc in results["successful_docs"] if doc["usable"]]

    # 4. ä¿å­˜çµæœ (JSONæ ¼å¼)
    output_file = f"processed_docs_{int(time.time())}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        import json
        json.dump({
            "processing_summary": {
                "total_files": results["total_files"],
                "successful": results["successful"],
                "usable": len(usable_docs),
                "processing_time": results["processing_time"]
            },
            "documents": usable_docs
        }, f, ensure_ascii=False, indent=2, default=str)

    print(f"ğŸ“„ è™•ç†çµæœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š å¯ç”¨æ–‡æª”æ•¸é‡: {len(usable_docs)}")

    # 5. ç°¡å–®çµ±è¨ˆ
    if usable_docs:
        avg_chunks = sum(len(doc["chunks"]) for doc in usable_docs) / len(usable_docs)
        total_chunks = sum(len(doc["chunks"]) for doc in usable_docs)

        print(f"ğŸ“ˆ ç¸½åˆ†å¡Šæ•¸: {total_chunks}")
        print(f"ğŸ“Š å¹³å‡æ¯æ–‡æª”åˆ†å¡Šæ•¸: {avg_chunks:.1f}")

        # éƒ¨é–€åˆ†ä½ˆ
        dept_dist = {}
        for doc in usable_docs:
            dept = doc["metadata"]["department"]
            dept_dist[dept] = dept_dist.get(dept, 0) + 1

        print("ğŸ¢ éƒ¨é–€åˆ†ä½ˆ:")
        for dept, count in dept_dist.items():
            print(f"  {dept}: {count} å€‹æ–‡æª”")

if __name__ == "__main__":
    main()
```

### 5.2 å¿«é€Ÿé©—è­‰è…³æœ¬

```bash
# æ¸¬è©¦è™•ç†æ•ˆæœ
python main.py ./test_documents

# æª¢æŸ¥è¼¸å‡º
cat processed_docs_*.json | jq '.processing_summary'

# æª¢æŸ¥å“è³ªåˆ†ä½ˆ
cat processed_docs_*.json | jq '.documents[].quality.grade' | sort | uniq -c
```

---

## ğŸ¯ é—œéµè¦é» (Linus Style)

### **åšå°çš„äº‹æƒ…**

1. **å…ˆè§£æ±º 80% çš„å•é¡Œ**ï¼šç°¡å–®åˆ†å¡Š + åŸºæœ¬å…ƒæ•¸æ“šå°±èƒ½è§£æ±ºå¤§éƒ¨åˆ†éœ€æ±‚
2. **æ¸¬é‡å¾Œå„ªåŒ–**ï¼šåˆ¥çŒœæ¸¬æ€§èƒ½ç“¶é ¸ï¼Œç”¨æ•¸æ“šèªªè©±
3. **å®‰å…¨ä¸èƒ½å¦¥å”**ï¼šPII æª¢æ¸¬å’ŒåŒ¿ååŒ–å¿…é ˆåšå°
4. **ä¿æŒç°¡å–®**ï¼šè¤‡é›œçš„è¨­è¨ˆé€šå¸¸æ˜¯éŒ¯èª¤è¨­è¨ˆçš„å¾µè±¡

### **é¿å…çš„é™·é˜±**

1. âŒ **éåº¦å·¥ç¨‹**: ä¸è¦ä¸€é–‹å§‹å°±æè¤‡é›œçš„èªç¾©åˆ†æ
2. âŒ **å®Œç¾ä¸»ç¾©**: ä¸è¦è©¦åœ–è™•ç†æ‰€æœ‰é‚Šç·£æƒ…æ³
3. âŒ **å…ƒæ•¸æ“šè†¨è„¹**: ä¸è¦æ”¶é›†ç”¨ä¸åˆ°çš„æ•¸æ“š
4. âŒ **å¿½è¦–æ€§èƒ½**: åˆ¥è®“è™•ç†æ™‚é–“è¶…éç”¨æˆ¶å¿å—ç¯„åœ

### **æˆåŠŸæª¢æŸ¥æ¸…å–®**

- âœ… èƒ½è™•ç†ä¼æ¥­å¸¸è¦‹æ ¼å¼ (PDF, Word, PPT)
- âœ… åˆ†å¡Šå¤§å°åˆç† (500-1500 å­—ç¬¦)
- âœ… PII æª¢æ¸¬è¦†è“‹ä¸»è¦é¡å‹
- âœ… è™•ç†é€Ÿåº¦å¯æ¥å— (ç§’ç´šè€Œéåˆ†é˜ç´š)
- âœ… éŒ¯èª¤è™•ç†å¥å…¨ (å–®å€‹æ–‡æª”å¤±æ•—ä¸å½±éŸ¿æ•´å€‹æµç¨‹)

---

## ğŸ’¡ å¯¦è¸ç·´ç¿’

### **ç·´ç¿’ 1: æ–‡æª”è™•ç†è©•ä¼°**
æ‰¾ä¸€å€‹çœŸå¯¦çš„ä¼æ¥­æ–‡æª”é›†åˆï¼Œç”¨æˆ‘å€‘çš„è™•ç†æµç¨‹è·‘ä¸€éï¼š
- çµ±è¨ˆè™•ç†æˆåŠŸç‡
- åˆ†æå“è³ªåˆ†ä½ˆ
- æª¢æŸ¥ PII æª¢æ¸¬æ•ˆæœ

### **ç·´ç¿’ 2: æ€§èƒ½æ¸¬è©¦**
æ¸¬è©¦ä¸åŒå·¥å…·åœ¨ä½ çš„ç’°å¢ƒä¸‹çš„æ€§èƒ½ï¼š
- Docling vs PyPDF vs Unstructured
- è™•ç†é€Ÿåº¦ã€æº–ç¢ºç‡ã€è³‡æºä½¿ç”¨

### **ç·´ç¿’ 3: æ”¹é€²å„ªåŒ–**
åŸºæ–¼å¯¦éš›çµæœå„ªåŒ–æµç¨‹ï¼š
- èª¿æ•´åˆ†å¡Šåƒæ•¸
- æ”¹é€²å…ƒæ•¸æ“šæå–
- å„ªåŒ–éŒ¯èª¤è™•ç†

---

## ğŸ”§ ä¸‹ä¸€æ­¥

ç¬¬2ç« æˆ‘å€‘æœƒå­¸æ··åˆæª¢ç´¢ï¼ŒæŠŠé€™äº›è™•ç†å¥½çš„æ–‡æª”è®Šæˆå¯æª¢ç´¢çš„å‘é‡ã€‚

**è¨˜ä½**: æ–‡æª”è™•ç†æ˜¯åŸºç¤ï¼Œåšä¸å¥½é€™ä¸€æ­¥ï¼Œå¾Œé¢çš„å†é«˜ç´šéƒ½æ²’ç”¨ã€‚

---

**å¯¦ç”¨æç¤º**: é€™ç« çš„ä»£ç¢¼å¯ä»¥ç›´æ¥ç”¨åœ¨ç”Ÿç”¢ç’°å¢ƒï¼Œå…ˆè·‘èµ·ä¾†ï¼Œæœ‰å•é¡Œå†å„ªåŒ–ã€‚