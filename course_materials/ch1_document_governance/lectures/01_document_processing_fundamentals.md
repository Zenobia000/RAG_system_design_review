# æ–‡æª”è™•ç†åŸºç¤ï¼šåˆ¥è®“åƒåœ¾æ•¸æ“šæ¯€äº†ä½ çš„ RAG
## ç¬¬1ç« ï¼šæŠŠä¼æ¥­æ–‡æª”è®Šæˆå¯ç”¨çš„çŸ¥è­˜

**å­¸ç¿’æ™‚é–“**: 2-3 å°æ™‚
**å‰ç½®çŸ¥è­˜**: æœƒç”¨ Pythonï¼ŒçŸ¥é“ä»€éº¼æ˜¯ PDF
**ç›®æ¨™**: å­¸æœƒè™•ç†ä¼æ¥­æ–‡æª”ï¼Œåˆ¥è¢«åƒåœ¾æ•¸æ“šå‘æ­»

---

## ğŸ¯ æ ¸å¿ƒå•é¡Œ

**ä¼æ¥­æ–‡æª” = ç½é›£ç¾å ´**

å¤§éƒ¨åˆ†ä¼æ¥­çš„æ–‡æª”ç‹€æ³ï¼š
- ğŸ“„ **æ ¼å¼æ··äº‚**: PDF/Word/PPT/Confluence åˆ°è™•éƒ½æ˜¯
- ğŸ—“ï¸ **ç‰ˆæœ¬æ··äº‚**: 2019å¹´çš„æ–‡ä»¶é‚„åœ¨ç”¨ï¼Œæ²’äººçŸ¥é“æ˜¯ä¸æ˜¯æœ€æ–°ç‰ˆ
- ğŸ” **æ‰¾ä¸åˆ°**: é—œéµä¿¡æ¯è—åœ¨æŸå€‹æ·±åº¦ç›®éŒ„çš„ Excel è¡¨æ ¼è£¡
- ğŸš« **æ¬Šé™æ··äº‚**: èª°èƒ½çœ‹ä»€éº¼ï¼Œé€£ IT éƒ½æä¸æ¸…æ¥š

**åº•ç·š**: åƒåœ¾é€²ï¼Œåƒåœ¾å‡º (GIGO)ã€‚RAG ç³»çµ±å†è°æ˜ï¼Œä¹Ÿæ•‘ä¸äº†çˆ›æ•¸æ“šã€‚

---

## ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼šå¯¦ç”¨çš„æ–‡æª”è™•ç†æµæ°´ç·š

### 1.1 æ–‡æª”è™•ç†çš„ç¾å¯¦é¸æ“‡

#### **å·¥å…·é¸å‹ï¼šç°¡å–®æœ‰æ•ˆ**

```python
# 2025å¹´å¯¦ç”¨çµ„åˆ
pip install docling              # IBMå‡ºå“ï¼ŒPDFè™•ç†æœ€å¼·
pip install unstructured         # å‚™ç”¨æ–¹æ¡ˆï¼Œæ ¼å¼æ”¯æ´å»£
pip install pypdf               # ç°¡å–®PDFï¼Œé€Ÿåº¦å¿«
```

**é¸æ“‡é‚è¼¯**:
- **ä¸»åŠ›**: Docling (æº–ç¢ºç‡95%+ï¼Œå€¼å¾—å­¸ç¿’æˆæœ¬)
- **å‚™ç”¨**: Unstructured (æ ¼å¼å…¨ï¼Œä½†æº–ç¢ºç‡å·®é»)
- **ç°¡å–®**: PyPDF (ç´”PDFå ´æ™¯ï¼Œæ€§èƒ½å¥½)

#### **å¯¦éš›ä»£ç¢¼ï¼š30è¡Œæå®šåŸºæœ¬è™•ç†**

```python
from docling.document_converter import DocumentConverter
from pathlib import Path

def process_enterprise_doc(file_path: str) -> dict:
    """è™•ç†ä¼æ¥­æ–‡æª” - 30ç§’å­¸æœƒç‰ˆæœ¬"""

    converter = DocumentConverter()

    try:
        # å°±é€™éº¼ç°¡å–®ï¼šä¸Ÿé€²å»ï¼Œæ‹¿çµæœ
        result = converter.convert(file_path)
        content = result.document.export_to_markdown()

        # åˆ¥æè¤‡é›œçµ±è¨ˆï¼Œæœ‰ç”¨çš„å°±é€™å¹¾å€‹
        return {
            "success": True,
            "content": content,
            "word_count": len(content.split()),
            "looks_good": len(content) > 100,  # å¤ªçŸ­é€šå¸¸æ˜¯å»¢æ–™
            "file_path": file_path
        }

    except Exception as e:
        # å¤±æ•—å°±å¤±æ•—ï¼Œåˆ¥éš±è—éŒ¯èª¤
        print(f"ğŸ’¥ è™•ç†å¤±æ•—: {file_path} - {str(e)}")
        return {"success": False, "error": str(e), "file_path": file_path}

# æ‰¹é‡è™•ç† - ç°¡å–®æš´åŠ›æœ‰æ•ˆ
def process_document_folder(folder_path: str) -> dict:
    """æ‰¹é‡è™•ç†æ–‡æª” - Linusé¢¨æ ¼ï¼šç°¡å–®ç²—æš´æœ‰æ•ˆ"""

    from pathlib import Path
    import time

    print(f"ğŸš€ é–‹å§‹è™•ç†: {folder_path}")
    start_time = time.time()

    # æ‰¾æ–‡ä»¶ï¼šæ”¯æ´å¸¸è¦‹æ ¼å¼å°±å¤ äº†
    supported = {'.pdf', '.docx', '.pptx', '.md', '.txt'}
    files = [f for f in Path(folder_path).rglob('*')
             if f.suffix.lower() in supported]

    print(f"ğŸ“„ æ‰¾åˆ° {len(files)} å€‹æ–‡ä»¶")

    # è™•ç†æ–‡ä»¶ï¼šåˆ¥ä¸¦è¡Œï¼Œç°¡å–®å¾ªç’°å°±å¥½
    successful = []
    failed = []

    for file_path in files:
        result = process_enterprise_doc(str(file_path))

        if result["success"] and result["looks_good"]:
            successful.append(result)
            print(f"âœ… {file_path.name}")
        else:
            failed.append(result)
            print(f"âŒ {file_path.name}")

    elapsed = time.time() - start_time
    print(f"â±ï¸ å®Œæˆ! {len(successful)}/{len(files)} æˆåŠŸï¼Œè€—æ™‚ {elapsed:.1f}ç§’")

    return {"successful": successful, "failed": failed, "stats": {
        "total": len(files), "success_rate": len(successful)/len(files)*100
    }}
```

### 1.2 æ–‡æª”åˆ†å¡Šï¼šåˆ¥æƒ³å¤ªè¤‡é›œ

#### **åˆ†å¡Šç­–ç•¥ï¼šå¯¦ç”¨ä¸»ç¾©**

å­¸è¡“ç•Œå–œæ­¡æè¤‡é›œçš„"èªç¾©åˆ†å¡Š"ã€‚ç¾å¯¦ä¸­ï¼Œç°¡å–®çš„è¦å‰‡åˆ†å¡Šå°±å¤ ç”¨ï¼š

```python
from langchain.text_splitters import RecursiveCharacterTextSplitter

def chunk_document(content: str) -> list:
    """æ–‡æª”åˆ†å¡Š - ä¸€å€‹é…ç½®æå®šæ‰€æœ‰å ´æ™¯"""

    from langchain.text_splitters import RecursiveCharacterTextSplitter

    # åˆ¥æè¤‡é›œé…ç½®ï¼Œä¸€å€‹åƒæ•¸çµ„åˆæ‡‰ä»˜80%å ´æ™¯
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,        # 1000å­—ç¬¦ï¼Œç¶“é©—æœ€ä½³å€¼
        chunk_overlap=200,      # 20%é‡ç–Šï¼Œé˜²æ­¢åˆ‡æ–·é—œéµä¿¡æ¯
        separators=["\n\n", "\n", "ã€‚", ".", " "]
    )

    chunks = splitter.split_text(content)

    # ç°¡å–®åŒ…è£ï¼Œåˆ¥æå¤ªå¤šå…ƒæ•¸æ“š
    return [{"text": chunk, "index": i} for i, chunk in enumerate(chunks)]

# æ¸¬è©¦ä½ çš„åˆ†å¡Šæ•ˆæœ
def test_chunking_quality(content: str) -> None:
    """å¿«é€Ÿæ¸¬è©¦åˆ†å¡Šå“è³ª"""

    chunks = chunk_document(content)

    print(f"ğŸ“Š åˆ†å¡Šçµ±è¨ˆ:")
    print(f"  åŸæ–‡: {len(content)} å­—ç¬¦")
    print(f"  åˆ†å¡Š: {len(chunks)} å€‹")
    print(f"  å¹³å‡: {len(content)//len(chunks)} å­—ç¬¦/å¡Š")
    print(f"  æœ€çŸ­: {min(len(c['text']) for c in chunks)}")
    print(f"  æœ€é•·: {max(len(c['text']) for c in chunks)}")

    # çœ‹çœ‹åˆ†å¡Šé‚Šç•Œæ˜¯å¦åˆç†
    if len(chunks) > 1:
        print(f"ğŸ“‹ åˆ†å¡Šç¤ºä¾‹:")
        print(f"  ç¬¬1å¡Šæœ«å°¾: ...{chunks[0]['text'][-50:]}")
        print(f"  ç¬¬2å¡Šé–‹é ­: {chunks[1]['text'][:50]}...")

# æ¸¬è©¦æ•ˆæœ
def test_chunking():
    """æ¸¬è©¦åˆ†å¡Šæ•ˆæœ"""

    sample_text = "ä½ çš„æ¸¬è©¦æ–‡æª”å…§å®¹..."
    chunks = smart_chunk_document(sample_text, "technical")

    print(f"åŸæ–‡é•·åº¦: {len(sample_text)}")
    print(f"åˆ†å¡Šæ•¸é‡: {len(chunks)}")
    print(f"å¹³å‡åˆ†å¡Šé•·åº¦: {sum(len(c['text']) for c in chunks) / len(chunks):.0f}")

    return chunks
```

#### **ç‚ºä»€éº¼ä¸ç”¨è¤‡é›œçš„èªç¾©åˆ†å¡Šï¼Ÿ**

**Linus è§€é»**:
> "è¤‡é›œçš„ç®—æ³•é€šå¸¸æ˜¯ç‚ºäº†æ©è“‹è¨­è¨ˆå•é¡Œã€‚å¥½çš„è¨­è¨ˆæ‡‰è©²è®“ç°¡å–®çš„ç®—æ³•å°±èƒ½å·¥ä½œã€‚"

**ç¾å¯¦æª¢é©—**:
- âœ… **ç°¡å–®åˆ†å¡Š** + **å¥½çš„æª¢ç´¢** = 95% å ´æ™¯å¤ ç”¨
- âŒ **è¤‡é›œåˆ†å¡Š** + **æ™®é€šæª¢ç´¢** = éåº¦å·¥ç¨‹ï¼Œæ€§èƒ½é‚„å¯èƒ½æ›´å·®
- ğŸ¯ **å…ˆåšç°¡å–®ç‰ˆæœ¬ï¼Œæ¸¬é‡æ•ˆæœï¼Œç¢ºå¯¦ä¸å¤ å†å„ªåŒ–**

---

## ğŸ’¾ å…ƒæ•¸æ“šï¼šåªè¦æœ‰ç”¨çš„

### 2.1 æœ€å°å¯è¡Œå…ƒæ•¸æ“š

**åˆ¥æè¤‡é›œçš„æœ¬é«”è«–ï¼** ä¼æ¥­éœ€è¦çš„å…ƒæ•¸æ“šå¾ˆç°¡å–®ï¼š

```python
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List
import hashlib

def extract_metadata(file_path: str, content: str) -> dict:
    """æå–æ–‡æª”å…ƒæ•¸æ“š - å¯¦ç”¨ç‰ˆæœ¬ï¼Œåˆ¥æè¤‡é›œçš„é¡å®šç¾©"""

    import os
    import hashlib
    from pathlib import Path

    # åŸºæœ¬ä¿¡æ¯ï¼šå¿…é ˆæœ‰çš„
    doc_id = hashlib.md5(file_path.encode()).hexdigest()[:12]  # çŸ­é»å°±å¤ 
    title = Path(file_path).stem.replace('_', ' ').replace('-', ' ')

    # å¾è·¯å¾‘çŒœæ¸¬éƒ¨é–€å’Œé¡å‹ - ç°¡å–®ç²—æš´ä½†æœ‰æ•ˆ
    path_lower = file_path.lower()

    if any(x in path_lower for x in ['eng', 'tech', 'dev']):
        department = 'engineering'
    elif any(x in path_lower for x in ['legal', 'compliance']):
        department = 'legal'
    elif any(x in path_lower for x in ['hr', 'people']):
        department = 'hr'
    else:
        department = 'general'

    if any(x in path_lower for x in ['manual', 'guide', 'howto']):
        doc_type = 'manual'
    elif any(x in path_lower for x in ['policy', 'procedure', 'rule']):
        doc_type = 'policy'
    elif any(x in path_lower for x in ['spec', 'design', 'api']):
        doc_type = 'tech_spec'
    else:
        doc_type = 'general'

    # æ™‚é–“ä¿¡æ¯
    try:
        stat = os.stat(file_path)
        modified = datetime.fromtimestamp(stat.st_mtime)
    except:
        modified = datetime.now()

    return {
        'id': doc_id,
        'title': title,
        'file_path': file_path,
        'department': department,
        'type': doc_type,
        'modified': modified,
        'word_count': len(content.split()),
        'is_old': (datetime.now() - modified).days > 365  # è¶…é1å¹´ç®—èˆŠ
    }

def extract_simple_metadata(file_path: str, content: str) -> SimpleDocumentMetadata:
    """æå–ç°¡å–®å¯¦ç”¨çš„å…ƒæ•¸æ“š"""

    from pathlib import Path
    import os

    file_info = Path(file_path)

    # å¾æ–‡ä»¶è·¯å¾‘æ¨æ–·ä¿¡æ¯
    department = "general"
    doc_type = "general"

    # ç°¡å–®çš„è·¯å¾‘åˆ†æ
    path_parts = str(file_info).lower().split('/')

    if any(dept in path_parts for dept in ["engineering", "tech", "dev"]):
        department = "engineering"
    elif any(dept in path_parts for dept in ["legal", "compliance"]):
        department = "legal"
    elif any(dept in path_parts for dept in ["hr", "people"]):
        department = "hr"

    if any(type_hint in path_parts for type_hint in ["manual", "guide"]):
        doc_type = "manual"
    elif any(type_hint in path_parts for type_hint in ["policy", "procedure"]):
        doc_type = "policy"
    elif any(type_hint in path_parts for type_hint in ["spec", "design"]):
        doc_type = "tech_spec"

    # å¾æ–‡ä»¶åæå–æ¨™é¡Œ
    title = file_info.stem.replace('_', ' ').replace('-', ' ').title()

    # æ™‚é–“ä¿¡æ¯
    try:
        file_stat = os.stat(file_path)
        created_at = datetime.fromtimestamp(file_stat.st_ctime)
        modified_at = datetime.fromtimestamp(file_stat.st_mtime)
    except:
        created_at = modified_at = datetime.now()

    return SimpleDocumentMetadata(
        doc_id=hashlib.md5(file_path.encode()).hexdigest()[:16],
        title=title,
        file_path=file_path,
        content_hash=hashlib.md5(content.encode()).hexdigest(),
        document_type=doc_type,
        department=department,
        created_at=created_at,
        modified_at=modified_at,
        processed_at=datetime.now(),
        word_count=len(content.split()),
        keywords=extract_simple_keywords(content)  # ç°¡å–®é—œéµè©æå–
    )

def extract_simple_keywords(content: str, max_keywords: int = 10) -> List[str]:
    """ç°¡å–®çš„é—œéµè©æå–"""

    # ç§»é™¤å¸¸ç”¨è©
    stopwords = {
        "çš„", "å’Œ", "åœ¨", "æ˜¯", "æœ‰", "ä¸", "äº†", "å¯ä»¥", "é€™å€‹", "é‚£å€‹",
        "the", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with"
    }

    # ç°¡å–®è©é »çµ±è¨ˆ
    words = content.lower().split()
    word_freq = {}

    for word in words:
        if len(word) > 2 and word not in stopwords:
            word_freq[word] = word_freq.get(word, 0) + 1

    # è¿”å›é«˜é »è©
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    return [word for word, freq in top_words[:max_keywords]]
```

### 2.2 æ–‡æª”å“è³ªï¼šåªæª¢æŸ¥é‡è¦çš„

**å¿˜æ‰è¤‡é›œçš„å“è³ªæ¨¡å‹ï¼** å¯¦éš›ä¸Šåªéœ€è¦æª¢æŸ¥å¹¾å€‹é—œéµé»ï¼š

```python
def quality_check(content: str, metadata: dict) -> dict:
    """æ–‡æª”å“è³ªæª¢æŸ¥ - å¯¦ç”¨ç‰ˆæœ¬ï¼Œåªæª¢æŸ¥æœƒå‡ºäº‹çš„å•é¡Œ"""

    issues = []

    # 1. æ˜é¡¯çš„å•é¡Œ
    if len(content) < 50:
        return {"usable": False, "issue": "æ–‡æª”å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯ç©ºçš„"}

    # 2. äº‚ç¢¼æª¢æŸ¥ - é€™å€‹æœƒæå£ RAG
    weird_chars = sum(1 for c in content[:1000] if not c.isprintable() and c not in '\n\t')
    if weird_chars > 50:  # å‰1000å­—ç¬¦æœ‰50å€‹ä»¥ä¸Šå¥‡æ€ªå­—ç¬¦
        return {"usable": False, "issue": "å¯èƒ½æœ‰äº‚ç¢¼"}

    # 3. é‡è¤‡åƒåœ¾æª¢æŸ¥
    lines = [line.strip() for line in content.split('\n') if line.strip()]
    if len(set(lines)) < len(lines) * 0.3:  # 70%ä»¥ä¸Šé‡è¤‡è¡Œ
        return {"usable": False, "issue": "é‡è¤‡å…§å®¹å¤ªå¤š"}

    # 4. æ™‚æ•ˆæ€§è­¦å‘Š
    if metadata.get('is_old', False):
        issues.append("æ–‡æª”å¯èƒ½å·²éæ™‚")

    return {
        "usable": True,
        "issues": issues,
        "warning_count": len(issues)
    }
```

---

## ğŸ“ å¯¦éš›çš„æ–‡æª”è™•ç†æµæ°´ç·š

### 3.1 å®Œæ•´çš„è™•ç†æµç¨‹

```python
import os
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor
import time

class DocumentProcessor:
    """æ–‡æª”è™•ç†å™¨ - Linusé¢¨æ ¼ï¼šç°¡å–®é…ç½®ï¼Œå°ˆæ³¨æ ¸å¿ƒåŠŸèƒ½"""

    def __init__(self):
        from docling.document_converter import DocumentConverter
        self.converter = DocumentConverter()

        # é…ç½®ï¼šç°¡å–®æ˜ç¢ºï¼Œåˆ¥æä¸€å †é¸é …
        self.max_size_mb = 50     # å¤§æ–‡ä»¶ç›´æ¥è·³é
        self.timeout = 60         # 60ç§’æä¸å®šå°±ç®—äº†
        self.formats = {".pdf", ".docx", ".pptx", ".md", ".txt"}

    def process_folder(self, folder_path: str) -> Dict:
        """è™•ç†æ–‡æª”æ–‡ä»¶å¤¾"""

        print(f"ğŸš€ é–‹å§‹è™•ç†æ–‡ä»¶å¤¾: {folder_path}")
        start_time = time.time()

        # æ‰¾åˆ°æ‰€æœ‰æ”¯æ´çš„æ–‡æª”
        all_files = []
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                if Path(file_path).suffix.lower() in self.config["supported_formats"]:
                    # æª¢æŸ¥æ–‡ä»¶å¤§å°
                    size_mb = os.path.getsize(file_path) / (1024 * 1024)
                    if size_mb <= self.config["max_file_size_mb"]:
                        all_files.append(file_path)
                    else:
                        print(f"âš ï¸  è·³éå¤§æ–‡ä»¶ ({size_mb:.1f}MB): {file_path}")

        print(f"ğŸ“„ æ‰¾åˆ° {len(all_files)} å€‹å¯è™•ç†æ–‡ä»¶")

        # ä¸¦è¡Œè™•ç† (ä½†åˆ¥é–‹å¤ªå¤šç·šç¨‹)
        max_workers = min(4, len(all_files))
        processed_docs = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(self._process_single_file, all_files))

        # çµ±è¨ˆçµæœ
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]

        processing_time = time.time() - start_time

        print(f"âœ… è™•ç†å®Œæˆ: {len(successful)} æˆåŠŸ, {len(failed)} å¤±æ•—")
        print(f"â±ï¸  ç¸½è€—æ™‚: {processing_time:.1f} ç§’")

        return {
            "total_files": len(all_files),
            "successful": len(successful),
            "failed": len(failed),
            "processing_time": processing_time,
            "successful_docs": successful,
            "failed_docs": failed
        }

    def _process_single_file(self, file_path: str) -> Dict:
        """è™•ç†å–®å€‹æ–‡ä»¶"""

        try:
            # 1. è½‰æ›æ–‡æª”
            result = process_enterprise_doc(file_path)

            if not result["success"]:
                return result

            content = result["content"]

            # 2. æå–å…ƒæ•¸æ“š
            metadata = extract_simple_metadata(file_path, content)

            # 3. å“è³ªæª¢æŸ¥
            quality = simple_quality_check(content, metadata)

            # 4. åˆ†å¡Šè™•ç†
            chunks = smart_chunk_document(content, metadata.document_type)

            # 5. çµ„è£æœ€çµ‚çµæœ
            return {
                "success": True,
                "file_path": file_path,
                "metadata": metadata.__dict__,
                "quality": quality,
                "chunks": chunks,
                "usable": quality["usable"]
            }

        except Exception as e:
            return {
                "success": False,
                "file_path": file_path,
                "error": str(e)
            }
```

---

## ğŸš¨ PII æª¢æ¸¬ï¼šä¸èƒ½é¦¬è™çš„å®‰å…¨æª¢æŸ¥

### 4.1 å¯¦ç”¨çš„ PII æª¢æ¸¬

```python
from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine

def setup_simple_pii_detector():
    """è¨­ç½®ç°¡å–®å¯¦ç”¨çš„ PII æª¢æ¸¬å™¨"""

    analyzer = AnalyzerEngine()
    anonymizer = AnonymizerEngine()

    # ä¼æ¥­å¸¸è¦‹çš„æ•æ„Ÿä¿¡æ¯é¡å‹
    pii_types = [
        "PERSON",           # äººå
        "EMAIL_ADDRESS",    # éƒµç®±
        "PHONE_NUMBER",     # é›»è©±
        "CREDIT_CARD",      # ä¿¡ç”¨å¡
        "US_SSN",          # èº«ä»½è­‰è™Ÿ
        "IP_ADDRESS"        # IPåœ°å€
    ]

    return analyzer, anonymizer, pii_types

def check_for_sensitive_info(content: str) -> dict:
    """æª¢æŸ¥æ•æ„Ÿä¿¡æ¯ - ç°¡åŒ–ç‰ˆï¼ŒæŠ“ä¸»è¦é¢¨éšªå°±å¤ äº†"""

    # ç°¡å–®æ­£å‰‡è¡¨é”å¼æª¢æ¸¬å¸¸è¦‹æ•æ„Ÿä¿¡æ¯
    import re

    patterns = {
        "email": r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        "phone": r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        "ssn": r'\b\d{3}-\d{2}-\d{4}\b',
        "credit_card": r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'
    }

    detected = {}
    for pii_type, pattern in patterns.items():
        matches = re.findall(pattern, content)
        if matches:
            detected[pii_type] = len(matches)

    # ç°¡å–®é¢¨éšªè©•ä¼°
    if "credit_card" in detected or "ssn" in detected:
        risk = "high"
    elif len(detected) >= 2:
        risk = "medium"
    elif detected:
        risk = "low"
    else:
        risk = "safe"

    return {
        "has_sensitive_info": bool(detected),
        "risk_level": risk,
        "detected_types": list(detected.keys()),
        "total_matches": sum(detected.values()),
        "action": "anonymize" if risk in ["high", "medium"] else "proceed"
    }

def simple_anonymize(content: str, sensitive_check: dict) -> str:
    """ç°¡å–®åŒ¿ååŒ– - ç›´æ¥æ›¿æ›ï¼Œåˆ¥æè¤‡é›œç®—æ³•"""

    if not sensitive_check["has_sensitive_info"]:
        return content

    import re

    # æš´åŠ›æ›¿æ›æ³•ï¼šç°¡å–®æœ‰æ•ˆ
    replacements = {
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b': '[EMAIL]',
        r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b': '[PHONE]',
        r'\b\d{3}-\d{2}-\d{4}\b': '[SSN]',
        r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b': '[CARD]'
    }

    anonymized = content
    for pattern, replacement in replacements.items():
        anonymized = re.sub(pattern, replacement, anonymized)

    return anonymized

def anonymize_if_needed(content: str, pii_check: Dict) -> str:
    """å¿…è¦æ™‚é€²è¡ŒåŒ¿ååŒ–"""

    if not pii_check["needs_anonymization"]:
        return content

    analyzer, anonymizer, _ = setup_simple_pii_detector()

    # é‡æ–°åˆ†æ (ç²å–ä½ç½®ä¿¡æ¯)
    pii_results = analyzer.analyze(text=content, language="en")

    # ç°¡å–®æ›¿æ›ç­–ç•¥
    anonymized_result = anonymizer.anonymize(
        text=content,
        analyzer_results=pii_results,
        operators={
            "PERSON": {"type": "replace", "new_value": "[PERSON]"},
            "EMAIL_ADDRESS": {"type": "replace", "new_value": "[EMAIL]"},
            "PHONE_NUMBER": {"type": "replace", "new_value": "[PHONE]"},
            "CREDIT_CARD": {"type": "replace", "new_value": "[CARD]"},
            "US_SSN": {"type": "replace", "new_value": "[SSN]"}
        }
    )

    return anonymized_result.text
```

---

## ğŸƒâ€â™‚ï¸ å¿«é€Ÿé–‹å§‹æŒ‡å—

### 5.1 30åˆ†é˜æ­å»ºæ–‡æª”è™•ç†ç³»çµ±

```python
# å®Œæ•´çš„æ–‡æª”è™•ç†è…³æœ¬ - æ‹¿ä¾†å°±ç”¨
def process_all_documents(folder_path: str) -> str:
    """ä¸€å€‹å‡½æ•¸æå®šæ‰€æœ‰æ–‡æª”è™•ç†"""

    import json
    import time
    from pathlib import Path

    print(f"ğŸš€ é–‹å§‹è™•ç†ä¼æ¥­æ–‡æª”: {folder_path}")

    # 1. æ‰¹é‡è™•ç†
    results = process_document_folder(folder_path)
    successful_docs = results["successful"]

    # 2. è™•ç†æ¯å€‹æˆåŠŸçš„æ–‡æª”
    final_docs = []
    for doc in successful_docs:
        # æå–å…ƒæ•¸æ“š
        metadata = extract_metadata(doc["file_path"], doc["content"])

        # å“è³ªæª¢æŸ¥
        quality = quality_check(doc["content"], metadata)

        if quality["usable"]:
            # PII æª¢æŸ¥
            pii_check = check_for_sensitive_info(doc["content"])

            # å¿…è¦æ™‚åŒ¿ååŒ–
            clean_content = simple_anonymize(doc["content"], pii_check)

            # åˆ†å¡Š
            chunks = chunk_document(clean_content)

            final_docs.append({
                "metadata": metadata,
                "content": clean_content,
                "chunks": chunks,
                "quality": quality,
                "pii_info": pii_check
            })

    # 3. ä¿å­˜çµæœ
    output_file = f"enterprise_knowledge_base_{int(time.time())}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "summary": {
                "total_processed": len(successful_docs),
                "usable_documents": len(final_docs),
                "total_chunks": sum(len(doc["chunks"]) for doc in final_docs),
                "processing_date": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "documents": final_docs
        }, f, ensure_ascii=False, indent=2, default=str)

    print(f"âœ… å®Œæˆ! å¯ç”¨æ–‡æª”: {len(final_docs)}")
    print(f"ğŸ“„ çµæœä¿å­˜åœ¨: {output_file}")

    return output_file

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print("ç”¨æ³•: python doc_processor.py <æ–‡æª”æ–‡ä»¶å¤¾>")
        print("ä¾‹å­: python doc_processor.py ./company_docs")
        sys.exit(1)

    output_file = process_all_documents(sys.argv[1])
    print(f"ğŸ‰ çŸ¥è­˜åº«æº–å‚™å®Œæˆ: {output_file}")
```

### 5.2 å¿«é€Ÿé©—è­‰è…³æœ¬

```bash
# æ¸¬è©¦è™•ç†æ•ˆæœ
python main.py ./test_documents

# æª¢æŸ¥è¼¸å‡º
cat processed_docs_*.json | jq '.processing_summary'

# æª¢æŸ¥å“è³ªåˆ†ä½ˆ
cat processed_docs_*.json | jq '.documents[].quality.grade' | sort | uniq -c
```

---

## ğŸ¯ é—œéµè¦é» (Linus Style)

### **åšå°çš„äº‹æƒ…**

1. **å…ˆè§£æ±º 80% çš„å•é¡Œ**ï¼šç°¡å–®åˆ†å¡Š + åŸºæœ¬å…ƒæ•¸æ“šå°±èƒ½è§£æ±ºå¤§éƒ¨åˆ†éœ€æ±‚
2. **æ¸¬é‡å¾Œå„ªåŒ–**ï¼šåˆ¥çŒœæ¸¬æ€§èƒ½ç“¶é ¸ï¼Œç”¨æ•¸æ“šèªªè©±
3. **å®‰å…¨ä¸èƒ½å¦¥å”**ï¼šPII æª¢æ¸¬å’ŒåŒ¿ååŒ–å¿…é ˆåšå°
4. **ä¿æŒç°¡å–®**ï¼šè¤‡é›œçš„è¨­è¨ˆé€šå¸¸æ˜¯éŒ¯èª¤è¨­è¨ˆçš„å¾µè±¡

### **é¿å…çš„é™·é˜±**

1. âŒ **éåº¦å·¥ç¨‹**: ä¸è¦ä¸€é–‹å§‹å°±æè¤‡é›œçš„èªç¾©åˆ†æ
2. âŒ **å®Œç¾ä¸»ç¾©**: ä¸è¦è©¦åœ–è™•ç†æ‰€æœ‰é‚Šç·£æƒ…æ³
3. âŒ **å…ƒæ•¸æ“šè†¨è„¹**: ä¸è¦æ”¶é›†ç”¨ä¸åˆ°çš„æ•¸æ“š
4. âŒ **å¿½è¦–æ€§èƒ½**: åˆ¥è®“è™•ç†æ™‚é–“è¶…éç”¨æˆ¶å¿å—ç¯„åœ

### **æˆåŠŸæª¢æŸ¥æ¸…å–®**

- âœ… èƒ½è™•ç†ä¼æ¥­å¸¸è¦‹æ ¼å¼ (PDF, Word, PPT)
- âœ… åˆ†å¡Šå¤§å°åˆç† (500-1500 å­—ç¬¦)
- âœ… PII æª¢æ¸¬è¦†è“‹ä¸»è¦é¡å‹
- âœ… è™•ç†é€Ÿåº¦å¯æ¥å— (ç§’ç´šè€Œéåˆ†é˜ç´š)
- âœ… éŒ¯èª¤è™•ç†å¥å…¨ (å–®å€‹æ–‡æª”å¤±æ•—ä¸å½±éŸ¿æ•´å€‹æµç¨‹)

---

## ğŸ’¡ å¯¦è¸ç·´ç¿’

### **ç·´ç¿’ 1: æ–‡æª”è™•ç†è©•ä¼°**
æ‰¾ä¸€å€‹çœŸå¯¦çš„ä¼æ¥­æ–‡æª”é›†åˆï¼Œç”¨æˆ‘å€‘çš„è™•ç†æµç¨‹è·‘ä¸€éï¼š
- çµ±è¨ˆè™•ç†æˆåŠŸç‡
- åˆ†æå“è³ªåˆ†ä½ˆ
- æª¢æŸ¥ PII æª¢æ¸¬æ•ˆæœ

### **ç·´ç¿’ 2: æ€§èƒ½æ¸¬è©¦**
æ¸¬è©¦ä¸åŒå·¥å…·åœ¨ä½ çš„ç’°å¢ƒä¸‹çš„æ€§èƒ½ï¼š
- Docling vs PyPDF vs Unstructured
- è™•ç†é€Ÿåº¦ã€æº–ç¢ºç‡ã€è³‡æºä½¿ç”¨

### **ç·´ç¿’ 3: æ”¹é€²å„ªåŒ–**
åŸºæ–¼å¯¦éš›çµæœå„ªåŒ–æµç¨‹ï¼š
- èª¿æ•´åˆ†å¡Šåƒæ•¸
- æ”¹é€²å…ƒæ•¸æ“šæå–
- å„ªåŒ–éŒ¯èª¤è™•ç†

---

## ğŸ”§ ä¸‹ä¸€æ­¥

ç¬¬2ç« æˆ‘å€‘æœƒå­¸æ··åˆæª¢ç´¢ï¼ŒæŠŠé€™äº›è™•ç†å¥½çš„æ–‡æª”è®Šæˆå¯æª¢ç´¢çš„å‘é‡ã€‚

**è¨˜ä½**: æ–‡æª”è™•ç†æ˜¯åŸºç¤ï¼Œåšä¸å¥½é€™ä¸€æ­¥ï¼Œå¾Œé¢çš„å†é«˜ç´šéƒ½æ²’ç”¨ã€‚

---

**å¯¦ç”¨æç¤º**: é€™ç« çš„ä»£ç¢¼å¯ä»¥ç›´æ¥ç”¨åœ¨ç”Ÿç”¢ç’°å¢ƒï¼Œå…ˆè·‘èµ·ä¾†ï¼Œæœ‰å•é¡Œå†å„ªåŒ–ã€‚