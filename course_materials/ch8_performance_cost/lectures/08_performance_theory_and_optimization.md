# æ€§èƒ½ç†è«–èˆ‡ç³»çµ±å„ªåŒ–
## å¤§å­¸æ•™ç§‘æ›¸ ç¬¬8ç« ï¼šä¼æ¥­ç´šç³»çµ±çš„æ€§èƒ½å·¥ç¨‹èˆ‡æˆæœ¬æ§åˆ¶

**èª²ç¨‹ç·¨è™Ÿ**: CS785 - ä¼æ¥­ç´šæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±
**ç« ç¯€**: ç¬¬8ç«  æ€§èƒ½èˆ‡æˆæœ¬å·¥ç¨‹
**å­¸ç¿’æ™‚æ•¸**: 8å°æ™‚
**å…ˆä¿®èª²ç¨‹**: æ¼”ç®—æ³•åˆ†æ, ç³»çµ±æ€§èƒ½, ç¬¬0-7ç« 
**ä½œè€…**: æ€§èƒ½å·¥ç¨‹ç ”ç©¶åœ˜éšŠ
**æœ€å¾Œæ›´æ–°**: 2025-01-06

---

## ğŸ“š å­¸ç¿’ç›®æ¨™ (Learning Objectives)

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œå­¸ç”Ÿæ‡‰èƒ½å¤ :

1. **æ€§èƒ½ç†è«–**: æŒæ¡åˆ†æ•£å¼ç³»çµ±æ€§èƒ½åˆ†æçš„æ•¸å­¸æ¨¡å‹å’Œæœ€ä½³åŒ–ç†è«–
2. **æ¶æ§‹è¨­è¨ˆ**: è¨­è¨ˆå¯æ“´å±•çš„é«˜æ€§èƒ½ RAG ç³»çµ±æ¶æ§‹
3. **æˆæœ¬å»ºæ¨¡**: å»ºç«‹å®Œæ•´çš„æˆæœ¬åˆ†ææ¨¡å‹å’Œé æ¸¬æ¡†æ¶
4. **å„ªåŒ–ç­–ç•¥**: å¯¦æ–½å¤šå±¤æ¬¡çš„æ€§èƒ½å„ªåŒ–å’Œæˆæœ¬æ§åˆ¶ç­–ç•¥

---

## 1. åˆ†æ•£å¼ç³»çµ±æ€§èƒ½ç†è«–

### 1.1 æ’éšŠç†è«–åœ¨ RAG ç³»çµ±ä¸­çš„æ‡‰ç”¨

#### **Little's Law åœ¨ LLM æœå‹™ä¸­çš„é«”ç¾**

**å®šç† 1.1** (Little's Law): å°æ–¼ç©©å®šçš„æ’éšŠç³»çµ±ï¼š

$$L = \lambda W$$

å…¶ä¸­ï¼š
- $L$: ç³»çµ±ä¸­çš„å¹³å‡è«‹æ±‚æ•¸ (ä½‡åˆ—é•·åº¦)
- $\lambda$: å¹³å‡åˆ°é”ç‡ (requests/second)
- $W$: å¹³å‡å›æ‡‰æ™‚é–“ (seconds)

**RAG ç³»çµ±æ‡‰ç”¨**: å°æ–¼ LLM æ¨ç†æœå‹™ï¼š

$$\text{Concurrent\_Requests} = \text{QPS} \times \text{Avg\_Response\_Time}$$

**æ¨è«– 1.1** (å®¹é‡è¦åŠƒ): è¦æ”¯æ´ç›®æ¨™ QPS $Q$ ä¸”ç¶­æŒå›æ‡‰æ™‚é–“ $W$ çš„ SLAï¼Œç³»çµ±éœ€è¦æ”¯æ´çš„æœ€å¤§ä¸¦è¡Œè«‹æ±‚æ•¸ç‚ºï¼š

$$\text{Max\_Parallel} = Q \times W \times \text{Safety\_Factor}$$

#### **æ’éšŠæ¨¡å‹çš„æ•¸å­¸åˆ†æ**

**æ¨¡å‹ 1.1** (M/M/c æ’éšŠæ¨¡å‹ for LLM Serving):

å°æ–¼ Poisson åˆ°é”ã€æŒ‡æ•¸æœå‹™æ™‚é–“ã€c å€‹æœå‹™å™¨çš„ç³»çµ±ï¼š

**åˆ©ç”¨ç‡**: $\rho = \frac{\lambda}{c \mu}$

**å¹³å‡ä½‡åˆ—é•·åº¦**: $L_q = \frac{\rho^{c+1}}{(c-1)!(c-\rho)^2} \cdot P_0$

**å¹³å‡ç­‰å¾…æ™‚é–“**: $W_q = \frac{L_q}{\lambda}$

å…¶ä¸­ $P_0$ ç‚ºç³»çµ±ç©ºé–’æ¦‚ç‡ï¼š

$$P_0 = \left[\sum_{n=0}^{c-1} \frac{\rho^n}{n!} + \frac{\rho^c}{c!(1-\rho/c)}\right]^{-1}$$

#### **æ€§èƒ½å»ºæ¨¡å¯¦ç¾**

```python
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import asyncio
from scipy.special import factorial
from collections import deque

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    timestamp: datetime
    request_rate: float           # requests/second
    response_time_p50: float      # milliseconds
    response_time_p95: float      # milliseconds
    response_time_p99: float      # milliseconds
    queue_length: int
    cpu_utilization: float        # 0-1
    gpu_utilization: float        # 0-1
    memory_utilization: float     # 0-1
    error_rate: float            # 0-1

class PerformanceModeler:
    """æ€§èƒ½å»ºæ¨¡å™¨"""

    def __init__(self):
        self.historical_metrics = deque(maxlen=10000)
        self.model_parameters = {}

    async def analyze_system_performance(self, metrics_history: List[PerformanceMetrics]) -> Dict:
        """åˆ†æç³»çµ±æ€§èƒ½ç‰¹å¾µ"""

        if len(metrics_history) < 100:
            return {"error": "Insufficient historical data"}

        # 1. åŸºç¤çµ±è¨ˆåˆ†æ
        basic_stats = await self._calculate_basic_statistics(metrics_history)

        # 2. æ’éšŠæ¨¡å‹åƒæ•¸ä¼°è¨ˆ
        queueing_params = await self._estimate_queueing_parameters(metrics_history)

        # 3. æ€§èƒ½ç“¶é ¸åˆ†æ
        bottleneck_analysis = await self._identify_performance_bottlenecks(metrics_history)

        # 4. å®¹é‡é æ¸¬
        capacity_prediction = await self._predict_capacity_requirements(
            metrics_history, queueing_params
        )

        # 5. æœ€ä½³åŒ–å»ºè­°
        optimization_recommendations = await self._generate_optimization_recommendations(
            basic_stats, bottleneck_analysis, capacity_prediction
        )

        return {
            "basic_statistics": basic_stats,
            "queueing_model": queueing_params,
            "bottleneck_analysis": bottleneck_analysis,
            "capacity_prediction": capacity_prediction,
            "optimization_recommendations": optimization_recommendations
        }

    async def _estimate_queueing_parameters(self, metrics: List[PerformanceMetrics]) -> Dict:
        """ä¼°è¨ˆæ’éšŠæ¨¡å‹åƒæ•¸"""

        # æå–é—œéµæ•¸æ“š
        arrival_rates = [m.request_rate for m in metrics]
        response_times = [m.response_time_p50 / 1000.0 for m in metrics]  # è½‰ç§’
        queue_lengths = [m.queue_length for m in metrics]

        # ä¼°è¨ˆåˆ°é”ç‡ Î»
        lambda_estimate = np.mean(arrival_rates)

        # ä¼°è¨ˆæœå‹™ç‡ Î¼ (åŸºæ–¼å›æ‡‰æ™‚é–“)
        service_times = [rt for rt in response_times if rt > 0]
        if service_times:
            mu_estimate = 1.0 / np.mean(service_times)
        else:
            mu_estimate = 1.0

        # ä¼°è¨ˆæœå‹™å™¨æ•¸é‡ c (åŸºæ–¼å¹³å‡åˆ©ç”¨ç‡)
        utilizations = [
            max(m.cpu_utilization, m.gpu_utilization) for m in metrics
        ]
        avg_utilization = np.mean(utilizations)

        # c = Î» / (Î¼ * Ï)ï¼Œå…¶ä¸­ Ï æ˜¯ç›®æ¨™åˆ©ç”¨ç‡
        if avg_utilization > 0:
            c_estimate = max(1, int(lambda_estimate / (mu_estimate * avg_utilization)))
        else:
            c_estimate = 1

        # é©—è­‰æ¨¡å‹å‡è¨­
        model_assumptions = await self._validate_model_assumptions(metrics)

        return {
            "lambda": lambda_estimate,          # åˆ°é”ç‡
            "mu": mu_estimate,                  # æœå‹™ç‡
            "c": c_estimate,                    # æœå‹™å™¨æ•¸é‡
            "rho": lambda_estimate / (c_estimate * mu_estimate),  # åˆ©ç”¨ç‡
            "model_type": "M/M/c" if model_assumptions["poisson_arrivals"] else "G/G/c",
            "assumptions_validated": model_assumptions
        }

    async def _identify_performance_bottlenecks(self, metrics: List[PerformanceMetrics]) -> Dict:
        """è­˜åˆ¥æ€§èƒ½ç“¶é ¸"""

        bottlenecks = {}

        # åˆ†æä¸åŒè³‡æºçš„åˆ©ç”¨ç‡æ¨¡å¼
        cpu_utilizations = [m.cpu_utilization for m in metrics]
        gpu_utilizations = [m.gpu_utilization for m in metrics]
        memory_utilizations = [m.memory_utilization for m in metrics]
        response_times = [m.response_time_p95 for m in metrics]

        # CPU ç“¶é ¸åˆ†æ
        cpu_p95 = np.percentile(cpu_utilizations, 95)
        if cpu_p95 > 0.8:
            bottlenecks["cpu"] = {
                "severity": "high" if cpu_p95 > 0.9 else "medium",
                "p95_utilization": cpu_p95,
                "recommendation": "è€ƒæ…® CPU æ“´å±•æˆ–å·¥ä½œè² è¼‰å„ªåŒ–"
            }

        # GPU ç“¶é ¸åˆ†æ
        gpu_p95 = np.percentile(gpu_utilizations, 95)
        if gpu_p95 > 0.85:
            bottlenecks["gpu"] = {
                "severity": "critical" if gpu_p95 > 0.95 else "high",
                "p95_utilization": gpu_p95,
                "recommendation": "GPU è¨˜æ†¶é«”æˆ–è¨ˆç®—èƒ½åŠ›ä¸è¶³ï¼Œè€ƒæ…®å‡ç´šæˆ–å¢åŠ ç¯€é»"
            }

        # è¨˜æ†¶é«”ç“¶é ¸åˆ†æ
        memory_p95 = np.percentile(memory_utilizations, 95)
        if memory_p95 > 0.85:
            bottlenecks["memory"] = {
                "severity": "high" if memory_p95 > 0.9 else "medium",
                "p95_utilization": memory_p95,
                "recommendation": "è¨˜æ†¶é«”ä¸è¶³ï¼Œè€ƒæ…®å¢åŠ è¨˜æ†¶é«”æˆ–å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨"
            }

        # å»¶é²ç“¶é ¸åˆ†æ
        response_p95 = np.percentile(response_times, 95)
        if response_p95 > 1000:  # 1ç§’
            bottlenecks["latency"] = {
                "severity": "high" if response_p95 > 2000 else "medium",
                "p95_latency_ms": response_p95,
                "recommendation": "å»¶é²éé«˜ï¼Œæª¢æŸ¥æ¨¡å‹è¤‡é›œåº¦ã€æ‰¹æ¬¡å¤§å°æˆ–ç¶²è·¯å»¶é²"
            }

        # ç“¶é ¸ç›¸é—œæ€§åˆ†æ
        bottleneck_correlation = await self._analyze_bottleneck_correlations(metrics)

        return {
            "identified_bottlenecks": bottlenecks,
            "bottleneck_correlation": bottleneck_correlation,
            "primary_bottleneck": self._identify_primary_bottleneck(bottlenecks),
            "optimization_priority": self._prioritize_optimizations(bottlenecks)
        }

    def _identify_primary_bottleneck(self, bottlenecks: Dict) -> Optional[str]:
        """è­˜åˆ¥ä¸»è¦ç“¶é ¸"""

        if not bottlenecks:
            return None

        # æŒ‰åš´é‡ç¨‹åº¦æ’åº
        severity_order = {"critical": 4, "high": 3, "medium": 2, "low": 1}

        sorted_bottlenecks = sorted(
            bottlenecks.items(),
            key=lambda x: severity_order.get(x[1]["severity"], 0),
            reverse=True
        )

        return sorted_bottlenecks[0][0]
```

---

## 2. è‡ªå‹•æ“´å±•ç†è«–èˆ‡å¯¦ç¾

### 2.1 å½ˆæ€§æ“´å±•çš„æ§åˆ¶ç†è«–

#### **åé¥‹æ§åˆ¶ç³»çµ±æ¨¡å‹**

**å®šç¾© 2.1** (è‡ªå‹•æ“´å±•æ§åˆ¶ç³»çµ±): RAG ç³»çµ±çš„è‡ªå‹•æ“´å±•å¯å»ºæ¨¡ç‚ºåé¥‹æ§åˆ¶ç³»çµ±ï¼š

$$u(t) = K_p \cdot e(t) + K_i \int_0^t e(\tau)d\tau + K_d \frac{de(t)}{dt}$$

å…¶ä¸­ï¼š
- $u(t)$: æ§åˆ¶è¼¸å‡º (æ“´å±•æ±ºç­–)
- $e(t)$: èª¤å·®ä¿¡è™Ÿ (ç›®æ¨™æ€§èƒ½ - å¯¦éš›æ€§èƒ½)
- $K_p, K_i, K_d$: PID æ§åˆ¶å™¨åƒæ•¸

**ç©©å®šæ€§æ¢ä»¶**: æ ¹æ“š Routh-Hurwitz åˆ¤æ“šï¼Œç³»çµ±ç©©å®šçš„å¿…è¦æ¢ä»¶æ˜¯æ‰€æœ‰ç‰¹å¾µå€¼çš„å¯¦éƒ¨ç‚ºè² ã€‚

#### **é æ¸¬æ€§æ“´å±•ç®—æ³•**

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from typing import Dict, List, Tuple, Optional
import asyncio
from datetime import datetime, timedelta

class PredictiveAutoScaler:
    """é æ¸¬æ€§è‡ªå‹•æ“´å±•å™¨"""

    def __init__(self):
        # é æ¸¬æ¨¡å‹
        self.load_predictor = LoadPredictor()
        self.capacity_planner = CapacityPlanner()

        # æ§åˆ¶åƒæ•¸
        self.control_params = {
            "kp": 0.5,    # æ¯”ä¾‹æ§åˆ¶åƒæ•¸
            "ki": 0.1,    # ç©åˆ†æ§åˆ¶åƒæ•¸
            "kd": 0.2,    # å¾®åˆ†æ§åˆ¶åƒæ•¸
            "deadband": 0.1,  # æ­»å€ï¼Œé¿å…æŒ¯è•©
            "max_scale_rate": 0.5,  # æœ€å¤§æ“´å±•é€Ÿç‡ (50%/min)
            "min_scale_interval": 300  # æœ€å°æ“´å±•é–“éš” (ç§’)
        }

        # æ­·å²æ•¸æ“š
        self.performance_history = deque(maxlen=1000)
        self.scaling_history = deque(maxlen=100)

    async def predict_and_scale(self, current_metrics: PerformanceMetrics,
                               prediction_horizon: int = 300) -> Dict:
        """é æ¸¬è² è¼‰ä¸¦åŸ·è¡Œæ“´å±•æ±ºç­–"""

        # 1. è² è¼‰é æ¸¬
        load_prediction = await self.load_predictor.predict_future_load(
            self.performance_history, prediction_horizon
        )

        # 2. å®¹é‡éœ€æ±‚è¨ˆç®—
        capacity_requirements = await self.capacity_planner.calculate_required_capacity(
            load_prediction, current_metrics
        )

        # 3. æ“´å±•æ±ºç­–
        scaling_decision = await self._make_scaling_decision(
            current_metrics, capacity_requirements
        )

        # 4. åŸ·è¡Œæ“´å±•æ“ä½œ
        if scaling_decision["action"] != "no_action":
            scaling_result = await self._execute_scaling_action(scaling_decision)
        else:
            scaling_result = {"action": "no_action", "reason": "Within optimal range"}

        # 5. è¨˜éŒ„æ±ºç­–æ­·å²
        await self._record_scaling_decision(
            current_metrics, load_prediction, scaling_decision, scaling_result
        )

        return {
            "current_metrics": current_metrics,
            "load_prediction": load_prediction,
            "capacity_requirements": capacity_requirements,
            "scaling_decision": scaling_decision,
            "scaling_result": scaling_result
        }

    async def _make_scaling_decision(self, current: PerformanceMetrics,
                                   required: Dict) -> Dict:
        """åšå‡ºæ“´å±•æ±ºç­–"""

        # ç•¶å‰å®¹é‡ vs éœ€æ±‚å®¹é‡
        current_capacity = await self._estimate_current_capacity(current)
        required_capacity = required["total_capacity"]

        # è¨ˆç®—èª¤å·®ä¿¡è™Ÿ
        capacity_error = (required_capacity - current_capacity) / current_capacity

        # PID æ§åˆ¶å™¨è¨ˆç®—
        pid_output = await self._calculate_pid_output(capacity_error)

        # æ±ºç­–é–¾å€¼
        scale_up_threshold = 0.2    # éœ€è¦å¢åŠ  20% ä»¥ä¸Šå®¹é‡
        scale_down_threshold = -0.3  # å¯ä»¥æ¸›å°‘ 30% ä»¥ä¸Šå®¹é‡

        # æ“´å±•æ±ºç­–
        if pid_output > scale_up_threshold:
            action = "scale_up"
            scale_factor = min(2.0, 1.0 + pid_output)  # æœ€å¤šæ“´å±• 2 å€
        elif pid_output < scale_down_threshold:
            action = "scale_down"
            scale_factor = max(0.5, 1.0 + pid_output)  # æœ€å¤šç¸®æ¸›åˆ° 50%
        else:
            action = "no_action"
            scale_factor = 1.0

        # å®‰å…¨æª¢æŸ¥
        safety_check = await self._perform_scaling_safety_check(
            action, scale_factor, current
        )

        return {
            "action": action if safety_check["safe"] else "no_action",
            "scale_factor": scale_factor,
            "capacity_error": capacity_error,
            "pid_output": pid_output,
            "safety_check": safety_check,
            "reasoning": self._generate_scaling_reasoning(
                capacity_error, pid_output, safety_check
            )
        }

    async def _calculate_pid_output(self, error: float) -> float:
        """è¨ˆç®— PID æ§åˆ¶å™¨è¼¸å‡º"""

        # æ›´æ–°èª¤å·®æ­·å²
        current_time = datetime.now()
        self.error_history.append({"time": current_time, "error": error})

        # ä¿æŒæ­·å²é•·åº¦
        if len(self.error_history) > 100:
            self.error_history.popleft()

        # æ¯”ä¾‹é …
        p_term = self.control_params["kp"] * error

        # ç©åˆ†é …
        if len(self.error_history) >= 2:
            time_deltas = [
                (self.error_history[i]["time"] - self.error_history[i-1]["time"]).total_seconds()
                for i in range(1, len(self.error_history))
            ]
            errors = [record["error"] for record in self.error_history]

            integral = sum(e * dt for e, dt in zip(errors, time_deltas))
            i_term = self.control_params["ki"] * integral
        else:
            i_term = 0.0

        # å¾®åˆ†é …
        if len(self.error_history) >= 2:
            error_diff = self.error_history[-1]["error"] - self.error_history[-2]["error"]
            time_diff = (self.error_history[-1]["time"] - self.error_history[-2]["time"]).total_seconds()

            if time_diff > 0:
                derivative = error_diff / time_diff
                d_term = self.control_params["kd"] * derivative
            else:
                d_term = 0.0
        else:
            d_term = 0.0

        # PID è¼¸å‡º
        pid_output = p_term + i_term + d_term

        # æ‡‰ç”¨æ­»å€ï¼Œé¿å…å°å¹…æŒ¯è•©
        if abs(pid_output) < self.control_params["deadband"]:
            pid_output = 0.0

        return pid_output

class LoadPredictor:
    """è² è¼‰é æ¸¬å™¨"""

    def __init__(self):
        # æ™‚é–“åºåˆ—é æ¸¬æ¨¡å‹
        self.trend_model = LinearRegression()
        self.seasonal_model = SeasonalDecomposition()
        self.anomaly_detector = LoadAnomalyDetector()

    async def predict_future_load(self, historical_data: List[PerformanceMetrics],
                                horizon_seconds: int) -> Dict:
        """é æ¸¬æœªä¾†è² è¼‰"""

        if len(historical_data) < 24:  # è‡³å°‘éœ€è¦ 24 å€‹æ•¸æ“šé»
            return {"error": "Insufficient data for prediction"}

        # 1. æ•¸æ“šé è™•ç†
        time_series = await self._prepare_time_series(historical_data)

        # 2. è¶¨å‹¢åˆ†æ
        trend_analysis = await self._analyze_trend(time_series)

        # 3. å­£ç¯€æ€§åˆ†æ
        seasonal_analysis = await self._analyze_seasonality(time_series)

        # 4. ç•°å¸¸æª¢æ¸¬èˆ‡æ¸…ç†
        cleaned_data = await self.anomaly_detector.remove_anomalies(time_series)

        # 5. é æ¸¬è¨ˆç®—
        prediction_points = horizon_seconds // 60  # æ¯åˆ†é˜ä¸€å€‹é æ¸¬é»
        predictions = await self._generate_predictions(
            cleaned_data, trend_analysis, seasonal_analysis, prediction_points
        )

        # 6. ä¸ç¢ºå®šæ€§é‡åŒ–
        uncertainty_bounds = await self._calculate_prediction_uncertainty(
            predictions, historical_data
        )

        return {
            "predictions": predictions,
            "uncertainty_bounds": uncertainty_bounds,
            "trend_analysis": trend_analysis,
            "seasonal_patterns": seasonal_analysis,
            "prediction_confidence": self._calculate_prediction_confidence(uncertainty_bounds)
        }

    async def _generate_predictions(self, historical_data: List,
                                  trend: Dict,
                                  seasonality: Dict,
                                  num_points: int) -> List[Dict]:
        """ç”Ÿæˆè² è¼‰é æ¸¬"""

        predictions = []
        base_timestamp = datetime.now()

        for i in range(num_points):
            prediction_time = base_timestamp + timedelta(minutes=i)

            # åŸºç¤è¶¨å‹¢é æ¸¬
            trend_value = trend["slope"] * i + trend["intercept"]

            # å­£ç¯€æ€§èª¿æ•´
            seasonal_factor = seasonality.get("factors", {}).get(
                f"hour_{prediction_time.hour}", 1.0
            )

            # ç¶œåˆé æ¸¬
            predicted_qps = max(0, trend_value * seasonal_factor)

            # é æ¸¬å€é–“
            confidence_interval = self._calculate_confidence_interval(
                predicted_qps, i, trend, seasonality
            )

            predictions.append({
                "timestamp": prediction_time,
                "predicted_qps": predicted_qps,
                "confidence_interval": confidence_interval,
                "prediction_horizon_minutes": i
            })

        return predictions

    def _calculate_confidence_interval(self, predicted_value: float,
                                     time_step: int,
                                     trend: Dict,
                                     seasonality: Dict) -> Tuple[float, float]:
        """è¨ˆç®—é æ¸¬ä¿¡å¿ƒå€é–“"""

        # åŸºç¤ä¸ç¢ºå®šæ€§
        base_uncertainty = predicted_value * 0.1  # 10% åŸºç¤ä¸ç¢ºå®šæ€§

        # æ™‚é–“è·é›¢æ‡²ç½° (é æ¸¬è¶Šé è¶Šä¸æº–ç¢º)
        time_penalty = time_step * 0.02  # æ¯åˆ†é˜å¢åŠ  2% ä¸ç¢ºå®šæ€§

        # è¶¨å‹¢ä¸ç¢ºå®šæ€§
        trend_uncertainty = abs(trend.get("slope", 0)) * time_step * 0.1

        # ç¸½ä¸ç¢ºå®šæ€§
        total_uncertainty = base_uncertainty + time_penalty + trend_uncertainty

        # 95% ä¿¡å¿ƒå€é–“
        margin = 1.96 * total_uncertainty

        lower_bound = max(0, predicted_value - margin)
        upper_bound = predicted_value + margin

        return (lower_bound, upper_bound)
```

---

## 3. æˆæœ¬å»ºæ¨¡èˆ‡å„ªåŒ–

### 3.1 ä¼æ¥­ç´šæˆæœ¬åˆ†ææ¨¡å‹

#### **ç¸½é«”æ“æœ‰æˆæœ¬ (TCO) æ¨¡å‹**

**æ¨¡å‹ 3.1** (RAG ç³»çµ± TCO):

$$\text{TCO} = \text{CAPEX} + \text{OPEX}$$

å…¶ä¸­ï¼š

**è³‡æœ¬æ”¯å‡º (CAPEX)**:
$$\text{CAPEX} = C_{\text{ç¡¬é«”}} + C_{\text{è»Ÿé«”æˆæ¬Š}} + C_{\text{åˆå§‹é–‹ç™¼}} + C_{\text{éƒ¨ç½²}}$$

**ç‡Ÿé‹æ”¯å‡º (OPEX)**:
$$\text{OPEX} = C_{\text{è¨ˆç®—}} + C_{\text{å­˜å„²}} + C_{\text{ç¶²è·¯}} + C_{\text{ç¶­è­·}} + C_{\text{äººåŠ›}}$$

#### **å‹•æ…‹æˆæœ¬å„ªåŒ–ç®—æ³•**

```python
from typing import Dict, List, Any, Optional
import numpy as np
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class ResourceCost:
    """è³‡æºæˆæœ¬çµæ§‹"""
    resource_type: str
    unit_cost: float           # æ¯å–®ä½æˆæœ¬
    current_usage: float       # ç•¶å‰ä½¿ç”¨é‡
    utilization_rate: float    # åˆ©ç”¨ç‡ (0-1)
    scaling_granularity: int   # æ“´å±•ç²’åº¦

class CostOptimizationEngine:
    """æˆæœ¬å„ªåŒ–å¼•æ“"""

    def __init__(self):
        # æˆæœ¬æ¨¡å‹
        self.cost_models = {
            "compute": ComputeCostModel(),
            "storage": StorageCostModel(),
            "network": NetworkCostModel(),
            "software": SoftwareLicenseCostModel()
        }

        # å„ªåŒ–ç®—æ³•
        self.optimization_algorithms = {
            "resource_rightsizing": ResourceRightsizingOptimizer(),
            "workload_scheduling": WorkloadSchedulingOptimizer(),
            "cache_optimization": CacheOptimizationEngine(),
            "model_optimization": ModelOptimizationEngine()
        }

    async def comprehensive_cost_optimization(self, current_config: Dict,
                                            performance_requirements: Dict) -> Dict:
        """ç¶œåˆæˆæœ¬å„ªåŒ–"""

        # 1. ç•¶å‰æˆæœ¬åˆ†æ
        current_cost_analysis = await self._analyze_current_costs(current_config)

        # 2. æˆæœ¬å„ªåŒ–æ©Ÿæœƒè­˜åˆ¥
        optimization_opportunities = await self._identify_optimization_opportunities(
            current_config, current_cost_analysis
        )

        # 3. å„ªåŒ–ç­–ç•¥åŸ·è¡Œ
        optimization_results = {}
        for strategy_name, optimizer in self.optimization_algorithms.items():
            if strategy_name in optimization_opportunities:
                result = await optimizer.optimize(
                    current_config,
                    optimization_opportunities[strategy_name],
                    performance_requirements
                )
                optimization_results[strategy_name] = result

        # 4. å„ªåŒ–æ•ˆæœè©•ä¼°
        optimization_impact = await self._evaluate_optimization_impact(
            current_cost_analysis, optimization_results
        )

        # 5. é¢¨éšªè©•ä¼°
        optimization_risks = await self._assess_optimization_risks(
            optimization_results, performance_requirements
        )

        return {
            "current_cost_analysis": current_cost_analysis,
            "optimization_opportunities": optimization_opportunities,
            "optimization_results": optimization_results,
            "optimization_impact": optimization_impact,
            "optimization_risks": optimization_risks,
            "implementation_plan": await self._create_optimization_implementation_plan(
                optimization_results, optimization_risks
            )
        }

    async def _analyze_current_costs(self, config: Dict) -> Dict:
        """åˆ†æç•¶å‰æˆæœ¬çµæ§‹"""

        cost_breakdown = {}
        total_monthly_cost = 0

        for cost_category, cost_model in self.cost_models.items():
            category_cost = await cost_model.calculate_monthly_cost(config)
            cost_breakdown[cost_category] = category_cost
            total_monthly_cost += category_cost["total"]

        # æˆæœ¬æ•ˆç‡åˆ†æ
        efficiency_metrics = await self._calculate_cost_efficiency(cost_breakdown, config)

        return {
            "total_monthly_cost": total_monthly_cost,
            "cost_breakdown": cost_breakdown,
            "efficiency_metrics": efficiency_metrics,
            "cost_per_query": total_monthly_cost / max(config.get("monthly_queries", 1), 1),
            "cost_trends": await self._analyze_cost_trends()
        }

    async def _identify_optimization_opportunities(self, config: Dict,
                                                 cost_analysis: Dict) -> Dict:
        """è­˜åˆ¥æˆæœ¬å„ªåŒ–æ©Ÿæœƒ"""

        opportunities = {}

        # 1. è³‡æºéåº¦é…ç½®æª¢æŸ¥
        overprovisioning = await self._detect_resource_overprovisioning(config, cost_analysis)
        if overprovisioning["detected"]:
            opportunities["resource_rightsizing"] = overprovisioning

        # 2. å·¥ä½œè² è¼‰èª¿åº¦å„ªåŒ–
        scheduling_potential = await self._assess_scheduling_optimization_potential(config)
        if scheduling_potential["potential_savings"] > 0.1:  # 10% ä»¥ä¸Šç¯€çœæ½›åŠ›
            opportunities["workload_scheduling"] = scheduling_potential

        # 3. å¿«å–å„ªåŒ–
        cache_analysis = await self._analyze_cache_optimization_potential(config, cost_analysis)
        if cache_analysis["optimization_potential"] > 0.05:  # 5% ä»¥ä¸Šç¯€çœ
            opportunities["cache_optimization"] = cache_analysis

        # 4. æ¨¡å‹å„ªåŒ–
        model_optimization = await self._assess_model_optimization_opportunities(config)
        if model_optimization["efficiency_gain"] > 0.15:  # 15% ä»¥ä¸Šæ•ˆç‡æå‡
            opportunities["model_optimization"] = model_optimization

        return opportunities

    async def _detect_resource_overprovisioning(self, config: Dict,
                                              cost_analysis: Dict) -> Dict:
        """æª¢æ¸¬è³‡æºéåº¦é…ç½®"""

        overprovisioning_analysis = {
            "detected": False,
            "overprovisioned_resources": [],
            "potential_savings": 0.0
        }

        efficiency_metrics = cost_analysis["efficiency_metrics"]

        # æª¢æŸ¥å„é¡è³‡æºåˆ©ç”¨ç‡
        resource_utilizations = {
            "cpu": efficiency_metrics.get("cpu_efficiency", 0.7),
            "gpu": efficiency_metrics.get("gpu_efficiency", 0.8),
            "memory": efficiency_metrics.get("memory_efficiency", 0.6),
            "storage": efficiency_metrics.get("storage_efficiency", 0.5)
        }

        for resource, utilization in resource_utilizations.items():
            if utilization < 0.3:  # åˆ©ç”¨ç‡ä½æ–¼ 30%
                # è¨ˆç®—å³èª¿å¤§å°å»ºè­°
                optimal_size_ratio = max(0.5, utilization / 0.7)  # ç›®æ¨™ 70% åˆ©ç”¨ç‡
                potential_savings = (1 - optimal_size_ratio) * cost_analysis["cost_breakdown"].get(resource, {}).get("total", 0)

                overprovisioning_analysis["overprovisioned_resources"].append({
                    "resource_type": resource,
                    "current_utilization": utilization,
                    "recommended_size_ratio": optimal_size_ratio,
                    "potential_monthly_savings": potential_savings
                })

                overprovisioning_analysis["potential_savings"] += potential_savings
                overprovisioning_analysis["detected"] = True

        return overprovisioning_analysis

class ResourceRightsizingOptimizer:
    """è³‡æºå³èª¿å„ªåŒ–å™¨"""

    def __init__(self):
        self.sizing_models = {
            "cpu": CPUSizingModel(),
            "gpu": GPUSizingModel(),
            "memory": MemorySizingModel(),
            "storage": StorageSizingModel()
        }

    async def optimize(self, current_config: Dict,
                     opportunity: Dict,
                     performance_req: Dict) -> Dict:
        """åŸ·è¡Œè³‡æºå³èª¿å„ªåŒ–"""

        optimization_plan = {}
        estimated_savings = 0

        for resource_info in opportunity["overprovisioned_resources"]:
            resource_type = resource_info["resource_type"]
            sizing_model = self.sizing_models[resource_type]

            # è¨ˆç®—æœ€ä½³å¤§å°
            optimal_sizing = await sizing_model.calculate_optimal_size(
                current_config,
                resource_info,
                performance_req
            )

            # é©—è­‰æ€§èƒ½å½±éŸ¿
            performance_impact = await sizing_model.assess_performance_impact(
                current_config, optimal_sizing
            )

            if performance_impact["acceptable"]:
                optimization_plan[resource_type] = {
                    "current_config": current_config.get(resource_type, {}),
                    "optimized_config": optimal_sizing,
                    "estimated_savings": resource_info["potential_monthly_savings"],
                    "performance_impact": performance_impact
                }

                estimated_savings += resource_info["potential_monthly_savings"]

        return {
            "optimization_plan": optimization_plan,
            "total_estimated_savings": estimated_savings,
            "savings_percentage": estimated_savings / opportunity.get("total_current_cost", 1) * 100,
            "implementation_complexity": self._assess_implementation_complexity(optimization_plan)
        }
```

---

## 4. ç³»çµ±å¯è§€æ¸¬æ€§èˆ‡ç›£æ§

### 4.1 å…¨æ£§ç›£æ§ç†è«–

#### **å¯è§€æ¸¬æ€§çš„ä¸‰æ”¯æŸ±ç†è«–**

**å®šç¾© 4.1** (ç³»çµ±å¯è§€æ¸¬æ€§): ç³»çµ±å¯è§€æ¸¬æ€§ $O$ å®šç¾©ç‚ºä¸‰å€‹ç¶­åº¦çš„è¯åˆæ¸¬é‡èƒ½åŠ›ï¼š

$$O = f(\text{Metrics}, \text{Logs}, \text{Traces})$$

**æŒ‡æ¨™ (Metrics)**: ç³»çµ±ç‹€æ…‹çš„æ•¸å€¼æ¸¬é‡
- **åæ‡‰å¼æŒ‡æ¨™**: CPUã€è¨˜æ†¶é«”ã€å»¶é²ç­‰
- **é æ¸¬å¼æŒ‡æ¨™**: è¶¨å‹¢ã€å®¹é‡ã€ç•°å¸¸åˆ†æ•¸

**æ—¥èªŒ (Logs)**: é›¢æ•£äº‹ä»¶çš„çµæ§‹åŒ–è¨˜éŒ„
- **çµæ§‹åŒ–æ—¥èªŒ**: JSON æ ¼å¼çš„æ©Ÿå™¨å¯è®€æ—¥èªŒ
- **èªç¾©æ—¥èªŒ**: åŒ…å«æ¥­å‹™èªç¾©çš„é«˜å±¤æ¬¡äº‹ä»¶

**è¿½è¹¤ (Traces)**: è·¨æœå‹™è«‹æ±‚çš„å®Œæ•´è·¯å¾‘
- **åˆ†æ•£å¼è¿½è¹¤**: å¾®æœå‹™é–“çš„è«‹æ±‚æµè¿½è¹¤
- **å› æœé—œä¿‚**: äº‹ä»¶é–“çš„å› æœä¾è³´é—œä¿‚

#### **ä¼æ¥­ç´šç›£æ§ç³»çµ±æ¶æ§‹**

```python
import asyncio
from typing import Dict, List, Any, Optional
import time
import json
from dataclasses import dataclass, asdict
from datetime import datetime
import uuid

@dataclass
class MetricData:
    """æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    name: str
    value: float
    unit: str
    labels: Dict[str, str]
    timestamp: datetime

@dataclass
class LogEntry:
    """æ—¥èªŒæ¢ç›®çµæ§‹"""
    level: str
    message: str
    component: str
    trace_id: Optional[str]
    span_id: Optional[str]
    timestamp: datetime
    metadata: Dict[str, Any]

@dataclass
class TraceSpan:
    """è¿½è¹¤ç‰‡æ®µçµæ§‹"""
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    operation_name: str
    start_time: datetime
    end_time: Optional[datetime]
    tags: Dict[str, Any]
    logs: List[Dict]

class EnterpriseObservabilityFramework:
    """ä¼æ¥­å¯è§€æ¸¬æ€§æ¡†æ¶"""

    def __init__(self):
        # ç›£æ§çµ„ä»¶
        self.metrics_collector = MetricsCollector()
        self.log_aggregator = LogAggregator()
        self.trace_collector = TraceCollector()

        # åˆ†æå¼•æ“
        self.anomaly_detector = AnomalyDetectionEngine()
        self.correlation_analyzer = CorrelationAnalyzer()
        self.root_cause_analyzer = RootCauseAnalyzer()

        # å‘Šè­¦ç³»çµ±
        self.alerting_engine = AlertingEngine()

    async def monitor_rag_system_health(self, system_components: List[str]) -> Dict:
        """ç›£æ§ RAG ç³»çµ±å¥åº·ç‹€æ³"""

        health_status = {}

        for component in system_components:
            component_health = await self._monitor_component_health(component)
            health_status[component] = component_health

        # ç³»çµ±ç´šå¥åº·åˆ†æ
        system_health = await self._analyze_system_health(health_status)

        # ç•°å¸¸é—œè¯åˆ†æ
        anomaly_correlation = await self.correlation_analyzer.analyze_cross_component_anomalies(
            health_status
        )

        # é æ¸¬æ€§å‘Šè­¦
        predictive_alerts = await self._generate_predictive_alerts(system_health)

        return {
            "component_health": health_status,
            "system_health": system_health,
            "anomaly_correlation": anomaly_correlation,
            "predictive_alerts": predictive_alerts,
            "overall_status": system_health["status"],
            "health_score": system_health["score"]
        }

    async def _monitor_component_health(self, component: str) -> Dict:
        """ç›£æ§çµ„ä»¶å¥åº·ç‹€æ³"""

        # æ”¶é›†çµ„ä»¶æŒ‡æ¨™
        component_metrics = await self.metrics_collector.collect_component_metrics(component)

        # æ”¶é›†çµ„ä»¶æ—¥èªŒ
        component_logs = await self.log_aggregator.get_recent_logs(component, minutes=5)

        # æ”¶é›†çµ„ä»¶è¿½è¹¤
        component_traces = await self.trace_collector.get_recent_traces(component, minutes=5)

        # å¥åº·è©•ä¼°
        health_assessment = await self._assess_component_health(
            component_metrics, component_logs, component_traces
        )

        return health_assessment

    async def _assess_component_health(self, metrics: List[MetricData],
                                     logs: List[LogEntry],
                                     traces: List[TraceSpan]) -> Dict:
        """è©•ä¼°çµ„ä»¶å¥åº·ç‹€æ³"""

        health_indicators = {}

        # 1. æŒ‡æ¨™å¥åº·åº¦
        metrics_health = await self._analyze_metrics_health(metrics)
        health_indicators["metrics"] = metrics_health

        # 2. éŒ¯èª¤ç‡åˆ†æ
        error_analysis = await self._analyze_error_patterns(logs)
        health_indicators["errors"] = error_analysis

        # 3. æ€§èƒ½åˆ†æ
        performance_analysis = await self._analyze_trace_performance(traces)
        health_indicators["performance"] = performance_analysis

        # ç¶œåˆå¥åº·åˆ†æ•¸
        weights = {"metrics": 0.4, "errors": 0.3, "performance": 0.3}
        health_score = sum(
            weights[category] * indicators["health_score"]
            for category, indicators in health_indicators.items()
        )

        # å¥åº·ç‹€æ…‹åˆ†é¡
        if health_score >= 0.9:
            status = "healthy"
        elif health_score >= 0.7:
            status = "warning"
        elif health_score >= 0.5:
            status = "degraded"
        else:
            status = "critical"

        return {
            "status": status,
            "health_score": health_score,
            "health_indicators": health_indicators,
            "recommendations": self._generate_health_recommendations(health_indicators)
        }

    async def _generate_predictive_alerts(self, system_health: Dict) -> List[Dict]:
        """ç”Ÿæˆé æ¸¬æ€§å‘Šè­¦"""

        predictive_alerts = []

        # åˆ†æå¥åº·è¶¨å‹¢
        health_trends = await self._analyze_health_trends(system_health)

        for component, trend in health_trends.items():
            if trend["direction"] == "declining" and trend["severity"] > 0.1:
                # é æ¸¬ä½•æ™‚å¯èƒ½å‡ºç¾å•é¡Œ
                time_to_failure = await self._estimate_time_to_failure(trend)

                if time_to_failure <= timedelta(hours=2):
                    severity = "critical"
                elif time_to_failure <= timedelta(hours=6):
                    severity = "warning"
                else:
                    severity = "info"

                predictive_alerts.append({
                    "type": "predictive_degradation",
                    "component": component,
                    "severity": severity,
                    "estimated_time_to_failure": time_to_failure,
                    "trend_analysis": trend,
                    "recommended_actions": self._suggest_preventive_actions(component, trend)
                })

        return predictive_alerts

    def _suggest_preventive_actions(self, component: str, trend: Dict) -> List[str]:
        """å»ºè­°é é˜²æ€§è¡Œå‹•"""

        actions = []

        if trend["primary_issue"] == "resource_exhaustion":
            actions.append(f"æ“´å±• {component} çš„è³‡æºé…ç½®")
            actions.append("æª¢æŸ¥æ˜¯å¦æœ‰è¨˜æ†¶é«”æ´©æ¼æˆ–è³‡æºæ³„æ¼")

        elif trend["primary_issue"] == "performance_degradation":
            actions.append(f"å„ªåŒ– {component} çš„æ€§èƒ½é…ç½®")
            actions.append("æª¢æŸ¥æ˜¯å¦éœ€è¦å¿«å–å„ªåŒ–æˆ–æŸ¥è©¢å„ªåŒ–")

        elif trend["primary_issue"] == "error_rate_increase":
            actions.append(f"æª¢æŸ¥ {component} çš„éŒ¯èª¤æ—¥èªŒ")
            actions.append("é©—è­‰ä¸Šæ¸¸ä¾è³´æ˜¯å¦æ­£å¸¸")

        actions.append("å¢åŠ ç›£æ§é »ç‡ä»¥ç²å¾—æ›´è©³ç´°çš„è¨ºæ–·è³‡è¨Š")

        return actions
```

---

## 5. SLA/SLO è¨­è¨ˆèˆ‡ç®¡ç†

### 5.1 æœå‹™ç­‰ç´šç®¡ç†ç†è«–

#### **SLO æ•¸å­¸å»ºæ¨¡**

**å®šç¾© 5.1** (æœå‹™ç­‰ç´šç›®æ¨™): SLO å®šç¾©ç‚ºæ¸¬é‡å‡½æ•¸ $M$ åœ¨æ™‚é–“çª—å£ $T$ å…§æ»¿è¶³é–¾å€¼ $\theta$ çš„æ¦‚ç‡ï¼š

$$\text{SLO} = P(M(t) \geq \theta, \forall t \in T) \geq \text{Target}$$

**å¸¸è¦‹ SLO é¡å‹**:
- **å¯ç”¨æ€§**: $\text{Availability} = \frac{\text{Uptime}}{\text{Total Time}} \geq 99.9\%$
- **å»¶é²**: $P(\text{Latency} \leq 500ms) \geq 95\%$
- **éŒ¯èª¤ç‡**: $\text{Error Rate} = \frac{\text{Failed Requests}}{\text{Total Requests}} \leq 0.1\%$

#### **éŒ¯èª¤é ç®—ç†è«–**

**å®šç¾© 5.2** (éŒ¯èª¤é ç®—): åœ¨ SLO å…è¨±ç¯„åœå…§çš„å¤±æ•—é¡åº¦ï¼š

$$\text{Error Budget} = (1 - \text{SLO Target}) \times \text{Total Operations}$$

**å®šç† 5.1** (éŒ¯èª¤é ç®—æ¶ˆè€—ç‡): éŒ¯èª¤é ç®—çš„æœ€å„ªæ¶ˆè€—ç­–ç•¥æ‡‰å¹³è¡¡å‰µæ–°é€Ÿåº¦å’Œç³»çµ±ç©©å®šæ€§ï¼š

$$\frac{d(\text{Error Budget})}{dt} = \alpha \cdot \text{Innovation Rate} - \beta \cdot \text{Stability Investment}$$

#### **SLO ç®¡ç†ç³»çµ±å¯¦ç¾**

```python
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import numpy as np
from dataclasses import dataclass

@dataclass
class SLODefinition:
    """SLO å®šç¾©"""
    name: str
    description: str
    metric_name: str
    measurement_window: timedelta
    threshold: float
    target_percentage: float  # e.g., 99.9
    measurement_type: str    # "availability", "latency", "error_rate"

class SLOManager:
    """SLO ç®¡ç†å™¨"""

    def __init__(self):
        # ä¼æ¥­æ¨™æº– SLO å®šç¾©
        self.standard_slos = {
            "availability": SLODefinition(
                name="System Availability",
                description="ç³»çµ±å¯ç”¨æ€§ï¼šç³»çµ±æ­£å¸¸é‹è¡Œçš„æ™‚é–“æ¯”ä¾‹",
                metric_name="uptime_ratio",
                measurement_window=timedelta(days=30),
                threshold=1.0,
                target_percentage=99.9,
                measurement_type="availability"
            ),
            "response_time": SLODefinition(
                name="Response Time P95",
                description="95% çš„è«‹æ±‚åœ¨ 500ms å…§å®Œæˆ",
                metric_name="response_time_p95",
                measurement_window=timedelta(hours=1),
                threshold=500.0,  # milliseconds
                target_percentage=95.0,
                measurement_type="latency"
            ),
            "error_rate": SLODefinition(
                name="Error Rate",
                description="éŒ¯èª¤ç‡ä¸è¶…é 0.1%",
                metric_name="error_rate",
                measurement_window=timedelta(hours=1),
                threshold=0.001,  # 0.1%
                target_percentage=99.9,
                measurement_type="error_rate"
            )
        }

        self.slo_calculator = SLOCalculator()
        self.error_budget_manager = ErrorBudgetManager()

    async def monitor_slo_compliance(self, metrics_data: List[Dict]) -> Dict:
        """ç›£æ§ SLO åˆè¦ç‹€æ³"""

        slo_compliance = {}

        for slo_name, slo_def in self.standard_slos.items():
            # è¨ˆç®— SLO åˆè¦æ€§
            compliance_result = await self.slo_calculator.calculate_slo_compliance(
                slo_def, metrics_data
            )

            slo_compliance[slo_name] = compliance_result

            # æ›´æ–°éŒ¯èª¤é ç®—
            if slo_def.measurement_type != "availability":
                await self.error_budget_manager.update_error_budget(
                    slo_name, compliance_result
                )

        # ç¸½é«”åˆè¦åˆ†æ
        overall_compliance = await self._analyze_overall_slo_compliance(slo_compliance)

        # éŒ¯èª¤é ç®—ç‹€æ³
        error_budget_status = await self.error_budget_manager.get_current_status()

        return {
            "slo_compliance": slo_compliance,
            "overall_compliance": overall_compliance,
            "error_budget_status": error_budget_status,
            "compliance_trends": await self._analyze_compliance_trends(slo_compliance)
        }

    async def _analyze_overall_slo_compliance(self, slo_compliance: Dict) -> Dict:
        """åˆ†ææ•´é«” SLO åˆè¦ç‹€æ³"""

        # è¨ˆç®—åˆè¦åˆ†æ•¸
        compliance_scores = []
        violated_slos = []

        for slo_name, compliance in slo_compliance.items():
            score = compliance.get("compliance_percentage", 0.0)
            compliance_scores.append(score)

            if score < self.standard_slos[slo_name].target_percentage:
                violation_severity = self._calculate_violation_severity(
                    score, self.standard_slos[slo_name].target_percentage
                )

                violated_slos.append({
                    "slo_name": slo_name,
                    "current_compliance": score,
                    "target_compliance": self.standard_slos[slo_name].target_percentage,
                    "violation_severity": violation_severity,
                    "gap": self.standard_slos[slo_name].target_percentage - score
                })

        overall_score = np.mean(compliance_scores) if compliance_scores else 0.0

        return {
            "overall_compliance_score": overall_score,
            "compliant_slos": len(compliance_scores) - len(violated_slos),
            "violated_slos": violated_slos,
            "compliance_status": "healthy" if overall_score >= 99.0 else
                               "degraded" if overall_score >= 95.0 else "critical"
        }

class ErrorBudgetManager:
    """éŒ¯èª¤é ç®—ç®¡ç†å™¨"""

    def __init__(self):
        self.budget_policies = {
            "conservative": {"burn_rate_threshold": 0.1, "action": "halt_deployments"},
            "moderate": {"burn_rate_threshold": 0.2, "action": "review_required"},
            "aggressive": {"burn_rate_threshold": 0.5, "action": "monitor_closely"}
        }

        self.current_budgets = {}

    async def calculate_error_budget(self, slo_def: SLODefinition,
                                   time_period: timedelta) -> Dict:
        """è¨ˆç®—éŒ¯èª¤é ç®—"""

        # æ™‚é–“æœŸé–“å…§çš„ç¸½æ“ä½œæ•¸ (ä¼°ç®—)
        estimated_operations_per_hour = 10000  # åŸºæ–¼æ­·å²æ•¸æ“š
        total_operations = (
            estimated_operations_per_hour *
            (time_period.total_seconds() / 3600)
        )

        # å…è¨±çš„å¤±æ•—æ“ä½œæ•¸
        allowed_failures = total_operations * (1 - slo_def.target_percentage / 100)

        return {
            "slo_name": slo_def.name,
            "time_period": time_period,
            "total_operations": total_operations,
            "allowed_failures": allowed_failures,
            "remaining_budget": allowed_failures,  # åˆå§‹ç‹€æ…‹
            "budget_utilization": 0.0
        }

    async def update_error_budget(self, slo_name: str, compliance_result: Dict):
        """æ›´æ–°éŒ¯èª¤é ç®—"""

        if slo_name not in self.current_budgets:
            # åˆå§‹åŒ–éŒ¯èª¤é ç®—
            slo_def = self.standard_slos[slo_name]
            budget = await self.calculate_error_budget(
                slo_def, timedelta(days=30)  # 30 å¤©çª—å£
            )
            self.current_budgets[slo_name] = budget

        # æ›´æ–°é ç®—ä½¿ç”¨æƒ…æ³
        budget = self.current_budgets[slo_name]
        failed_operations = compliance_result.get("failed_operations", 0)

        budget["remaining_budget"] -= failed_operations
        budget["budget_utilization"] = (
            (budget["allowed_failures"] - budget["remaining_budget"]) /
            budget["allowed_failures"]
        )

        # æª¢æŸ¥é ç®—æ¶ˆè€—ç‡
        burn_rate = await self._calculate_burn_rate(slo_name)

        if burn_rate > self.budget_policies["conservative"]["burn_rate_threshold"]:
            await self._trigger_budget_alert(slo_name, budget, burn_rate)

    async def _calculate_burn_rate(self, slo_name: str) -> float:
        """è¨ˆç®—éŒ¯èª¤é ç®—æ¶ˆè€—ç‡"""

        if slo_name not in self.current_budgets:
            return 0.0

        budget = self.current_budgets[slo_name]

        # è¨ˆç®—æœ€è¿‘ 1 å°æ™‚çš„æ¶ˆè€—ç‡
        recent_consumption = budget["allowed_failures"] - budget["remaining_budget"]
        hours_elapsed = 1  # ç°¡åŒ–è¨ˆç®—

        # é ç®—æ¶ˆè€—ç‡ (æ¯å°æ™‚)
        burn_rate = recent_consumption / (budget["allowed_failures"] * hours_elapsed)

        return burn_rate
```

---

## 6. æœ¬ç« ç¸½çµ

### 6.1 æ€§èƒ½å·¥ç¨‹è¦é»

1. **ç†è«–åŸºç¤**: æ’éšŠç†è«–ã€æ§åˆ¶ç†è«–åœ¨åˆ†æ•£å¼ç³»çµ±ä¸­çš„æ‡‰ç”¨
2. **ç³»çµ±è¨­è¨ˆ**: å¯æ“´å±•ã€å¯è§€æ¸¬ã€å¯æ§åˆ¶çš„æ¶æ§‹è¨­è¨ˆåŸå‰‡
3. **æˆæœ¬å„ªåŒ–**: åŸºæ–¼æ•¸å­¸æ¨¡å‹çš„æˆæœ¬åˆ†æå’Œå„ªåŒ–ç­–ç•¥
4. **å“è³ªä¿è­‰**: SLO/SLA çš„ç§‘å­¸è¨­è¨ˆå’Œç®¡ç†æ–¹æ³•

### 6.2 å¯¦è¸æŒ‡å°åŸå‰‡

1. **æ¸¬é‡å„ªæ–¼çŒœæ¸¬**: æ‰€æœ‰å„ªåŒ–æ±ºç­–éƒ½æ‡‰åŸºæ–¼é‡åŒ–æ¸¬é‡
2. **é é˜²å„ªæ–¼å›æ‡‰**: å»ºç«‹é æ¸¬æ€§ç›£æ§å’Œä¸»å‹•å„ªåŒ–æ©Ÿåˆ¶
3. **æ•´é«”å„ªæ–¼å±€éƒ¨**: ç³»çµ±ç´šå„ªåŒ–è€Œéå–®å€‹çµ„ä»¶å„ªåŒ–
4. **æŒçºŒæ”¹é€²**: å»ºç«‹æ€§èƒ½å„ªåŒ–çš„æŒçºŒæ”¹é€²å¾ªç’°

### 6.3 ä¸‹ç« é å‘Š

ç¬¬9ç« å°‡é€šéå…·é«”çš„ä¼æ¥­æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºå¦‚ä½•å°‡å‰é¢å­¸åˆ°çš„ç†è«–å’ŒæŠ€è¡“æ‡‰ç”¨åˆ°çœŸå¯¦çš„ä¼æ¥­ç’°å¢ƒä¸­ï¼Œåˆ†ææˆåŠŸæ¨¡å¼å’Œå¤±æ•—æ•™è¨“ã€‚

---

**èª²ç¨‹è©•ä¼°**: æœ¬ç« å…§å®¹åœ¨æœŸæœ«è€ƒè©¦ä¸­å 20%æ¬Šé‡ï¼Œé‡é»è€ƒæŸ¥æ€§èƒ½åˆ†æå’Œç³»çµ±å„ªåŒ–èƒ½åŠ›ã€‚

**é …ç›®è¦æ±‚**: å­¸ç”Ÿéœ€å®Œæˆä¸€å€‹æ€§èƒ½å„ªåŒ–é …ç›®ï¼ŒåŒ…æ‹¬ç“¶é ¸åˆ†æã€å„ªåŒ–ç­–ç•¥è¨­è¨ˆå’Œæ•ˆæœé©—è­‰ã€‚