# LLM æœå‹™èˆ‡å¼•ç”¨ç†è«–
## å¤§å­¸æ•™ç§‘æ›¸ ç¬¬4ç« ï¼šå¤§å‹èªè¨€æ¨¡å‹çš„ç”Ÿç”¢éƒ¨ç½²èˆ‡å¼•ç”¨ç³»çµ±

**èª²ç¨‹ç·¨è™Ÿ**: CS785 - ä¼æ¥­ç´šæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±
**ç« ç¯€**: ç¬¬4ç«  ç”Ÿæˆæ§åˆ¶
**å­¸ç¿’æ™‚æ•¸**: 8å°æ™‚
**å…ˆä¿®èª²ç¨‹**: æ·±åº¦å­¸ç¿’åŸºç¤, è‡ªç„¶èªè¨€ç”Ÿæˆ, ç¬¬0-3ç« 
**ä½œè€…**: èªè¨€æ¨¡å‹ç ”ç©¶åœ˜éšŠ
**æœ€å¾Œæ›´æ–°**: 2025-01-06

---

## ğŸ“š å­¸ç¿’ç›®æ¨™ (Learning Objectives)

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œå­¸ç”Ÿæ‡‰èƒ½å¤ :

1. **ç†è«–åŸºç¤**: æŒæ¡å¤§å‹èªè¨€æ¨¡å‹çš„æ•¸å­¸åŸç†å’Œç”Ÿæˆæ§åˆ¶ç†è«–
2. **ç³»çµ±æ¶æ§‹**: è¨­è¨ˆä¼æ¥­ç´š LLM æœå‹™æ¶æ§‹å’Œå¼•ç”¨é©—è­‰ç³»çµ±
3. **æ€§èƒ½å„ªåŒ–**: å¯¦ç¾é«˜æ•ˆèƒ½çš„æ¨¡å‹æ¨ç†å’Œæ‰¹æ¬¡è™•ç†ç­–ç•¥
4. **å“è³ªæ§åˆ¶**: å»ºç«‹å®Œæ•´çš„äº‹å¯¦æª¢æŸ¥å’Œå¼•ç”¨å°é½Šæ©Ÿåˆ¶

---

## 1. å¤§å‹èªè¨€æ¨¡å‹çš„ç†è«–åŸºç¤

### 1.1 Transformer æ¶æ§‹çš„æ•¸å­¸åŸç†

#### **æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ•¸å­¸è¡¨ç¤º**

**å®šç¾© 1.1** (å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶): å°æ–¼è¼¸å…¥åºåˆ— $X \in \mathbb{R}^{n \times d}$ï¼Œå¤šé ­æ³¨æ„åŠ›è¨ˆç®—ç‚ºï¼š

$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

å…¶ä¸­æ¯å€‹æ³¨æ„åŠ›é ­å®šç¾©ç‚ºï¼š

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**å®šç† 1.1** (æ³¨æ„åŠ›è¤‡é›œåº¦): æ¨™æº–æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ™‚é–“è¤‡é›œåº¦ç‚º $O(n^2d)$ï¼Œå…¶ä¸­ $n$ ç‚ºåºåˆ—é•·åº¦ï¼Œ$d$ ç‚ºç‰¹å¾µç¶­åº¦ã€‚

#### **ä½ç½®ç·¨ç¢¼çš„ç†è«–åˆ†æ**

**RoPE (Rotary Position Embedding)** çš„æ•¸å­¸åŸç† (Su et al., 2021)[^23]:

$$f(x_m, m) = \begin{pmatrix}
\cos m\theta & -\sin m\theta \\
\sin m\theta & \cos m\theta
\end{pmatrix} \begin{pmatrix}
x_{m,2i} \\
x_{m,2i+1}
\end{pmatrix}$$

å…¶ä¸­ $\theta = 10000^{-2i/d}$ï¼Œ$m$ ç‚ºä½ç½®ç´¢å¼•ã€‚

**æ€§è³ª 1.1** (RoPE çš„ç›¸å°ä½ç½®ä¸è®Šæ€§): RoPE ç·¨ç¢¼ä¿è­‰äº†ç›¸å°ä½ç½®é—œä¿‚åœ¨å…§ç©ç©ºé–“ä¸­çš„ç·šæ€§è¡¨ç¤ºã€‚

### 1.2 ç”Ÿæˆæ§åˆ¶çš„æ¦‚ç‡ç†è«–

#### **æ¢ä»¶ç”Ÿæˆçš„æ•¸å­¸æ¡†æ¶**

**å®šç¾© 1.2** (æ¢ä»¶æ–‡æœ¬ç”Ÿæˆ): çµ¦å®šä¸Šä¸‹æ–‡ $c$ å’ŒæŸ¥è©¢ $q$ï¼Œæ¨¡å‹ç”Ÿæˆå›æ‡‰ $y$ çš„æ¦‚ç‡ç‚ºï¼š

$$P(y|c,q) = \prod_{t=1}^{|y|} P(y_t|y_{<t}, c, q)$$

**å®šç† 1.2** (ç”Ÿæˆå¿ å¯¦åº¦ç•Œé™): å°æ–¼æª¢ç´¢å¢å¼·ç”Ÿæˆï¼Œå¿ å¯¦åº¦çš„ç†è«–ä¸Šç•Œç‚ºï¼š

$$\text{Faithfulness} \leq \min\left(P(\text{relevant}|c), P(\text{faithful}|c,\text{relevant})\right)$$

**è­‰æ˜æ€è·¯**: ç”Ÿæˆå¿ å¯¦åº¦å—åˆ¶æ–¼ä¸Šä¸‹æ–‡ç›¸é—œæ€§å’Œæ¨¡å‹çš„å¿ å¯¦ç”Ÿæˆèƒ½åŠ›ï¼Œå…©è€…çš„æœ€å°å€¼æ±ºå®šäº†ç³»çµ±çš„å¿ å¯¦åº¦ä¸Šç•Œã€‚â–¡

#### **å¼•ç”¨å°é½Šçš„è³‡è¨Šç†è«–**

**å®šç¾© 1.3** (å¼•ç”¨å°é½Š): ç”Ÿæˆæ–‡æœ¬èˆ‡æºæ–‡æª”ä¹‹é–“çš„è³‡è¨Šå°æ‡‰é—œä¿‚ï¼Œé‡åŒ–ç‚ºï¼š

$$\text{Citation-Alignment} = \frac{I(Y;C)}{H(Y)}$$

å…¶ä¸­ $I(Y;C)$ ç‚ºç”Ÿæˆæ–‡æœ¬ $Y$ èˆ‡ä¸Šä¸‹æ–‡ $C$ çš„äº’è³‡è¨Šï¼Œ$H(Y)$ ç‚ºç”Ÿæˆæ–‡æœ¬çš„ç†µã€‚

---

## 2. vLLM ç”Ÿç”¢éƒ¨ç½²æ·±åº¦è§£æ

### 2.1 vLLM æ¶æ§‹åŸç†

#### **PagedAttention çš„å‰µæ–°æ©Ÿåˆ¶**

vLLM (Kwon et al., 2023)[^24] çš„æ ¸å¿ƒå‰µæ–°æ˜¯ PagedAttention æ©Ÿåˆ¶ï¼š

**åŸç†**: å°‡æ³¨æ„åŠ›è¨ˆç®—çš„ KV Cache åˆ†é ç®¡ç†ï¼Œé¡ä¼¼ä½œæ¥­ç³»çµ±çš„è™›æ“¬è¨˜æ†¶é«”ï¼š

$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{Q \cdot \text{PagedK}^T}{\sqrt{d_k}}\right) \cdot \text{PagedV}$$

**å„ªå‹¢åˆ†æ**:
- **è¨˜æ†¶é«”æ•ˆç‡**: æ¸›å°‘ 60-80% è¨˜æ†¶é«”æµªè²»
- **å‹•æ…‹æ‰¹æ¬¡**: æ”¯æ´ä¸åŒåºåˆ—é•·åº¦çš„å‹•æ…‹æ‰¹æ¬¡è™•ç†
- **ä¸¦è¡Œå„ªåŒ–**: æ›´å¥½çš„ GPU åˆ©ç”¨ç‡

#### **ä¼æ¥­ç´š vLLM éƒ¨ç½²å¯¦ç¾**

```python
import asyncio
import torch
from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams
from vllm.utils import random_uuid
from typing import Dict, List, Optional, AsyncGenerator
import time
from dataclasses import dataclass

@dataclass
class GenerationRequest:
    """ç”Ÿæˆè«‹æ±‚æ•¸æ“šçµæ§‹"""
    request_id: str
    prompt: str
    sampling_params: SamplingParams
    metadata: Dict
    priority: int = 5  # 1-10, 10ç‚ºæœ€é«˜å„ªå…ˆç´š

class EnterprisevLLMService:
    """ä¼æ¥­ç´š vLLM æœå‹™"""

    def __init__(self, model_config: Dict):
        # vLLM å¼•æ“é…ç½®
        self.engine_args = AsyncEngineArgs(
            model=model_config["model_path"],
            tokenizer=model_config.get("tokenizer_path"),

            # ä¸¦è¡Œé…ç½®
            tensor_parallel_size=model_config.get("tensor_parallel_size", 4),
            pipeline_parallel_size=model_config.get("pipeline_parallel_size", 1),

            # è¨˜æ†¶é«”å„ªåŒ–
            gpu_memory_utilization=model_config.get("gpu_memory_utilization", 0.9),
            swap_space=model_config.get("swap_space", 4),  # GB

            # æ€§èƒ½å„ªåŒ–
            max_num_batched_tokens=model_config.get("max_batched_tokens", 8192),
            max_num_seqs=model_config.get("max_num_seqs", 256),
            enable_chunked_prefill=model_config.get("enable_chunked_prefill", True),

            # ç²¾åº¦é…ç½®
            dtype=model_config.get("dtype", "bfloat16"),
            quantization=model_config.get("quantization"),  # "awq", "gptq"

            # å…¶ä»–é…ç½®
            disable_log_stats=False,
            trust_remote_code=True
        )

        self.engine = AsyncLLMEngine.from_engine_args(self.engine_args)

        # è«‹æ±‚èª¿åº¦å™¨
        self.request_scheduler = RequestScheduler()

        # æ€§èƒ½ç›£æ§
        self.performance_monitor = vLLMPerformanceMonitor()

    async def generate_with_context(self, prompt: str,
                                  context_sources: List[Dict],
                                  generation_config: Dict) -> Dict:
        """å¸¶ä¸Šä¸‹æ–‡çš„å—æ§ç”Ÿæˆ"""

        # éšæ®µ1: ä¸Šä¸‹æ–‡é è™•ç†
        processed_context = await self._preprocess_context(
            context_sources, generation_config
        )

        # éšæ®µ2: æç¤ºå·¥ç¨‹
        structured_prompt = await self._build_structured_prompt(
            prompt, processed_context, generation_config
        )

        # éšæ®µ3: ç”Ÿæˆåƒæ•¸é…ç½®
        sampling_params = self._configure_sampling_parameters(generation_config)

        # éšæ®µ4: å—æ§ç”Ÿæˆ
        generation_result = await self._controlled_generation(
            structured_prompt, sampling_params
        )

        # éšæ®µ5: å¾Œè™•ç†èˆ‡é©—è­‰
        validated_result = await self._post_process_and_validate(
            generation_result, context_sources, prompt
        )

        return validated_result

    async def _preprocess_context(self, sources: List[Dict],
                                config: Dict) -> Dict:
        """é è™•ç†ä¸Šä¸‹æ–‡è³‡æ–™"""

        # 1. ä¾†æºæ’åºèˆ‡é¸æ“‡
        ranked_sources = await self._rank_sources_by_relevance(sources, config)

        # 2. å…§å®¹æ¸…ç†èˆ‡æ ¼å¼åŒ–
        cleaned_sources = []
        for source in ranked_sources[:config.get("max_sources", 10)]:
            cleaned_content = await self._clean_source_content(source["content"])

            # æ·»åŠ ä¾†æºæ¨™è­˜
            source_id = f"SOURCE_{len(cleaned_sources) + 1}"
            formatted_content = f"[{source_id}] {cleaned_content}"

            cleaned_sources.append({
                "id": source_id,
                "content": formatted_content,
                "metadata": source.get("metadata", {}),
                "confidence": source.get("confidence", 1.0)
            })

        # 3. ä¸Šä¸‹æ–‡é•·åº¦æ§åˆ¶
        total_length = sum(len(s["content"]) for s in cleaned_sources)
        max_context_length = config.get("max_context_tokens", 16384)

        if total_length > max_context_length:
            # æ™ºèƒ½æˆªæ–·ï¼šä¿ç•™æœ€é‡è¦çš„ä¾†æº
            truncated_sources = await self._intelligent_truncation(
                cleaned_sources, max_context_length
            )
        else:
            truncated_sources = cleaned_sources

        return {
            "formatted_sources": truncated_sources,
            "total_tokens": sum(len(s["content"]) for s in truncated_sources),
            "truncated": len(truncated_sources) < len(cleaned_sources)
        }

    async def _build_structured_prompt(self, user_query: str,
                                     context: Dict,
                                     config: Dict) -> str:
        """æ§‹å»ºçµæ§‹åŒ–æç¤º"""

        template_type = config.get("template_type", "standard")

        if template_type == "enterprise_qa":
            template = """
æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„ä¼æ¥­çŸ¥è­˜åŠ©ç†ã€‚è«‹åŸºæ–¼æä¾›çš„æ¬Šå¨è³‡æ–™ä¾†æºå›ç­”å•é¡Œã€‚

## é‡è¦æŒ‡ç¤º
1. åƒ…åŸºæ–¼æä¾›çš„è³‡æ–™ä¾†æºå›ç­”å•é¡Œ
2. å°æ‰€æœ‰é—œéµé™³è¿°ä½¿ç”¨ [SOURCE_N] æ ¼å¼å¼•ç”¨ä¾†æº
3. å¦‚æœè³‡æ–™ä¸è¶³ï¼Œæ˜ç¢ºèªªæ˜é™åˆ¶
4. ä¿æŒå®¢è§€ä¸­æ€§çš„èªèª¿
5. æä¾›çµæ§‹åŒ–çš„å›ç­”

## è³‡æ–™ä¾†æº
{context_text}

## ç”¨æˆ¶å•é¡Œ
{user_query}

## å›ç­”æ ¼å¼
**ä¸»è¦å›ç­”**: [åŸºæ–¼è³‡æ–™çš„ç›´æ¥å›ç­”]
**è©³ç´°èªªæ˜**: [æ”¯æŒæ€§ç´°ç¯€å’Œåˆ†æ]
**è³‡æ–™ä¾†æº**: [å¼•ç”¨çš„å…·é«”ä¾†æº]
**é™åˆ¶èªªæ˜**: [å¦‚æœ‰è³‡æ–™é™åˆ¶æˆ–ä¸ç¢ºå®šæ€§]

å›ç­”:
"""
        elif template_type == "technical_support":
            template = """
æ‚¨æ˜¯æŠ€è¡“æ”¯æ´å°ˆå®¶ã€‚è«‹åŸºæ–¼æŠ€è¡“æ–‡æª”æä¾›æº–ç¢ºçš„æŠ€è¡“æŒ‡å°ã€‚

## æŠ€è¡“è³‡æ–™
{context_text}

## æŠ€è¡“å•é¡Œ
{user_query}

## å›ç­”è¦æ±‚
1. æä¾›æ˜ç¢ºçš„æŠ€è¡“è§£æ±ºæ–¹æ¡ˆ
2. åˆ—å‡ºå…·é«”æ“ä½œæ­¥é©Ÿ
3. æ¨™è¨»æ½›åœ¨é¢¨éšªå’Œæ³¨æ„äº‹é …
4. å¼•ç”¨ç›¸é—œæŠ€è¡“æ–‡æª” [SOURCE_N]

æŠ€è¡“å›ç­”:
"""
        else:  # standard template
            template = """
è«‹åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼Œä¸¦ç¢ºä¿ï¼š
1. å›ç­”æº–ç¢ºä¸”åŸºæ–¼äº‹å¯¦
2. é©ç•¶å¼•ç”¨è³‡æ–™ä¾†æº [SOURCE_N]
3. æ‰¿èªè³‡æ–™é™åˆ¶

è³‡æ–™:
{context_text}

å•é¡Œ: {user_query}

å›ç­”:
"""

        # æ ¼å¼åŒ–æ¨¡æ¿
        context_text = "\n\n".join([
            source["content"] for source in context["formatted_sources"]
        ])

        structured_prompt = template.format(
            context_text=context_text,
            user_query=user_query
        )

        return structured_prompt

    def _configure_sampling_parameters(self, config: Dict) -> SamplingParams:
        """é…ç½®æ¡æ¨£åƒæ•¸"""

        return SamplingParams(
            temperature=config.get("temperature", 0.1),
            top_p=config.get("top_p", 0.9),
            top_k=config.get("top_k", 50),
            max_tokens=config.get("max_tokens", 2048),
            stop=config.get("stop_sequences", ["\n\nHuman:", "<|end|>"]),
            presence_penalty=config.get("presence_penalty", 0.0),
            frequency_penalty=config.get("frequency_penalty", 0.0),
            repetition_penalty=config.get("repetition_penalty", 1.1),
            include_stop_str_in_output=False
        )

    async def _controlled_generation(self, prompt: str,
                                   sampling_params: SamplingParams) -> Dict:
        """åŸ·è¡Œå—æ§ç”Ÿæˆ"""

        request_id = random_uuid()
        start_time = time.time()

        try:
            # ç”Ÿæˆæ–‡æœ¬
            outputs = []
            async for request_output in self.engine.generate(
                prompt, sampling_params, request_id
            ):
                outputs.append(request_output)

            # ç²å–æœ€çµ‚è¼¸å‡º
            final_output = outputs[-1]
            generated_text = final_output.outputs[0].text

            generation_time = time.time() - start_time

            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            await self.performance_monitor.record_generation(
                request_id=request_id,
                input_tokens=len(prompt.split()),  # ç°¡åŒ–è¨ˆç®—
                output_tokens=len(generated_text.split()),
                generation_time=generation_time,
                model_name=self.engine_args.model
            )

            return {
                "generated_text": generated_text.strip(),
                "request_id": request_id,
                "generation_time": generation_time,
                "token_count": {
                    "input": len(prompt.split()),
                    "output": len(generated_text.split()),
                    "total": len(prompt.split()) + len(generated_text.split())
                },
                "finish_reason": final_output.outputs[0].finish_reason
            }

        except Exception as e:
            return {
                "error": str(e),
                "request_id": request_id,
                "generation_time": time.time() - start_time
            }

    async def _post_process_and_validate(self, generation_result: Dict,
                                       context_sources: List[Dict],
                                       original_query: str) -> Dict:
        """å¾Œè™•ç†èˆ‡é©—è­‰"""

        if "error" in generation_result:
            return generation_result

        generated_text = generation_result["generated_text"]

        # 1. å¼•ç”¨æå–èˆ‡é©—è­‰
        citations = await self._extract_and_validate_citations(
            generated_text, context_sources
        )

        # 2. äº‹å¯¦ä¸€è‡´æ€§æª¢æŸ¥
        factual_consistency = await self._check_factual_consistency(
            generated_text, context_sources
        )

        # 3. ç›¸é—œæ€§è©•ä¼°
        relevance_score = await self._assess_response_relevance(
            generated_text, original_query
        )

        # 4. å®‰å…¨æ€§æª¢æŸ¥
        safety_check = await self._perform_safety_check(generated_text)

        # 5. å“è³ªç¶œåˆè©•åˆ†
        quality_score = self._calculate_generation_quality(
            citations, factual_consistency, relevance_score, safety_check
        )

        return {
            **generation_result,
            "citations": citations,
            "factual_consistency": factual_consistency,
            "relevance_score": relevance_score,
            "safety_check": safety_check,
            "quality_score": quality_score,
            "validation_status": "passed" if quality_score > 0.7 else "failed"
        }

class vLLMPerformanceMonitor:
    """vLLM æ€§èƒ½ç›£æ§å™¨"""

    def __init__(self):
        self.metrics_buffer = []
        self.performance_thresholds = {
            "max_latency_ms": 5000,
            "min_throughput_tokens_per_sec": 100,
            "max_gpu_memory_percent": 95,
            "max_error_rate_percent": 5
        }

    async def record_generation(self, **kwargs):
        """è¨˜éŒ„ç”Ÿæˆæ€§èƒ½æŒ‡æ¨™"""

        metrics = {
            "timestamp": time.time(),
            **kwargs
        }

        self.metrics_buffer.append(metrics)

        # ä¿æŒç·©è¡å€å¤§å°
        if len(self.metrics_buffer) > 1000:
            self.metrics_buffer.pop(0)

        # æª¢æŸ¥æ€§èƒ½å‘Šè­¦
        await self._check_performance_alerts(metrics)

    async def _check_performance_alerts(self, current_metrics: Dict):
        """æª¢æŸ¥æ€§èƒ½å‘Šè­¦æ¢ä»¶"""

        # å»¶é²å‘Šè­¦
        if current_metrics.get("generation_time", 0) * 1000 > self.performance_thresholds["max_latency_ms"]:
            await self._trigger_alert("high_latency", current_metrics)

        # è¨ˆç®—æœ€è¿‘çš„å¹³å‡æ€§èƒ½
        recent_metrics = self.metrics_buffer[-10:]  # æœ€è¿‘10æ¬¡
        if len(recent_metrics) >= 5:
            avg_tokens_per_sec = sum(
                m.get("output_tokens", 0) / max(m.get("generation_time", 1), 0.001)
                for m in recent_metrics
            ) / len(recent_metrics)

            if avg_tokens_per_sec < self.performance_thresholds["min_throughput_tokens_per_sec"]:
                await self._trigger_alert("low_throughput", {
                    "current_throughput": avg_tokens_per_sec,
                    "threshold": self.performance_thresholds["min_throughput_tokens_per_sec"]
                })

    async def _trigger_alert(self, alert_type: str, metrics: Dict):
        """è§¸ç™¼æ€§èƒ½å‘Šè­¦"""

        alert_message = {
            "alert_type": alert_type,
            "timestamp": time.time(),
            "metrics": metrics,
            "severity": "warning" if alert_type == "low_throughput" else "critical"
        }

        # å¯¦éš›å¯¦ç¾ä¸­æœƒç™¼é€åˆ°å‘Šè­¦ç³»çµ±
        print(f"ğŸš¨ Performance Alert: {alert_message}")
```

---

## 3. å¼•ç”¨ç³»çµ±çš„ç†è«–èˆ‡å¯¦ç¾

### 3.1 è‡ªå‹•å¼•ç”¨ç”Ÿæˆç†è«–

#### **ä¾†æºæ­¸å±¬çš„æ¼”ç®—æ³•æ¡†æ¶**

**å®šç¾© 3.1** (ä¾†æºæ­¸å±¬å•é¡Œ): çµ¦å®šç”Ÿæˆæ–‡æœ¬ $y = \{s_1, s_2, ..., s_n\}$ (å¥å­åºåˆ—) å’Œä¾†æºé›†åˆ $C = \{c_1, c_2, ..., c_m\}$ï¼Œæ‰¾åˆ°æœ€å„ªæ­¸å±¬å‡½æ•¸ï¼š

$$\text{Attribution}: \{s_1, ..., s_n\} \rightarrow \mathcal{P}(C)$$

ä½¿å¾—æ­¸å±¬æº–ç¢ºåº¦æœ€å¤§åŒ–ï¼š

$$\max \sum_{i=1}^{n} \text{Accuracy}(\text{Attribution}(s_i), \text{TrueSource}(s_i))$$

#### **èªç¾©ç›¸ä¼¼åº¦é©—è­‰ç®—æ³•**

```python
from sentence_transformers import SentenceTransformer, CrossEncoder
import numpy as np
from typing import Dict, List, Tuple
import re

class AutomaticCitationGenerator:
    """è‡ªå‹•å¼•ç”¨ç”Ÿæˆç³»çµ±"""

    def __init__(self):
        # èªç¾©ç›¸ä¼¼åº¦æ¨¡å‹
        self.sentence_model = SentenceTransformer(
            'paraphrase-multilingual-MiniLM-L12-v2'
        )

        # å¼•ç”¨é©—è­‰æ¨¡å‹
        self.citation_verifier = CrossEncoder(
            'cross-encoder/ms-marco-MiniLM-L-6-v2'
        )

        # å¼•ç”¨æ ¼å¼æ­£å‰‡è¡¨é”å¼
        self.citation_pattern = re.compile(r'\[SOURCE_(\d+)\]')

    async def generate_citations(self, generated_text: str,
                               source_documents: List[Dict]) -> Dict:
        """ç”Ÿæˆä¸¦é©—è­‰å¼•ç”¨"""

        # 1. å¥å­åˆ†å‰²
        sentences = await self._split_into_sentences(generated_text)

        # 2. ç‚ºæ¯å€‹å¥å­æ‰¾åˆ°æœ€ä½³ä¾†æº
        sentence_attributions = []
        for sentence in sentences:
            attribution = await self._find_best_source_attribution(
                sentence, source_documents
            )
            sentence_attributions.append(attribution)

        # 3. ç”Ÿæˆå¼•ç”¨å¢å¼·æ–‡æœ¬
        citation_enhanced_text = await self._insert_citations(
            sentences, sentence_attributions
        )

        # 4. é©—è­‰å¼•ç”¨å“è³ª
        citation_quality = await self._validate_citation_quality(
            citation_enhanced_text, source_documents
        )

        # 5. ç”Ÿæˆåƒè€ƒæ–‡ç»
        bibliography = self._generate_bibliography(
            source_documents, sentence_attributions
        )

        return {
            "original_text": generated_text,
            "citation_enhanced_text": citation_enhanced_text,
            "sentence_attributions": sentence_attributions,
            "citation_quality": citation_quality,
            "bibliography": bibliography,
            "citation_coverage": len([a for a in sentence_attributions if a["has_citation"]]) / len(sentences)
        }

    async def _find_best_source_attribution(self, sentence: str,
                                          sources: List[Dict]) -> Dict:
        """ç‚ºå¥å­æ‰¾åˆ°æœ€ä½³ä¾†æºæ­¸å±¬"""

        if not sources:
            return {"has_citation": False, "reason": "no_sources"}

        # è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦
        sentence_embedding = self.sentence_model.encode([sentence])[0]

        best_attribution = {
            "has_citation": False,
            "source_id": None,
            "confidence": 0.0,
            "similarity_score": 0.0,
            "evidence_text": ""
        }

        for i, source in enumerate(sources):
            source_content = source["content"]

            # å°‡ä¾†æºåˆ†å‰²ç‚ºæ®µè½é€²è¡ŒåŒ¹é…
            source_paragraphs = self._split_into_paragraphs(source_content)

            for paragraph in source_paragraphs:
                # è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦
                para_embedding = self.sentence_model.encode([paragraph])[0]
                similarity = self._cosine_similarity(sentence_embedding, para_embedding)

                # ä½¿ç”¨äº¤å‰ç·¨ç¢¼å™¨é€²è¡Œç²¾ç¢ºé©—è­‰
                if similarity > 0.5:  # åˆæ­¥ç¯©é¸
                    cross_encoder_score = self.citation_verifier.predict([
                        (sentence, paragraph)
                    ])[0]

                    # ç¶œåˆè©•åˆ†
                    combined_score = 0.6 * similarity + 0.4 * cross_encoder_score

                    if combined_score > best_attribution["confidence"]:
                        best_attribution = {
                            "has_citation": combined_score > 0.7,
                            "source_id": f"SOURCE_{i + 1}",
                            "confidence": combined_score,
                            "similarity_score": similarity,
                            "cross_encoder_score": cross_encoder_score,
                            "evidence_text": paragraph[:200] + "..." if len(paragraph) > 200 else paragraph
                        }

        return best_attribution

    async def _validate_citation_quality(self, cited_text: str,
                                       sources: List[Dict]) -> Dict:
        """é©—è­‰å¼•ç”¨å“è³ª"""

        # æå–æ‰€æœ‰å¼•ç”¨
        citations = self.citation_pattern.findall(cited_text)

        validation_results = {
            "total_citations": len(citations),
            "valid_citations": 0,
            "invalid_citations": [],
            "coverage_analysis": {},
            "accuracy_score": 0.0
        }

        for citation in set(citations):  # å»é‡
            source_idx = int(citation) - 1  # è½‰æ›ç‚ºç´¢å¼•

            if 0 <= source_idx < len(sources):
                validation_results["valid_citations"] += 1

                # é©—è­‰å¼•ç”¨æº–ç¢ºæ€§
                citation_accuracy = await self._verify_citation_accuracy(
                    cited_text, citation, sources[source_idx]
                )

                validation_results["coverage_analysis"][citation] = citation_accuracy
            else:
                validation_results["invalid_citations"].append({
                    "citation": citation,
                    "error": "source_not_found"
                })

        # è¨ˆç®—ç¸½é«”æº–ç¢ºæ€§
        if validation_results["valid_citations"] > 0:
            accuracy_scores = [
                acc["accuracy"] for acc in validation_results["coverage_analysis"].values()
            ]
            validation_results["accuracy_score"] = sum(accuracy_scores) / len(accuracy_scores)

        return validation_results

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""

        dot_product = np.dot(vec1, vec2)
        norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)

        if norm_product == 0:
            return 0.0

        return dot_product / norm_product

    async def _verify_citation_accuracy(self, cited_text: str,
                                      citation: str,
                                      source: Dict) -> Dict:
        """é©—è­‰ç‰¹å®šå¼•ç”¨çš„æº–ç¢ºæ€§"""

        # æ‰¾åˆ°åŒ…å«è©²å¼•ç”¨çš„å¥å­
        sentences_with_citation = []
        for sentence in cited_text.split('.'):
            if f"[SOURCE_{citation}]" in sentence:
                sentences_with_citation.append(sentence.strip())

        if not sentences_with_citation:
            return {"accuracy": 0.0, "reason": "citation_not_found_in_text"}

        # é©—è­‰æ¯å€‹åŒ…å«å¼•ç”¨çš„å¥å­
        accuracy_scores = []
        for sentence in sentences_with_citation:
            # æ¸…é™¤å¼•ç”¨æ¨™è¨˜ï¼Œåªä¿ç•™é™³è¿°å…§å®¹
            clean_sentence = re.sub(r'\[SOURCE_\d+\]', '', sentence).strip()

            if len(clean_sentence) < 10:
                continue

            # åœ¨ä¾†æºä¸­æŸ¥æ‰¾æ”¯æŒè­‰æ“š
            evidence_found = await self._find_supporting_evidence(
                clean_sentence, source["content"]
            )

            accuracy_scores.append(evidence_found["confidence"])

        if accuracy_scores:
            avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)
        else:
            avg_accuracy = 0.0

        return {
            "accuracy": avg_accuracy,
            "sentences_checked": len(sentences_with_citation),
            "evidence_quality": accuracy_scores
        }

    async def _find_supporting_evidence(self, statement: str,
                                      source_content: str) -> Dict:
        """åœ¨ä¾†æºä¸­æŸ¥æ‰¾æ”¯æŒè­‰æ“š"""

        # å°‡ä¾†æºåˆ†å‰²ç‚ºå¯æª¢ç´¢çš„ç‰‡æ®µ
        source_chunks = self._split_into_chunks(source_content, chunk_size=200)

        best_evidence = {"confidence": 0.0, "evidence_text": "", "chunk_index": -1}

        # ç‚ºæ¯å€‹ç‰‡æ®µè¨ˆç®—æ”¯æŒåº¦
        for i, chunk in enumerate(source_chunks):
            # èªç¾©ç›¸ä¼¼åº¦
            similarity = await self._calculate_semantic_similarity(statement, chunk)

            # è©å½™é‡ç–Šåº¦
            lexical_overlap = self._calculate_lexical_overlap(statement, chunk)

            # ç¶œåˆç½®ä¿¡åº¦
            confidence = 0.7 * similarity + 0.3 * lexical_overlap

            if confidence > best_evidence["confidence"]:
                best_evidence = {
                    "confidence": confidence,
                    "evidence_text": chunk,
                    "chunk_index": i
                }

        return best_evidence

    def _calculate_lexical_overlap(self, text1: str, text2: str) -> float:
        """è¨ˆç®—è©å½™é‡ç–Šåº¦"""

        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1:
            return 0.0

        intersection = words1 & words2
        return len(intersection) / len(words1)

    def _split_into_chunks(self, text: str, chunk_size: int = 200) -> List[str]:
        """å°‡æ–‡æœ¬åˆ†å‰²ç‚ºç‰‡æ®µ"""

        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)

        return chunks

    def _generate_bibliography(self, sources: List[Dict],
                             attributions: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆåƒè€ƒæ–‡ç»"""

        # çµ±è¨ˆè¢«å¼•ç”¨çš„ä¾†æº
        cited_sources = set()
        for attr in attributions:
            if attr.get("has_citation") and attr.get("source_id"):
                source_num = int(attr["source_id"].replace("SOURCE_", ""))
                cited_sources.add(source_num - 1)  # è½‰ç‚ºç´¢å¼•

        # ç”Ÿæˆåƒè€ƒæ–‡ç»æ¢ç›®
        bibliography = []
        for i in sorted(cited_sources):
            if i < len(sources):
                source = sources[i]
                bib_entry = {
                    "source_number": i + 1,
                    "title": source.get("metadata", {}).get("title", f"Document {i + 1}"),
                    "author": source.get("metadata", {}).get("author", "Unknown"),
                    "date": source.get("metadata", {}).get("date", "Unknown"),
                    "url": source.get("metadata", {}).get("url", ""),
                    "document_type": source.get("metadata", {}).get("type", "Document")
                }
                bibliography.append(bib_entry)

        return bibliography
```

---

## 4. äº‹å¯¦æª¢æŸ¥èˆ‡é©—è­‰ç³»çµ±

### 4.1 å¤šå±¤æ¬¡äº‹å¯¦é©—è­‰æ¡†æ¶

#### **äº‹å¯¦é©—è­‰çš„ç†è«–æ¨¡å‹**

**å®šç¾© 4.1** (äº‹å¯¦é™³è¿°): å¯ä»¥è¢«å®¢è§€é©—è­‰ç‚ºçœŸæˆ–å‡çš„é™³è¿°ã€‚

**å®šç† 4.1** (äº‹å¯¦é©—è­‰çš„ä¸å®Œå…¨æ€§): åœ¨é–‹æ”¾åŸŸçŸ¥è­˜ç³»çµ±ä¸­ï¼Œä¸å­˜åœ¨å®Œç¾çš„äº‹å¯¦é©—è­‰ç®—æ³•ï¼Œä»»ä½•é©—è­‰ç³»çµ±éƒ½å­˜åœ¨ï¼š

- **ç¬¬ä¸€é¡éŒ¯èª¤** (å‡é™½æ€§): å°‡éŒ¯èª¤é™³è¿°åˆ¤å®šç‚ºæ­£ç¢º
- **ç¬¬äºŒé¡éŒ¯èª¤** (å‡é™°æ€§): å°‡æ­£ç¢ºé™³è¿°åˆ¤å®šç‚ºéŒ¯èª¤

**å„ªåŒ–ç›®æ¨™**: åœ¨çµ¦å®šçš„éŒ¯èª¤å®¹å¿åº¦ä¸‹ï¼Œæœ€å¤§åŒ–é©—è­‰è¦†è“‹ç‡ã€‚

#### **å¤šå±¤æ¬¡é©—è­‰æ¶æ§‹**

```python
from transformers import pipeline
import spacy
from typing import Dict, List, Tuple, Optional

class MultiLevelFactChecker:
    """å¤šå±¤æ¬¡äº‹å¯¦æª¢æŸ¥å™¨"""

    def __init__(self):
        # NLI æ¨¡å‹ç”¨æ–¼è˜Šå«é—œä¿‚æª¢æŸ¥
        self.nli_model = pipeline(
            "text-classification",
            model="microsoft/deberta-large-mnli",
            device=0 if torch.cuda.is_available() else -1
        )

        # äº‹å¯¦æ€§æª¢æŸ¥æ¨¡å‹
        self.factuality_checker = pipeline(
            "text-classification",
            model="tals/albert-xlarge-vitaminc-mnli",
            device=0 if torch.cuda.is_available() else -1
        )

        # NLP è™•ç†å·¥å…·
        self.nlp = spacy.load("en_core_web_lg")

    async def comprehensive_fact_check(self, generated_text: str,
                                     source_contexts: List[str],
                                     external_knowledge: Optional[Dict] = None) -> Dict:
        """å…¨é¢äº‹å¯¦æª¢æŸ¥"""

        # ç¬¬ä¸€å±¤: é™³è¿°æŠ½å–
        factual_statements = await self._extract_factual_statements(generated_text)

        # ç¬¬äºŒå±¤: ä¸Šä¸‹æ–‡è˜Šå«æª¢æŸ¥
        entailment_results = await self._check_context_entailment(
            factual_statements, source_contexts
        )

        # ç¬¬ä¸‰å±¤: å¤–éƒ¨çŸ¥è­˜é©—è­‰ (å¦‚æœå¯ç”¨)
        external_validation = {}
        if external_knowledge:
            external_validation = await self._validate_against_external_kb(
                factual_statements, external_knowledge
            )

        # ç¬¬å››å±¤: ä¸€è‡´æ€§æª¢æŸ¥
        consistency_check = await self._check_internal_consistency(factual_statements)

        # ç¶œåˆè©•ä¼°
        overall_assessment = await self._synthesize_fact_check_results(
            entailment_results, external_validation, consistency_check
        )

        return {
            "factual_statements": factual_statements,
            "entailment_results": entailment_results,
            "external_validation": external_validation,
            "consistency_check": consistency_check,
            "overall_assessment": overall_assessment
        }

    async def _extract_factual_statements(self, text: str) -> List[Dict]:
        """æå–äº‹å¯¦é™³è¿°"""

        doc = self.nlp(text)
        factual_statements = []

        for sent in doc.sents:
            sentence_text = sent.text.strip()

            if len(sentence_text) < 10:
                continue

            # æª¢æŸ¥æ˜¯å¦ç‚ºäº‹å¯¦é™³è¿° (vs è§€é»ã€æŒ‡ä»¤ç­‰)
            is_factual = await self._classify_statement_type(sentence_text)

            if is_factual["type"] == "factual":
                # æŠ½å–é—œéµå¯¦é«”å’Œæ•¸å€¼
                entities = [(ent.text, ent.label_) for ent in sent.ents]
                numbers = re.findall(r'\b\d+(?:,\d{3})*(?:\.\d+)?\b', sentence_text)
                dates = re.findall(r'\b\d{4}\b|\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b', sentence_text)

                factual_statements.append({
                    "text": sentence_text,
                    "sentence_id": len(factual_statements),
                    "entities": entities,
                    "numbers": numbers,
                    "dates": dates,
                    "factual_confidence": is_factual["confidence"],
                    "statement_type": "factual"
                })

        return factual_statements

    async def _check_context_entailment(self, statements: List[Dict],
                                      contexts: List[str]) -> Dict:
        """æª¢æŸ¥ä¸Šä¸‹æ–‡è˜Šå«é—œä¿‚"""

        entailment_results = []
        combined_context = "\n\n".join(contexts)

        for statement in statements:
            statement_text = statement["text"]

            # ä½¿ç”¨ NLI æ¨¡å‹æª¢æŸ¥è˜Šå«é—œä¿‚
            nli_result = self.nli_model(f"{combined_context} {statement_text}")

            # è§£æ NLI çµæœ
            entailment_score = 0.0
            for result in nli_result:
                if result["label"] == "ENTAILMENT":
                    entailment_score = result["score"]
                    break

            # ä½¿ç”¨äº‹å¯¦æ€§æª¢æŸ¥æ¨¡å‹é€²è¡ŒäºŒæ¬¡é©—è­‰
            factuality_result = self.factuality_checker(f"{combined_context} [SEP] {statement_text}")

            factuality_score = 0.0
            for result in factuality_result:
                if result["label"] in ["SUPPORTS", "ENTAILMENT"]:
                    factuality_score = result["score"]
                    break

            # ç¶œåˆåˆ¤æ–·
            combined_confidence = (entailment_score + factuality_score) / 2

            entailment_results.append({
                "statement": statement_text,
                "entailment_score": entailment_score,
                "factuality_score": factuality_score,
                "combined_confidence": combined_confidence,
                "supported": combined_confidence > 0.7,
                "evidence_strength": "strong" if combined_confidence > 0.8 else
                                   "moderate" if combined_confidence > 0.6 else "weak"
            })

        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        supported_count = sum(1 for r in entailment_results if r["supported"])
        total_statements = len(entailment_results)

        return {
            "statement_results": entailment_results,
            "overall_support_rate": supported_count / total_statements if total_statements > 0 else 0,
            "average_confidence": sum(r["combined_confidence"] for r in entailment_results) / total_statements if total_statements > 0 else 0,
            "unsupported_statements": [r for r in entailment_results if not r["supported"]]
        }

    async def _classify_statement_type(self, statement: str) -> Dict:
        """åˆ†é¡é™³è¿°é¡å‹"""

        # äº‹å¯¦é™³è¿°çš„èªè¨€ç‰¹å¾µ
        factual_indicators = [
            "is", "are", "was", "were", "has", "have", "will",
            "reports", "shows", "indicates", "found", "discovered"
        ]

        # è§€é»é™³è¿°çš„èªè¨€ç‰¹å¾µ
        opinion_indicators = [
            "think", "believe", "feel", "opinion", "seems", "appears",
            "should", "must", "recommend", "suggest"
        ]

        statement_lower = statement.lower()

        factual_score = sum(1 for indicator in factual_indicators
                           if indicator in statement_lower)
        opinion_score = sum(1 for indicator in opinion_indicators
                          if indicator in statement_lower)

        if factual_score > opinion_score:
            return {"type": "factual", "confidence": 0.8}
        elif opinion_score > factual_score:
            return {"type": "opinion", "confidence": 0.8}
        else:
            return {"type": "uncertain", "confidence": 0.5}
```

---

## 5. ç”Ÿç”¢ç´šéƒ¨ç½²æœ€ä½³å¯¦è¸

### 5.1 é«˜å¯ç”¨æ€§æ¶æ§‹è¨­è¨ˆ

#### **è² è¼‰å¹³è¡¡èˆ‡æ•…éšœåˆ‡æ›**

```python
import asyncio
from typing import Dict, List, Optional, Any
import random
import time

class LLMLoadBalancer:
    """LLM è² è¼‰å¹³è¡¡å™¨"""

    def __init__(self, model_instances: Dict[str, Dict]):
        self.instances = model_instances
        self.health_checker = InstanceHealthChecker()
        self.request_router = RequestRouter()

        # è² è¼‰å¹³è¡¡ç­–ç•¥
        self.balancing_strategies = {
            "round_robin": self._round_robin_selection,
            "least_connections": self._least_connections_selection,
            "weighted_response_time": self._weighted_response_time_selection,
            "resource_aware": self._resource_aware_selection
        }

        self.current_strategy = "resource_aware"

    async def route_generation_request(self, request: GenerationRequest) -> Dict:
        """è·¯ç”±ç”Ÿæˆè«‹æ±‚åˆ°æœ€ä½³å¯¦ä¾‹"""

        # 1. å¥åº·å¯¦ä¾‹ç¯©é¸
        healthy_instances = await self._get_healthy_instances()

        if not healthy_instances:
            return {
                "error": "No healthy instances available",
                "status": "service_unavailable"
            }

        # 2. è«‹æ±‚è·¯ç”±
        selected_instance = await self._select_optimal_instance(
            healthy_instances, request
        )

        # 3. åŸ·è¡Œè«‹æ±‚
        try:
            result = await self._execute_on_instance(selected_instance, request)

            # 4. è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            await self._update_instance_metrics(selected_instance, result, success=True)

            return result

        except Exception as e:
            # æ•…éšœåˆ‡æ›
            await self._handle_instance_failure(selected_instance, str(e))

            # å˜—è©¦å‚™ç”¨å¯¦ä¾‹
            backup_result = await self._try_backup_instances(
                healthy_instances, request, exclude=[selected_instance]
            )

            return backup_result

    async def _get_healthy_instances(self) -> List[str]:
        """ç²å–å¥åº·çš„å¯¦ä¾‹åˆ—è¡¨"""

        healthy_instances = []

        for instance_id, instance_config in self.instances.items():
            health_status = await self.health_checker.check_instance_health(
                instance_id, instance_config
            )

            if health_status["status"] == "healthy":
                healthy_instances.append(instance_id)

        return healthy_instances

    async def _select_optimal_instance(self, healthy_instances: List[str],
                                     request: GenerationRequest) -> str:
        """é¸æ“‡æœ€ä½³å¯¦ä¾‹"""

        strategy_func = self.balancing_strategies[self.current_strategy]
        return await strategy_func(healthy_instances, request)

    async def _resource_aware_selection(self, instances: List[str],
                                      request: GenerationRequest) -> str:
        """è³‡æºæ„ŸçŸ¥çš„å¯¦ä¾‹é¸æ“‡"""

        instance_scores = {}

        for instance_id in instances:
            instance_config = self.instances[instance_id]

            # ç²å–ç•¶å‰è³‡æºä½¿ç”¨æƒ…æ³
            resource_usage = await self._get_instance_resource_usage(instance_id)

            # è¨ˆç®—è² è¼‰åˆ†æ•¸ (è¶Šä½è¶Šå¥½)
            load_score = (
                0.4 * resource_usage.get("gpu_utilization", 0) +
                0.3 * resource_usage.get("memory_utilization", 0) +
                0.2 * resource_usage.get("cpu_utilization", 0) +
                0.1 * resource_usage.get("queue_depth", 0) / 100  # æ¨™æº–åŒ–
            )

            # è€ƒæ…®å¯¦ä¾‹æ€§èƒ½æ¬Šé‡
            performance_weight = instance_config.get("performance_weight", 1.0)

            # ç¶œåˆè©•åˆ† (è¶Šä½è¶Šå¥½)
            instance_scores[instance_id] = load_score / performance_weight

        # é¸æ“‡è² è¼‰æœ€ä½çš„å¯¦ä¾‹
        best_instance = min(instance_scores.keys(), key=lambda k: instance_scores[k])

        return best_instance

    async def _execute_on_instance(self, instance_id: str,
                                 request: GenerationRequest) -> Dict:
        """åœ¨æŒ‡å®šå¯¦ä¾‹ä¸ŠåŸ·è¡Œè«‹æ±‚"""

        instance_config = self.instances[instance_id]
        instance_client = instance_config["client"]

        start_time = time.time()

        # åŸ·è¡Œç”Ÿæˆè«‹æ±‚
        result = await instance_client.generate(
            prompt=request.prompt,
            sampling_params=request.sampling_params
        )

        execution_time = time.time() - start_time

        return {
            "result": result,
            "instance_id": instance_id,
            "execution_time": execution_time,
            "status": "success"
        }

    async def _handle_instance_failure(self, instance_id: str, error: str):
        """è™•ç†å¯¦ä¾‹æ•…éšœ"""

        # æ¨™è¨˜å¯¦ä¾‹ç‚ºä¸å¥åº·
        await self.health_checker.mark_instance_unhealthy(instance_id, error)

        # è¨˜éŒ„æ•…éšœ
        await self._log_instance_failure(instance_id, error)

        # è§¸ç™¼å‘Šè­¦
        await self._trigger_failure_alert(instance_id, error)

    async def _try_backup_instances(self, available_instances: List[str],
                                  request: GenerationRequest,
                                  exclude: List[str]) -> Dict:
        """å˜—è©¦å‚™ç”¨å¯¦ä¾‹"""

        backup_instances = [i for i in available_instances if i not in exclude]

        if not backup_instances:
            return {
                "error": "No backup instances available",
                "status": "all_instances_failed"
            }

        # é¸æ“‡å‚™ç”¨å¯¦ä¾‹
        backup_instance = backup_instances[0]  # ç°¡åŒ–é¸æ“‡

        try:
            return await self._execute_on_instance(backup_instance, request)
        except Exception as e:
            return {
                "error": f"Backup instance also failed: {str(e)}",
                "status": "backup_failed"
            }
```

---

## 6. æœ¬ç« ç¸½çµ

### 6.1 æ ¸å¿ƒå­¸ç¿’è¦é»

1. **ç†è«–åŸºç¤**: æ·±åº¦ç†è§£ Transformer æ¶æ§‹å’Œç”Ÿæˆæ§åˆ¶ç†è«–
2. **ç³»çµ±è¨­è¨ˆ**: æŒæ¡ä¼æ¥­ç´š LLM æœå‹™çš„æ¶æ§‹è¨­è¨ˆåŸå‰‡
3. **å“è³ªä¿è­‰**: å»ºç«‹å®Œæ•´çš„äº‹å¯¦æª¢æŸ¥å’Œå¼•ç”¨é©—è­‰æ©Ÿåˆ¶
4. **æ€§èƒ½å„ªåŒ–**: å¯¦ç¾é«˜æ•ˆèƒ½ã€é«˜å¯ç”¨çš„ç”Ÿç”¢éƒ¨ç½²ç­–ç•¥

### 6.2 å¯¦è¸æŒ‡å°åŸå‰‡

1. **å“è³ªå„ªæ–¼é€Ÿåº¦**: åœ¨ä¼æ¥­ç’°å¢ƒä¸­ï¼Œæº–ç¢ºæ€§æ¯”ç”Ÿæˆé€Ÿåº¦æ›´é‡è¦
2. **å¯è¿½æº¯æ€§**: æ‰€æœ‰ç”Ÿæˆå…§å®¹éƒ½æ‡‰æœ‰æ˜ç¢ºçš„ä¾†æºæ­¸å±¬
3. **æ¼¸é€²å¼éƒ¨ç½²**: å¾ä½é¢¨éšªå ´æ™¯é–‹å§‹ï¼Œé€æ­¥æ“´å±•åˆ°é—œéµæ¥­å‹™
4. **æŒçºŒç›£æ§**: å»ºç«‹å®Œå–„çš„æ€§èƒ½å’Œå“è³ªç›£æ§æ©Ÿåˆ¶

### 6.3 ä¸‹ç« é å‘Š

ç¬¬5ç« å°‡æ·±å…¥æ¢è¨ RAG ç³»çµ±çš„è©•ä¼°ç†è«–èˆ‡ç›£æ§é«”ç³»ï¼Œé‡é»åˆ†æå¦‚ä½•ç§‘å­¸åœ°æ¸¬é‡å’ŒæŒçºŒæ”¹é€²ç³»çµ±æ€§èƒ½ï¼Œé€™æ˜¯ç¢ºä¿ä¼æ¥­ç´š RAG ç³»çµ±é•·æœŸæˆåŠŸçš„é—œéµã€‚

---

**èª²ç¨‹è©•ä¼°**: æœ¬ç« å…§å®¹åœ¨æœŸæœ«è€ƒè©¦ä¸­å 20%æ¬Šé‡ï¼Œé‡é»è€ƒæŸ¥ LLM æœå‹™æ¶æ§‹å’Œå“è³ªæ§åˆ¶èƒ½åŠ›ã€‚

**é …ç›®è¦æ±‚**: å­¸ç”Ÿéœ€å¯¦ç¾ä¸€å€‹å®Œæ•´çš„ LLM æœå‹™ç³»çµ±ï¼ŒåŒ…æ‹¬è² è¼‰å¹³è¡¡ã€å¼•ç”¨ç”Ÿæˆå’Œäº‹å¯¦æª¢æŸ¥åŠŸèƒ½ã€‚