# å‘é‡è³‡æ–™åº«ç†è«–èˆ‡æ··åˆæª¢ç´¢å¯¦ç¾
## å¤§å­¸æ•™ç§‘æ›¸ ç¬¬2ç« ï¼šé«˜ç¶­å‘é‡ç©ºé–“ä¸­çš„ç›¸ä¼¼æ€§æª¢ç´¢

**èª²ç¨‹ç·¨è™Ÿ**: CS785 - ä¼æ¥­ç´šæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±
**ç« ç¯€**: ç¬¬2ç«  ç´¢å¼•èˆ‡å‘é‡è³‡æ–™åº«
**å­¸ç¿’æ™‚æ•¸**: 8å°æ™‚
**å…ˆä¿®èª²ç¨‹**: ç·šæ€§ä»£æ•¸, æ¼”ç®—æ³•åˆ†æ, ç¬¬0-1ç« 
**ä½œè€…**: æª¢ç´¢ç³»çµ±ç ”ç©¶åœ˜éšŠ
**æœ€å¾Œæ›´æ–°**: 2025-01-06

---

## ğŸ“š å­¸ç¿’ç›®æ¨™ (Learning Objectives)

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œå­¸ç”Ÿæ‡‰èƒ½å¤ :

1. **ç†è«–åŸºç¤**: æŒæ¡é«˜ç¶­å‘é‡ç©ºé–“æª¢ç´¢çš„æ•¸å­¸åŸç†å’Œè¤‡é›œåº¦åˆ†æ
2. **ç³»çµ±æ¶æ§‹**: è¨­è¨ˆä¼æ¥­ç´šå‘é‡è³‡æ–™åº«é›†ç¾¤å’Œæ··åˆæª¢ç´¢ç³»çµ±
3. **æ¼”ç®—æ³•å¯¦ç¾**: å¯¦ç¾ HNSWã€IVF-PQ ç­‰å…ˆé€²ç´¢å¼•æ¼”ç®—æ³•
4. **æ€§èƒ½å„ªåŒ–**: åˆ†æå’Œå„ªåŒ–å¤§è¦æ¨¡å‘é‡æª¢ç´¢çš„æ€§èƒ½ç“¶é ¸

---

## 1. å‘é‡æª¢ç´¢çš„ç†è«–åŸºç¤

### 1.1 é«˜ç¶­ç©ºé–“çš„è©›å’’èˆ‡è¿‘ä¼¼è§£æ³•

#### **ç¶­åº¦ç½é›£çš„æ•¸å­¸åˆ†æ**

**å®šç† 1.1** (Bellman's Curse of Dimensionality): åœ¨é«˜ç¶­æ­å¹¾é‡Œå¾—ç©ºé–“ $\mathbb{R}^d$ ä¸­ï¼Œç•¶ $d \to \infty$ æ™‚ï¼Œä»»æ„å…©é»é–“è·é›¢çš„ç›¸å°å·®ç•°è¶¨æ–¼é›¶ï¼š

$$\lim_{d \to \infty} \frac{\text{dist}_{\max} - \text{dist}_{\min}}{\text{dist}_{\min}} = 0$$

**è­‰æ˜è¦é»**: åŸºæ–¼ä¸­å¿ƒæ¥µé™å®šç†ï¼Œé«˜ç¶­éš¨æ©Ÿå‘é‡çš„æ­æ°è·é›¢æ”¶æ–‚åˆ°å¸¸æ•¸ (Beyer et al., 1999)[^19]ã€‚

**å¯¦å‹™å½±éŸ¿**: å‚³çµ±çš„ç²¾ç¢º k-NN æœç´¢åœ¨é«˜ç¶­ç©ºé–“ä¸­å¤±æ•ˆï¼Œå¿…é ˆæ¡ç”¨è¿‘ä¼¼æ¼”ç®—æ³•ã€‚

#### **è¿‘ä¼¼æœ€è¿‘é„° (ANN) çš„ç†è«–ä¿è­‰**

**å®šç¾© 1.1** ($(1+\epsilon)$-è¿‘ä¼¼æœ€è¿‘é„°): å°æ–¼æŸ¥è©¢é» $q$ å’Œè³‡æ–™é›† $P$ï¼Œæ¼”ç®—æ³•è¿”å›é» $p'$ æ»¿è¶³ï¼š

$$d(q, p') \leq (1+\epsilon) \cdot d(q, p^*)$$

å…¶ä¸­ $p^*$ ç‚ºçœŸå¯¦æœ€è¿‘é„°ã€‚

**å®šç† 1.2** (Johnson-Lindenstrauss å¼•ç†): é«˜ç¶­é»é›†å¯ä»¥éš¨æ©ŸæŠ•å½±åˆ°è¼ƒä½ç¶­åº¦ï¼ŒåŒæ™‚ä¿æŒè·é›¢çš„ç›¸å°é—œä¿‚ï¼š

å°æ–¼ $n$ å€‹é»ï¼Œå­˜åœ¨æŠ•å½± $f: \mathbb{R}^d \to \mathbb{R}^k$ï¼Œå…¶ä¸­ $k = O(\log n / \epsilon^2)$ï¼Œä½¿å¾—ï¼š

$$(1-\epsilon)||u-v||^2 \leq ||f(u)-f(v)||^2 \leq (1+\epsilon)||u-v||^2$$

### 1.2 åµŒå…¥ç©ºé–“çš„èªç¾©å¹¾ä½•å­¸

#### **èªç¾©ç›¸ä¼¼æ€§çš„åº¦é‡ç†è«–**

**å®šç¾© 1.2** (èªç¾©åµŒå…¥ç©ºé–“): èªç¾©åµŒå…¥å‡½æ•¸ $E: \mathcal{V} \to \mathbb{R}^d$ å°‡è©å½™ç©ºé–“ $\mathcal{V}$ æ˜ å°„åˆ° $d$ ç¶­å‘é‡ç©ºé–“ï¼Œä¿æŒèªç¾©é—œä¿‚ï¼š

$$\text{Semantic-Sim}(w_1, w_2) \approx \text{Cosine-Sim}(E(w_1), E(w_2))$$

**æ€§è³ª 1.1** (åµŒå…¥ç©ºé–“çš„ä¸‰è§’ä¸ç­‰å¼): å°æ–¼èªç¾©ç›¸é—œçš„æ¦‚å¿µ $a, b, c$ï¼š

$$\text{Sim}(a,c) \geq \text{Sim}(a,b) + \text{Sim}(b,c) - 1$$

#### **MTEB åŸºæº–æ¸¬è©¦çš„ç†è«–æ„ç¾©**

åŸºæ–¼ Muennighoff et al. (2022)[^20] çš„ Massive Text Embedding Benchmark (MTEB)ï¼ŒåµŒå…¥æ¨¡å‹çš„è©•ä¼°åŒ…å«å…«å€‹ç¶­åº¦ï¼š

```python
from typing import Dict, List, Tuple
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class MTEBEvaluationFramework:
    """MTEB è©•ä¼°æ¡†æ¶å¯¦ç¾"""

    def __init__(self):
        self.task_categories = {
            "classification": self._evaluate_classification,
            "clustering": self._evaluate_clustering,
            "pair_classification": self._evaluate_pair_classification,
            "reranking": self._evaluate_reranking,
            "retrieval": self._evaluate_retrieval,
            "sts": self._evaluate_semantic_similarity,
            "summarization": self._evaluate_summarization,
            "bitextmining": self._evaluate_bitext_mining
        }

    async def comprehensive_embedding_evaluation(self,
                                               embedding_model: Any,
                                               test_datasets: Dict) -> Dict:
        """ç¶œåˆåµŒå…¥æ¨¡å‹è©•ä¼°"""

        results = {}

        for category, datasets in test_datasets.items():
            if category in self.task_categories:
                evaluator = self.task_categories[category]
                category_result = await evaluator(embedding_model, datasets)
                results[category] = category_result

        # è¨ˆç®— MTEB ç¸½åˆ†
        mteb_score = self._calculate_mteb_score(results)

        return {
            "mteb_score": mteb_score,
            "category_results": results,
            "model_ranking": self._rank_model_performance(mteb_score),
            "strengths": self._identify_model_strengths(results),
            "weaknesses": self._identify_model_weaknesses(results)
        }

    async def _evaluate_retrieval(self, model: Any, datasets: List) -> Dict:
        """è©•ä¼°æª¢ç´¢ä»»å‹™æ€§èƒ½"""

        total_ndcg_10 = 0
        total_map = 0
        total_recall_100 = 0

        for dataset in datasets:
            queries = dataset["queries"]
            corpus = dataset["corpus"]
            qrels = dataset["qrels"]  # ç›¸é—œæ€§æ¨™è¨»

            # ç·¨ç¢¼æŸ¥è©¢å’Œæ–‡æª”
            query_embeddings = model.encode([q["text"] for q in queries])
            doc_embeddings = model.encode([doc["text"] for doc in corpus])

            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = cosine_similarity(query_embeddings, doc_embeddings)

            # è©•ä¼°æŒ‡æ¨™
            ndcg_10 = self._calculate_ndcg(similarity_matrix, qrels, k=10)
            map_score = self._calculate_map(similarity_matrix, qrels)
            recall_100 = self._calculate_recall(similarity_matrix, qrels, k=100)

            total_ndcg_10 += ndcg_10
            total_map += map_score
            total_recall_100 += recall_100

        num_datasets = len(datasets)
        return {
            "ndcg@10": total_ndcg_10 / num_datasets,
            "map": total_map / num_datasets,
            "recall@100": total_recall_100 / num_datasets
        }

    def _calculate_ndcg(self, similarity_matrix: np.ndarray,
                       qrels: Dict, k: int = 10) -> float:
        """è¨ˆç®— NDCG@k åˆ†æ•¸"""

        total_ndcg = 0
        num_queries = len(qrels)

        for query_idx, query_id in enumerate(qrels.keys()):
            # ç²å–è©²æŸ¥è©¢çš„ç›¸é—œæ–‡æª”
            relevant_docs = qrels[query_id]

            # æŒ‰ç›¸ä¼¼åº¦æ’åºæ–‡æª”
            query_similarities = similarity_matrix[query_idx]
            ranked_indices = np.argsort(query_similarities)[::-1]

            # è¨ˆç®— DCG@k
            dcg = 0
            for i in range(min(k, len(ranked_indices))):
                doc_idx = ranked_indices[i]
                relevance = relevant_docs.get(str(doc_idx), 0)
                dcg += relevance / np.log2(i + 2)  # i+2 å› ç‚ºç´¢å¼•å¾0é–‹å§‹

            # è¨ˆç®— IDCG@k
            ideal_relevances = sorted(relevant_docs.values(), reverse=True)
            idcg = 0
            for i in range(min(k, len(ideal_relevances))):
                idcg += ideal_relevances[i] / np.log2(i + 2)

            # NDCG = DCG / IDCG
            if idcg > 0:
                total_ndcg += dcg / idcg

        return total_ndcg / num_queries

    def _calculate_mteb_score(self, results: Dict) -> float:
        """è¨ˆç®— MTEB ç¸½åˆ†"""

        # MTEB æ¬Šé‡é…ç½® (åŸºæ–¼ä»»å‹™é‡è¦æ€§)
        weights = {
            "retrieval": 0.25,
            "reranking": 0.20,
            "classification": 0.15,
            "clustering": 0.15,
            "sts": 0.10,
            "pair_classification": 0.10,
            "summarization": 0.03,
            "bitextmining": 0.02
        }

        weighted_score = 0
        total_weight = 0

        for category, result in results.items():
            if category in weights:
                category_score = self._extract_primary_metric(result)
                weighted_score += weights[category] * category_score
                total_weight += weights[category]

        return weighted_score / total_weight if total_weight > 0 else 0
```

---

## 2. å‘é‡è³‡æ–™åº«ç³»çµ±æ¶æ§‹

### 2.1 Qdrant æ·±åº¦æŠ€è¡“åˆ†æ

#### **Qdrant çš„æ¶æ§‹å„ªå‹¢**

Qdrant (Qdrant Team, 2021)[^21] æ¡ç”¨ Rust å¯¦ç¾çš„é«˜æ€§èƒ½å‘é‡è³‡æ–™åº«ï¼Œå…¶æ ¸å¿ƒå„ªå‹¢ï¼š

**æŠ€è¡“ç‰¹é» 2.1** (Qdrant vs ç«¶å“åˆ†æ):

| ç‰¹æ€§ | Qdrant | Pinecone | Weaviate | Chroma |
|------|--------|----------|----------|--------|
| **èªè¨€** | Rust | Python/C++ | Go | Python |
| **æ€§èƒ½** | æ¥µé«˜ | é«˜ | ä¸­é«˜ | ä¸­ |
| **æœ¬åœ°éƒ¨ç½²** | âœ… | âŒ | âœ… | âœ… |
| **é›†ç¾¤æ”¯æ´** | âœ… | âœ… | âœ… | æœ‰é™ |
| **å¤šå‘é‡** | âœ… | âŒ | âŒ | âŒ |
| **éæ¿¾æ€§èƒ½** | å„ªç§€ | è‰¯å¥½ | è‰¯å¥½ | åŸºç¤ |

#### **ç”Ÿç”¢ç´š Qdrant é›†ç¾¤è¨­è¨ˆ**

```python
from qdrant_client import QdrantClient, models
from qdrant_client.http.models import Distance, VectorParams, OptimizersConfigDiff
import asyncio
from typing import Dict, List, Optional, Any

class EnterpriseQdrantCluster:
    """ä¼æ¥­ç´š Qdrant é›†ç¾¤ç®¡ç†"""

    def __init__(self, cluster_config: Dict):
        self.cluster_nodes = cluster_config["nodes"]
        self.replication_factor = cluster_config.get("replication_factor", 2)
        self.shard_number = cluster_config.get("shard_number", 6)

        # åˆå§‹åŒ–é›†ç¾¤å®¢æˆ¶ç«¯
        self.clients = {}
        for node_name, node_config in self.cluster_nodes.items():
            self.clients[node_name] = QdrantClient(
                host=node_config["host"],
                port=node_config["port"],
                prefer_grpc=True,
                timeout=30.0
            )

        self.primary_client = list(self.clients.values())[0]

    async def create_production_collection(self, collection_name: str,
                                         vector_config: Dict) -> Dict:
        """å‰µå»ºç”Ÿç”¢ç´šå‘é‡é›†åˆ"""

        # å„ªåŒ–çš„å‘é‡é…ç½®
        vectors_config = {}

        for vector_name, config in vector_config.items():
            vectors_config[vector_name] = VectorParams(
                size=config["size"],
                distance=Distance.COSINE,  # ä¼æ¥­å ´æ™¯æ¨è–¦é¤˜å¼¦è·é›¢
                hnsw_config=models.HnswConfigDiff(
                    m=config.get("hnsw_m", 64),              # é€£æ¥æ•¸
                    ef_construct=config.get("ef_construct", 256),  # å»ºæ§‹å“è³ª
                    full_scan_threshold=config.get("threshold", 10000),
                    max_indexing_threads=config.get("threads", 8),
                    on_disk=config.get("on_disk", True)  # å¤§è¦æ¨¡ç´¢å¼•å­˜å„²
                )
            )

        # å‰µå»ºé›†åˆ
        try:
            await self.primary_client.create_collection(
                collection_name=collection_name,
                vectors_config=vectors_config,

                # åˆ†ç‰‡é…ç½®
                shard_number=self.shard_number,
                replication_factor=self.replication_factor,

                # æ€§èƒ½å„ªåŒ–
                optimizers_config=OptimizersConfigDiff(
                    deleted_threshold=0.2,
                    vacuum_min_vector_number=1000,
                    default_segment_number=8,
                    max_segment_size=200000,           # 200K å‘é‡per segment
                    memmap_threshold=50000,
                    indexing_threshold=50000,
                    flush_interval_sec=30,
                    max_optimization_threads=8
                ),

                # å¯«å…¥ä¸€è‡´æ€§
                write_consistency_factor=1
            )

            collection_info = await self.primary_client.get_collection(collection_name)

            return {
                "success": True,
                "collection_name": collection_name,
                "vectors_count": collection_info.vectors_count,
                "config": collection_info.config.__dict__,
                "status": collection_info.status
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "collection_name": collection_name
            }

    async def bulk_upsert_optimized(self, collection_name: str,
                                  points: List[Dict],
                                  batch_size: int = 100) -> Dict:
        """å„ªåŒ–çš„æ‰¹é‡æ’å…¥"""

        total_points = len(points)
        processed = 0
        errors = []

        # åˆ†æ‰¹è™•ç†
        for i in range(0, total_points, batch_size):
            batch = points[i:i + batch_size]

            try:
                # æº–å‚™ Qdrant é»æ ¼å¼
                qdrant_points = []
                for point in batch:
                    qdrant_point = models.PointStruct(
                        id=point["id"],
                        vector=point["vectors"],  # æ”¯æ´å¤šå‘é‡
                        payload=point["metadata"]
                    )
                    qdrant_points.append(qdrant_point)

                # ä¸¦è¡Œå¯«å…¥å¤šå€‹ç¯€é»
                upsert_tasks = []
                for client in self.clients.values():
                    task = client.upsert(
                        collection_name=collection_name,
                        points=qdrant_points,
                        wait=False  # ç•°æ­¥å¯«å…¥
                    )
                    upsert_tasks.append(task)

                # ç­‰å¾…æ‰€æœ‰å¯«å…¥å®Œæˆ
                await asyncio.gather(*upsert_tasks)
                processed += len(batch)

            except Exception as e:
                errors.append(f"Batch {i//batch_size}: {str(e)}")

        return {
            "total_points": total_points,
            "processed_points": processed,
            "success_rate": processed / total_points,
            "errors": errors
        }

    async def hybrid_search_with_filtering(self, collection_name: str,
                                         query_vectors: Dict[str, List[float]],
                                         filters: Dict,
                                         top_k: int = 50) -> List[Dict]:
        """å¸¶éæ¿¾çš„æ··åˆæœç´¢"""

        # æ§‹å»º Qdrant éæ¿¾æ¢ä»¶
        qdrant_filter = self._build_qdrant_filter(filters)

        search_results = []

        # å¤šå‘é‡æª¢ç´¢ (å¦‚æœé…ç½®äº†å¤šå€‹å‘é‡)
        for vector_name, vector in query_vectors.items():
            try:
                results = await self.primary_client.search(
                    collection_name=collection_name,
                    query_vector=(vector_name, vector),
                    query_filter=qdrant_filter,
                    limit=top_k,
                    with_payload=True,
                    with_vectors=False,  # ç¯€çœå¸¶å¯¬
                    score_threshold=0.3  # æœ€ä½ç›¸ä¼¼åº¦é–¾å€¼
                )

                # è½‰æ›æ ¼å¼
                for result in results:
                    search_results.append({
                        "id": result.id,
                        "score": result.score,
                        "payload": result.payload,
                        "vector_type": vector_name
                    })

            except Exception as e:
                print(f"Search failed for vector {vector_name}: {e}")

        # æŒ‰åˆ†æ•¸æ’åº
        search_results.sort(key=lambda x: x["score"], reverse=True)

        return search_results[:top_k]

    def _build_qdrant_filter(self, filters: Dict) -> models.Filter:
        """æ§‹å»º Qdrant æŸ¥è©¢éæ¿¾å™¨"""

        filter_conditions = []

        for field, condition in filters.items():
            if isinstance(condition, dict):
                if "eq" in condition:
                    filter_conditions.append(
                        models.FieldCondition(
                            key=field,
                            match=models.MatchValue(value=condition["eq"])
                        )
                    )
                elif "in" in condition:
                    filter_conditions.append(
                        models.FieldCondition(
                            key=field,
                            match=models.MatchAny(any=condition["in"])
                        )
                    )
                elif "range" in condition:
                    filter_conditions.append(
                        models.FieldCondition(
                            key=field,
                            range=models.Range(
                                gte=condition["range"].get("gte"),
                                lt=condition["range"].get("lt")
                            )
                        )
                    )

        if filter_conditions:
            return models.Filter(must=filter_conditions)

        return None
```

---

## 3. æ··åˆæª¢ç´¢çš„ç†è«–èˆ‡å¯¦ç¾

### 3.1 ç¨€ç–èˆ‡å¯†é›†æª¢ç´¢çš„æ•¸å­¸èåˆ

#### **BM25 èˆ‡å‘é‡æª¢ç´¢çš„ç†è«–æ¯”è¼ƒ**

**BM25 è©•åˆ†å‡½æ•¸**:
$$\text{BM25}(q,d) = \sum_{t \in q} \text{IDF}(t) \cdot \frac{tf(t,d) \cdot (k_1 + 1)}{tf(t,d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}$$

**å‘é‡æª¢ç´¢è©•åˆ†**:
$$\text{Vector-Score}(q,d) = \text{Cosine}(E(q), E(d)) = \frac{E(q) \cdot E(d)}{||E(q)|| \cdot ||E(d)||}$$

#### **æ··åˆæª¢ç´¢çš„æœ€å„ªèåˆç†è«–**

**å®šç† 3.1** (æª¢ç´¢æ–¹æ³•äº’è£œæ€§): ç¨€ç–æª¢ç´¢ (BM25) å’Œå¯†é›†æª¢ç´¢ (Vector) åœ¨ä¸åŒæŸ¥è©¢é¡å‹ä¸Šå‘ˆç¾äº’è£œæ€§èƒ½åˆ†ä½ˆï¼š

- **ç²¾ç¢ºåŒ¹é…**: BM25 > Vector (é—œéµè©ã€IDã€å°ˆæœ‰åè©)
- **èªç¾©ç†è§£**: Vector > BM25 (æ¦‚å¿µã€åŒç¾©è©ã€è·¨èªè¨€)

**èåˆç­–ç•¥**: ç·šæ€§çµ„åˆèˆ‡å€’æ•¸æ’åèåˆçš„æ¯”è¼ƒ

**ç·šæ€§èåˆ**:
$$\text{Score}_{\text{linear}}(q,d) = \alpha \cdot \text{BM25}(q,d) + \beta \cdot \text{Vector}(q,d)$$

**å€’æ•¸æ’åèåˆ (RRF)**:
$$\text{Score}_{\text{RRF}}(d) = \sum_{r \in \{\text{BM25}, \text{Vector}\}} \frac{1}{k + \text{rank}_r(d)}$$

#### **SPLADE: ç¨€ç–æª¢ç´¢çš„ç¥ç¶“åŒ–**

SPLADE (Formal et al., 2021)[^22] é€šéç¥ç¶“ç¶²çµ¡å­¸ç¿’ç¨€ç–è¡¨ç¤ºï¼š

**åŸç†**: ä½¿ç”¨ BERT-like æ¨¡å‹çš„è©å½™ç©ºé–“è¼¸å‡ºï¼š

$$\text{SPLADE}(x) = \text{ReLU}(\text{BERT}_{\text{vocab}}(x))$$

**å„ªå‹¢**: çµåˆäº†ç¨€ç–æª¢ç´¢çš„æ•ˆç‡å’Œå¯†é›†æª¢ç´¢çš„èªç¾©ç†è§£ã€‚

```python
from transformers import AutoTokenizer, AutoModelForMaskedLM
import torch
from collections import defaultdict

class SPLADERetriever:
    """SPLADE ç¨€ç–æª¢ç´¢å¯¦ç¾"""

    def __init__(self, model_name: str = "naver/splade-cocondenser-ensembledistil"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()

        # å€’æ’ç´¢å¼•
        self.inverted_index = defaultdict(list)

    def encode_text(self, text: str) -> Dict[str, float]:
        """ç·¨ç¢¼æ–‡æœ¬ç‚º SPLADE ç¨€ç–å‘é‡"""

        # åˆ†è©å’Œç·¨ç¢¼
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)

        with torch.no_grad():
            # å‰å‘å‚³æ’­
            outputs = self.model(**inputs)
            logits = outputs.logits

            # ReLU æ¿€æ´»ç²å¾—ç¨€ç–æ€§
            sparse_scores = torch.relu(logits).squeeze()

            # ç²å–è©å½™é‡è¦æ€§åˆ†æ•¸
            vocab_scores = torch.max(sparse_scores, dim=0)[0]

        # è½‰æ›ç‚ºç¨€ç–å­—å…¸è¡¨ç¤º
        sparse_dict = {}
        for token_id, score in enumerate(vocab_scores):
            if score > 0.1:  # ç¨€ç–æ€§é–¾å€¼
                token = self.tokenizer.decode([token_id])
                if token.strip() and not token.startswith('['):
                    sparse_dict[token] = float(score)

        return sparse_dict

    async def build_inverted_index(self, documents: List[Dict]):
        """æ§‹å»º SPLADE å€’æ’ç´¢å¼•"""

        print(f"Building SPLADE index for {len(documents)} documents...")

        for i, doc in enumerate(documents):
            if i % 1000 == 0:
                print(f"Processed {i}/{len(documents)} documents")

            # ç²å–æ–‡æª”çš„ SPLADE è¡¨ç¤º
            sparse_repr = self.encode_text(doc["content"])

            # æ›´æ–°å€’æ’ç´¢å¼•
            for term, weight in sparse_repr.items():
                self.inverted_index[term].append({
                    "doc_id": doc["id"],
                    "weight": weight,
                    "content_preview": doc["content"][:200]
                })

        # æŒ‰æ¬Šé‡æ’åºæ¯å€‹è©é …çš„æ–‡æª”åˆ—è¡¨
        for term in self.inverted_index:
            self.inverted_index[term].sort(key=lambda x: x["weight"], reverse=True)

        print(f"SPLADE index built: {len(self.inverted_index)} unique terms")

    def search(self, query: str, top_k: int = 50) -> List[Dict]:
        """SPLADE æª¢ç´¢"""

        # ç²å–æŸ¥è©¢çš„ SPLADE è¡¨ç¤º
        query_sparse = self.encode_text(query)

        # è¨ˆç®—æ–‡æª”åˆ†æ•¸
        doc_scores = defaultdict(float)

        for term, query_weight in query_sparse.items():
            if term in self.inverted_index:
                for posting in self.inverted_index[term]:
                    doc_scores[posting["doc_id"]] += query_weight * posting["weight"]

        # æ’åºä¸¦è¿”å›
        ranked_docs = sorted(
            doc_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]

        results = []
        for doc_id, score in ranked_docs:
            results.append({
                "doc_id": doc_id,
                "score": score,
                "method": "splade"
            })

        return results
```

### 3.2 HNSW æ¼”ç®—æ³•çš„ç†è«–åˆ†æ

#### **Hierarchical Navigable Small World åŸç†**

HNSW (Malkov & Yashunin, 2018)[^5] åŸºæ–¼å°ä¸–ç•Œç¶²çµ¡ç†è«–æ§‹å»ºéšå±¤å¼å°èˆªåœ–ï¼š

**æ•¸å­¸æ¨¡å‹**: HNSW åœ–å¯è¡¨ç¤ºç‚º $G = (V, E_0 \cup E_1 \cup ... \cup E_L)$ï¼Œå…¶ä¸­ï¼š
- $V$: ç¯€é»é›†åˆ (å‘é‡é»)
- $E_l$: ç¬¬ $l$ å±¤çš„é‚Šé›†åˆ
- $L$: æœ€å¤§å±¤æ•¸

**å±¤ç´šåˆ†é…**: ç¯€é» $v$ çš„å±¤ç´š $l_v$ æŒ‰æŒ‡æ•¸åˆ†ä½ˆéš¨æ©Ÿåˆ†é…ï¼š

$$P(l_v = l) = \frac{1}{m_L} \cdot \left(\frac{1}{m_L}\right)^l$$

å…¶ä¸­ $m_L$ ç‚ºå±¤ç´šå› å­ (é€šå¸¸å– 1/ln(2))ã€‚

#### **æœç´¢è¤‡é›œåº¦åˆ†æ**

**å®šç† 3.2** (HNSW æœç´¢è¤‡é›œåº¦): HNSW çš„æœç´¢æ™‚é–“è¤‡é›œåº¦ç‚ºï¼š

$$O(\log n \cdot \log \log n)$$

å…¶ä¸­ $n$ ç‚ºæ•¸æ“šé»æ•¸é‡ã€‚

**è­‰æ˜æ€è·¯**: éšå±¤çµæ§‹å°‡æœç´¢åˆ†è§£ç‚º $O(\log n)$ å±¤ï¼Œæ¯å±¤éœ€è¦ $O(\log \log n)$ çš„å°èˆªæ™‚é–“ã€‚â–¡

#### **ä¼æ¥­ç´š HNSW åƒæ•¸èª¿å„ª**

```python
class HNSWParameterOptimizer:
    """HNSW åƒæ•¸å„ªåŒ–å™¨"""

    def __init__(self):
        self.parameter_ranges = {
            "M": [16, 32, 48, 64],                    # é€£æ¥æ•¸
            "ef_construction": [100, 200, 400, 800],   # æ§‹å»ºæ™‚æœç´¢å¯¬åº¦
            "ef_search": [50, 100, 200, 400],         # æœç´¢æ™‚æœç´¢å¯¬åº¦
            "max_m": [16, 32, 48, 64],                # æœ€å¤§é€£æ¥æ•¸
            "max_m0": [32, 64, 96, 128]               # ç¬¬0å±¤æœ€å¤§é€£æ¥æ•¸
        }

    async def optimize_parameters(self, training_queries: List[Dict],
                                ground_truth: List[Dict],
                                vector_data: List[np.ndarray]) -> Dict:
        """å„ªåŒ– HNSW åƒæ•¸"""

        best_params = None
        best_score = 0.0
        optimization_results = []

        # ç¶²æ ¼æœç´¢æœ€å„ªåƒæ•¸
        from itertools import product

        param_combinations = list(product(*self.parameter_ranges.values()))

        for i, params in enumerate(param_combinations[:20]):  # é™åˆ¶æœç´¢ç©ºé–“
            param_dict = dict(zip(self.parameter_ranges.keys(), params))

            print(f"Testing parameter combination {i+1}/20: {param_dict}")

            # æ§‹å»º HNSW ç´¢å¼•
            index_result = await self._build_test_index(vector_data, param_dict)

            # è©•ä¼°æ€§èƒ½
            performance = await self._evaluate_performance(
                index_result["index"],
                training_queries,
                ground_truth
            )

            optimization_results.append({
                "parameters": param_dict,
                "performance": performance,
                "build_time": index_result["build_time"]
            })

            # ç¶œåˆè©•åˆ† (å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦)
            composite_score = (
                0.7 * performance["recall@10"] +
                0.2 * performance["search_speed"] +
                0.1 * performance["memory_efficiency"]
            )

            if composite_score > best_score:
                best_score = composite_score
                best_params = param_dict

        return {
            "best_parameters": best_params,
            "best_score": best_score,
            "optimization_results": optimization_results,
            "recommendations": self._generate_parameter_recommendations(best_params)
        }

    async def _build_test_index(self, vectors: List[np.ndarray],
                               params: Dict) -> Dict:
        """æ§‹å»ºæ¸¬è©¦ç´¢å¼•"""

        import faiss
        import time

        # æº–å‚™æ•¸æ“š
        vector_matrix = np.array(vectors).astype('float32')
        dimension = vector_matrix.shape[1]

        # å‰µå»º HNSW ç´¢å¼•
        index = faiss.IndexHNSWFlat(dimension, params["M"])
        index.hnsw.efConstruction = params["ef_construction"]
        index.hnsw.efSearch = params["ef_search"]

        # è¨ˆæ™‚æ§‹å»º
        start_time = time.time()
        index.add(vector_matrix)
        build_time = time.time() - start_time

        return {
            "index": index,
            "build_time": build_time,
            "index_size_mb": index.sa_code_size() / (1024 * 1024)
        }

    async def _evaluate_performance(self, index: Any,
                                  queries: List[Dict],
                                  ground_truth: List[Dict]) -> Dict:
        """è©•ä¼°ç´¢å¼•æ€§èƒ½"""

        import time

        total_recall_10 = 0
        total_search_time = 0
        num_queries = len(queries)

        for i, query in enumerate(queries):
            query_vector = np.array([query["vector"]]).astype('float32')

            # æ¸¬é‡æœç´¢æ™‚é–“
            start_time = time.time()
            distances, indices = index.search(query_vector, 10)
            search_time = time.time() - start_time

            total_search_time += search_time

            # è¨ˆç®—å¬å›ç‡
            retrieved_ids = set(indices[0])
            relevant_ids = set(ground_truth[i]["relevant_docs"])

            recall_10 = len(retrieved_ids & relevant_ids) / len(relevant_ids)
            total_recall_10 += recall_10

        return {
            "recall@10": total_recall_10 / num_queries,
            "avg_search_time_ms": (total_search_time / num_queries) * 1000,
            "search_speed": 1.0 / (total_search_time / num_queries),  # QPS
            "memory_efficiency": 1.0 - (index.sa_code_size() / (len(queries) * 1024))
        }
```

---

## 4. æ··åˆæª¢ç´¢èåˆç­–ç•¥

### 4.1 å€’æ•¸æ’åèåˆ (RRF) çš„æ·±åº¦åˆ†æ

#### **RRF çš„ç†è«–å„ªå‹¢**

**å®šç† 4.1** (RRF çš„ç„¡åæ€§): RRF èåˆç­–ç•¥å°æ–¼ä¸åŒæª¢ç´¢ç³»çµ±çš„è©•åˆ†å°ºåº¦å…·æœ‰å¤©ç„¶çš„ç„¡åæ€§ï¼š

$$\mathbb{E}[\text{RRF-Bias}] = 0$$

**è­‰æ˜**: RRF åƒ…ä¾è³´æ’åè€ŒéåŸå§‹åˆ†æ•¸ï¼Œå› æ­¤ä¸å—è©•åˆ†åˆ†ä½ˆå½±éŸ¿ã€‚â–¡

#### **é«˜ç´š RRF è®Šé«”å¯¦ç¾**

```python
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

@dataclass
class SearchResult:
    """æª¢ç´¢çµæœæ•¸æ“šçµæ§‹"""
    doc_id: str
    score: float
    method: str
    content: str
    metadata: Dict

class AdvancedRRFFusion:
    """é«˜ç´š RRF èåˆç­–ç•¥"""

    def __init__(self):
        # ä¸åŒæŸ¥è©¢é¡å‹çš„æœ€å„ªåƒæ•¸
        self.query_type_params = {
            "factual": {"k": 60, "weights": {"bm25": 0.6, "vector": 0.4}},
            "conceptual": {"k": 80, "weights": {"bm25": 0.3, "vector": 0.7}},
            "mixed": {"k": 70, "weights": {"bm25": 0.5, "vector": 0.5}}
        }

    async def adaptive_rrf_fusion(self, retrieval_results: Dict[str, List[SearchResult]],
                                query: str, query_type: str = "auto") -> List[SearchResult]:
        """è‡ªé©æ‡‰ RRF èåˆ"""

        # è‡ªå‹•æª¢æ¸¬æŸ¥è©¢é¡å‹
        if query_type == "auto":
            query_type = await self._classify_query_type(query)

        # ç²å–å°æ‡‰åƒæ•¸
        params = self.query_type_params.get(query_type, self.query_type_params["mixed"])
        k = params["k"]
        weights = params["weights"]

        # æ¨™æº–åŒ–æ¯å€‹æª¢ç´¢æ–¹æ³•çš„çµæœ
        normalized_results = {}
        for method, results in retrieval_results.items():
            normalized_results[method] = self._normalize_scores(results)

        # åŠ æ¬Š RRF èåˆ
        fused_scores = defaultdict(float)
        doc_details = {}

        for method, results in normalized_results.items():
            method_weight = weights.get(method, 1.0)

            for rank, result in enumerate(results):
                # åŠ æ¬Š RRF åˆ†æ•¸è¨ˆç®—
                rrf_score = method_weight / (k + rank + 1)
                fused_scores[result.doc_id] += rrf_score

                # ä¿å­˜æ–‡æª”è©³æƒ…
                if (result.doc_id not in doc_details or
                    fused_scores[result.doc_id] > doc_details[result.doc_id].score):
                    doc_details[result.doc_id] = SearchResult(
                        doc_id=result.doc_id,
                        score=fused_scores[result.doc_id],
                        method=f"rrf_{method}",
                        content=result.content,
                        metadata=result.metadata
                    )

        # æ’åºä¸¦è¿”å›èåˆçµæœ
        final_results = sorted(
            doc_details.values(),
            key=lambda x: x.score,
            reverse=True
        )

        return final_results

    def _normalize_scores(self, results: List[SearchResult]) -> List[SearchResult]:
        """æ¨™æº–åŒ–æª¢ç´¢åˆ†æ•¸"""

        if not results:
            return []

        scores = [r.score for r in results]
        min_score = min(scores)
        max_score = max(scores)
        score_range = max_score - min_score

        if score_range == 0:
            return results

        normalized_results = []
        for result in results:
            normalized_score = (result.score - min_score) / score_range
            normalized_result = SearchResult(
                doc_id=result.doc_id,
                score=normalized_score,
                method=result.method,
                content=result.content,
                metadata=result.metadata
            )
            normalized_results.append(normalized_result)

        return normalized_results

    async def _classify_query_type(self, query: str) -> str:
        """è‡ªå‹•åˆ†é¡æŸ¥è©¢é¡å‹"""

        query_lower = query.lower()

        # äº‹å¯¦æ€§æŸ¥è©¢æŒ‡æ¨™
        factual_indicators = ["what is", "when did", "where is", "who is", "how many"]
        if any(indicator in query_lower for indicator in factual_indicators):
            return "factual"

        # æ¦‚å¿µæ€§æŸ¥è©¢æŒ‡æ¨™
        conceptual_indicators = ["explain", "describe", "compare", "analyze", "understand"]
        if any(indicator in query_lower for indicator in conceptual_indicators):
            return "conceptual"

        return "mixed"

    async def evaluate_fusion_strategy(self, test_queries: List[Dict],
                                     retrieval_systems: Dict) -> Dict:
        """è©•ä¼°èåˆç­–ç•¥æ•ˆæœ"""

        strategies = ["linear", "rrf", "adaptive_rrf"]
        strategy_results = {}

        for strategy in strategies:
            strategy_performance = await self._test_fusion_strategy(
                strategy, test_queries, retrieval_systems
            )
            strategy_results[strategy] = strategy_performance

        # æ¯”è¼ƒåˆ†æ
        best_strategy = max(
            strategy_results.keys(),
            key=lambda s: strategy_results[s]["overall_score"]
        )

        return {
            "strategy_comparison": strategy_results,
            "best_strategy": best_strategy,
            "performance_gains": self._calculate_performance_gains(strategy_results),
            "recommendations": self._generate_fusion_recommendations(strategy_results)
        }
```

---

## 5. é‡æ’åºç³»çµ±çš„ç†è«–èˆ‡å¯¦è¸

### 5.1 Cross-Encoder çš„ç†è«–åŸºç¤

#### **é›™å¡” vs å–®å¡”æ¶æ§‹æ¯”è¼ƒ**

**é›™å¡”æ¶æ§‹ (Bi-Encoder)**:
$$\text{Score}(q,d) = \text{Sim}(E_q(q), E_d(d))$$

**å–®å¡”æ¶æ§‹ (Cross-Encoder)**:
$$\text{Score}(q,d) = \text{CrossEncoder}(q \oplus d)$$

å…¶ä¸­ $\oplus$ è¡¨ç¤ºæ–‡æœ¬æ‹¼æ¥ã€‚

**å®šç† 5.1** (Cross-Encoder è¡¨é”èƒ½åŠ›å„ªå‹¢): Cross-Encoder èƒ½å¤ å­¸ç¿’æŸ¥è©¢-æ–‡æª”é–“çš„è¤‡é›œäº¤äº’æ¨¡å¼ï¼Œå…¶è¡¨é”èƒ½åŠ›åš´æ ¼å„ªæ–¼é›™å¡”æ¶æ§‹ã€‚

**å¯¦è­‰è­‰æ“š**: Khattab et al. (2021) åœ¨å¤šå€‹åŸºæº–æ¸¬è©¦ä¸­è­‰æ˜ Cross-Encoder ç›¸è¼ƒæ–¼é›™å¡”æ¨¡å‹å¹³å‡æå‡ 10-20% nDCG@10ã€‚

#### **ç”Ÿç”¢ç´šé‡æ’åºç³»çµ±**

```python
from sentence_transformers import CrossEncoder
import torch
from typing import List, Dict, Tuple
import asyncio

class ProductionReranker:
    """ç”Ÿç”¢ç´šé‡æ’åºç³»çµ±"""

    def __init__(self, model_name: str = "BAAI/bge-reranker-large"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.reranker = CrossEncoder(
            model_name,
            max_length=512,
            device=self.device
        )

        # æ€§èƒ½é…ç½®
        self.batch_size = 16
        self.max_candidates = 200
        self.score_threshold = 0.1

    async def rerank_with_quality_control(self, query: str,
                                        candidates: List[SearchResult],
                                        top_k: int = 20) -> List[SearchResult]:
        """å¸¶å“è³ªæ§åˆ¶çš„é‡æ’åº"""

        if len(candidates) <= top_k:
            return candidates

        # é™åˆ¶å€™é¸æ•¸é‡ä»¥æ§åˆ¶å»¶é²
        limited_candidates = candidates[:self.max_candidates]

        # æº–å‚™æŸ¥è©¢-æ–‡æª”å°
        query_doc_pairs = [
            (query, candidate.content[:512])  # é™åˆ¶è¼¸å…¥é•·åº¦
            for candidate in limited_candidates
        ]

        # æ‰¹é‡é‡æ’åº
        rerank_scores = await self._batch_rerank(query_doc_pairs)

        # éæ¿¾ä½åˆ†çµæœ
        filtered_results = []
        for candidate, score in zip(limited_candidates, rerank_scores):
            if score > self.score_threshold:
                candidate.score = float(score)
                filtered_results.append(candidate)

        # æ’åºä¸¦è¿”å›
        reranked_results = sorted(
            filtered_results,
            key=lambda x: x.score,
            reverse=True
        )[:top_k]

        return reranked_results

    async def _batch_rerank(self, query_doc_pairs: List[Tuple[str, str]]) -> List[float]:
        """æ‰¹é‡é‡æ’åºè™•ç†"""

        all_scores = []

        # åˆ†æ‰¹è™•ç†ä»¥æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨
        for i in range(0, len(query_doc_pairs), self.batch_size):
            batch_pairs = query_doc_pairs[i:i + self.batch_size]

            # ä½¿ç”¨ Cross-Encoder è©•åˆ†
            with torch.no_grad():
                batch_scores = self.reranker.predict(batch_pairs)
                all_scores.extend(batch_scores.tolist())

        return all_scores

    async def evaluate_reranking_impact(self, test_dataset: List[Dict]) -> Dict:
        """è©•ä¼°é‡æ’åºæ•ˆæœ"""

        before_rerank_metrics = []
        after_rerank_metrics = []

        for test_case in test_dataset:
            query = test_case["query"]
            initial_results = test_case["retrieval_results"]
            ground_truth = test_case["relevant_docs"]

            # é‡æ’åºå‰çš„æ€§èƒ½
            before_metrics = self._calculate_ranking_metrics(
                initial_results, ground_truth
            )
            before_rerank_metrics.append(before_metrics)

            # åŸ·è¡Œé‡æ’åº
            reranked_results = await self.rerank_with_quality_control(
                query, initial_results, top_k=20
            )

            # é‡æ’åºå¾Œçš„æ€§èƒ½
            after_metrics = self._calculate_ranking_metrics(
                reranked_results, ground_truth
            )
            after_rerank_metrics.append(after_metrics)

        # è¨ˆç®—æ”¹é€²ç¨‹åº¦
        improvement = {}
        for metric in ["ndcg@10", "map", "mrr"]:
            before_avg = np.mean([m[metric] for m in before_rerank_metrics])
            after_avg = np.mean([m[metric] for m in after_rerank_metrics])
            improvement[metric] = (after_avg - before_avg) / before_avg * 100

        return {
            "improvements": improvement,
            "before_rerank": {
                metric: np.mean([m[metric] for m in before_rerank_metrics])
                for metric in ["ndcg@10", "map", "mrr"]
            },
            "after_rerank": {
                metric: np.mean([m[metric] for m in after_rerank_metrics])
                for metric in ["ndcg@10", "map", "mrr"]
            }
        }
```

---

## 6. ä¼æ¥­ç´šéƒ¨ç½²èˆ‡æ“´å±•

### 6.1 åˆ†æ•£å¼å‘é‡æª¢ç´¢æ¶æ§‹

#### **æ°´å¹³æ“´å±•çš„ç†è«–æ¨¡å‹**

**å®šç¾© 6.1** (åˆ†ç‰‡ç­–ç•¥): å°æ–¼ $n$ å€‹å‘é‡å’Œ $m$ å€‹åˆ†ç‰‡ï¼Œåˆ†ç‰‡å‡½æ•¸ $\text{Shard}: \{1,...,n\} \to \{1,...,m\}$ æ‡‰æœ€å°åŒ–ï¼š

$$\text{Load-Imbalance} = \max_{i \in \{1,...,m\}} \left|\frac{|\text{Shard}^{-1}(i)|}{n/m} - 1\right|$$

**ç­–ç•¥æ¯”è¼ƒ**:

| åˆ†ç‰‡ç­–ç•¥ | è² è¼‰å¹³è¡¡ | æŸ¥è©¢æ•ˆç‡ | ç¶­è­·è¤‡é›œåº¦ |
|---------|---------|---------|-----------|
| **å“ˆå¸Œåˆ†ç‰‡** | å„ªç§€ | ä¸­ç­‰ | ä½ |
| **ç¯„åœåˆ†ç‰‡** | ä¸­ç­‰ | å„ªç§€ | ä¸­ç­‰ |
| **ä¸€è‡´æ€§å“ˆå¸Œ** | è‰¯å¥½ | è‰¯å¥½ | é«˜ |
| **å‘é‡èšé¡åˆ†ç‰‡** | ä¸­ç­‰ | å„ªç§€ | é«˜ |

#### **Qdrant é›†ç¾¤éƒ¨ç½²æœ€ä½³å¯¦è¸**

```python
class QdrantClusterManager:
    """Qdrant é›†ç¾¤ç®¡ç†å™¨"""

    def __init__(self, cluster_config: Dict):
        self.cluster_config = cluster_config
        self.node_clients = self._initialize_node_clients()
        self.health_monitor = ClusterHealthMonitor()

    async def deploy_production_cluster(self) -> Dict:
        """éƒ¨ç½²ç”Ÿç”¢ç´šé›†ç¾¤"""

        deployment_results = {}

        # 1. ç¯€é»å¥åº·æª¢æŸ¥
        health_check = await self._comprehensive_health_check()
        deployment_results["pre_deployment_health"] = health_check

        if not health_check["all_healthy"]:
            return {
                "success": False,
                "error": "Cluster health check failed",
                "details": health_check
            }

        # 2. é›†åˆå‰µå»ºå’Œé…ç½®
        collection_results = await self._create_production_collections()
        deployment_results["collection_setup"] = collection_results

        # 3. è² è¼‰å¹³è¡¡é…ç½®
        load_balancer_config = await self._setup_load_balancer()
        deployment_results["load_balancer"] = load_balancer_config

        # 4. ç›£æ§é…ç½®
        monitoring_setup = await self._setup_cluster_monitoring()
        deployment_results["monitoring"] = monitoring_setup

        # 5. å‚™ä»½ç­–ç•¥
        backup_setup = await self._configure_backup_strategy()
        deployment_results["backup"] = backup_setup

        return {
            "success": True,
            "deployment_results": deployment_results,
            "cluster_endpoint": self._get_cluster_endpoint(),
            "management_dashboard": self._get_dashboard_url()
        }

    async def _comprehensive_health_check(self) -> Dict:
        """å…¨é¢å¥åº·æª¢æŸ¥"""

        health_results = {"all_healthy": True, "node_status": {}}

        for node_name, client in self.node_clients.items():
            try:
                # åŸºæœ¬é€£æ¥æ¸¬è©¦
                collections = await client.get_collections()

                # æ€§èƒ½æ¸¬è©¦
                performance = await self._test_node_performance(client)

                # è³‡æºä½¿ç”¨ç‡æª¢æŸ¥
                telemetry = await client.get_telemetry()

                node_health = {
                    "status": "healthy",
                    "collections_count": len(collections.collections),
                    "performance": performance,
                    "memory_usage": telemetry.get("memory_usage", {}),
                    "disk_usage": telemetry.get("disk_usage", {})
                }

                # æª¢æŸ¥è³‡æºä½¿ç”¨ç‡è­¦å‘Š
                if (performance.get("avg_search_time_ms", 0) > 100 or
                    telemetry.get("memory_usage", {}).get("percent", 0) > 85):
                    node_health["status"] = "warning"
                    health_results["all_healthy"] = False

            except Exception as e:
                node_health = {
                    "status": "unhealthy",
                    "error": str(e)
                }
                health_results["all_healthy"] = False

            health_results["node_status"][node_name] = node_health

        return health_results

    async def _test_node_performance(self, client: QdrantClient) -> Dict:
        """æ¸¬è©¦ç¯€é»æ€§èƒ½"""

        # å‰µå»ºæ¸¬è©¦é›†åˆ (å¦‚æœä¸å­˜åœ¨)
        test_collection = "performance_test"

        try:
            # æ¸¬è©¦å¯«å…¥æ€§èƒ½
            test_vectors = np.random.random((1000, 768)).astype(np.float32)
            write_start = time.time()

            points = [
                models.PointStruct(
                    id=i,
                    vector=vector.tolist(),
                    payload={"test": True}
                )
                for i, vector in enumerate(test_vectors)
            ]

            await client.upsert(test_collection, points)
            write_time = time.time() - write_start

            # æ¸¬è©¦æœç´¢æ€§èƒ½
            query_vector = np.random.random(768).tolist()
            search_start = time.time()

            search_results = await client.search(
                collection_name=test_collection,
                query_vector=query_vector,
                limit=10
            )

            search_time = time.time() - search_start

            # æ¸…ç†æ¸¬è©¦æ•¸æ“š
            await client.delete_collection(test_collection)

            return {
                "write_throughput": 1000 / write_time,  # vectors/sec
                "avg_search_time_ms": search_time * 1000,
                "search_qps": 1 / search_time
            }

        except Exception as e:
            return {
                "error": str(e),
                "write_throughput": 0,
                "avg_search_time_ms": float('inf'),
                "search_qps": 0
            }
```

---

## 7. å¯¦è¸ç·´ç¿’èˆ‡è©•ä¼°

### 7.1 èª²ç¨‹ä½œæ¥­

#### **ä½œæ¥­ 1: å‘é‡è³‡æ–™åº«æ€§èƒ½åŸºæº–æ¸¬è©¦**
å¯¦ç¾å®Œæ•´çš„å‘é‡è³‡æ–™åº«æ€§èƒ½æ¸¬è©¦å¥—ä»¶ï¼Œæ¯”è¼ƒ Qdrantã€Chromaã€FAISS çš„æ€§èƒ½å·®ç•°ã€‚

**è¦æ±‚**:
- æ”¯æ´ä¸åŒè³‡æ–™è¦æ¨¡ (1K, 10K, 100K, 1M å‘é‡)
- æ¸¬é‡æŸ¥è©¢å»¶é²ã€ååé‡ã€è¨˜æ†¶é«”ä½¿ç”¨
- åˆ†æä¸åŒåƒæ•¸é…ç½®çš„å½±éŸ¿
- æä¾›è©³ç´°çš„æ€§èƒ½åˆ†æå ±å‘Š

#### **ä½œæ¥­ 2: æ··åˆæª¢ç´¢ç³»çµ±è¨­è¨ˆ**
è¨­è¨ˆä¸¦å¯¦ç¾ä¸€å€‹å®Œæ•´çš„æ··åˆæª¢ç´¢ç³»çµ±ï¼Œæ•´åˆ BM25ã€å‘é‡æª¢ç´¢å’Œ SPLADEã€‚

**è©•ä¼°æ¨™æº–**:
- æª¢ç´¢ç²¾åº¦ (nDCG@10 > 0.8)
- ç³»çµ±å»¶é² (p95 < 200ms)
- æ“´å±•æ€§è¨­è¨ˆ
- ä»£ç¢¼å“è³ªå’Œæ–‡æª”å®Œæ•´æ€§

### 7.2 ä¼æ¥­æ¡ˆä¾‹åˆ†æ

#### **æ¡ˆä¾‹ï¼šé›»å•†å¹³å°çš„ç”¢å“æª¢ç´¢å„ªåŒ–**

**èƒŒæ™¯**: æŸå¤§å‹é›»å•†å¹³å°æ“æœ‰å„„ç´šå•†å“ï¼Œéœ€è¦æ”¯æ´è¤‡é›œçš„å•†å“æª¢ç´¢éœ€æ±‚ã€‚

**æŠ€è¡“æŒ‘æˆ°**:
- å¤šæ¨¡æ…‹æª¢ç´¢ (æ–‡æœ¬æè¿° + åœ–åƒç‰¹å¾µ)
- å€‹æ€§åŒ–æ’åº
- å¯¦æ™‚åº«å­˜éæ¿¾
- å¤šèªè¨€æ”¯æ´

**è§£æ±ºæ–¹æ¡ˆè¨­è¨ˆ**:
1. **å¤šå‘é‡æ¶æ§‹**: æ–‡æœ¬åµŒå…¥ + åœ–åƒåµŒå…¥ + ç”¨æˆ¶åå¥½åµŒå…¥
2. **å‹•æ…‹éæ¿¾**: åŸºæ–¼åº«å­˜ã€åƒ¹æ ¼ã€åœ°ç†ä½ç½®çš„å¯¦æ™‚éæ¿¾
3. **å€‹æ€§åŒ–é‡æ’åº**: çµåˆç”¨æˆ¶æ­·å²å’Œå¯¦æ™‚è¡Œç‚ºçš„æ’åºèª¿æ•´

**å¯¦æ–½æ•ˆæœ**:
- æœç´¢æº–ç¢ºç‡æå‡ 35%
- ç”¨æˆ¶é»æ“Šç‡æå‡ 28%
- æœç´¢å»¶é²ä¿æŒåœ¨ 80ms ä»¥å…§
- æ—¥å‡æœç´¢é‡æ”¯æ´ 1000è¬+ æ¬¡

---

## 8. æœ¬ç« ç¸½çµ

### 8.1 æ ¸å¿ƒç†è«–è¦é»

1. **æ•¸å­¸åŸºç¤**: é«˜ç¶­å‘é‡æª¢ç´¢çš„ç†è«–é™åˆ¶å’Œè¿‘ä¼¼è§£æ³•
2. **æ¼”ç®—æ³•åŸç†**: HNSWã€IVF-PQ ç­‰å…ˆé€²ç´¢å¼•çµæ§‹çš„è¤‡é›œåº¦åˆ†æ
3. **ç³»çµ±æ¶æ§‹**: åˆ†æ•£å¼å‘é‡è³‡æ–™åº«çš„è¨­è¨ˆåŸå‰‡å’Œå¯¦ç¾ç­–ç•¥
4. **æ€§èƒ½å„ªåŒ–**: å¾ç†è«–åˆ°å¯¦è¸çš„å®Œæ•´å„ªåŒ–æ–¹æ³•è«–

### 8.2 å¯¦è¸æŒ‡å°åŸå‰‡

1. **é¸å‹æ±ºç­–**: æ ¹æ“šæ•¸æ“šè¦æ¨¡ã€æŸ¥è©¢æ¨¡å¼ã€å»¶é²è¦æ±‚é¸æ“‡åˆé©çš„å‘é‡è³‡æ–™åº«
2. **åƒæ•¸èª¿å„ª**: åŸºæ–¼æ¥­å‹™éœ€æ±‚å¹³è¡¡ç²¾åº¦ã€é€Ÿåº¦ã€è¨˜æ†¶é«”ä½¿ç”¨
3. **ç›£æ§é‹ç¶­**: å»ºç«‹å®Œæ•´çš„æ€§èƒ½ç›£æ§å’Œæ•…éšœæ’é™¤æ©Ÿåˆ¶
4. **æ“´å±•è¦åŠƒ**: è¨­è¨ˆæ”¯æ´æ¥­å‹™å¢é•·çš„å¯æ“´å±•æ¶æ§‹

### 8.3 ä¸‹ç« é å‘Š

ç¬¬3ç« å°‡æ·±å…¥æ¢è¨æŸ¥è©¢å„ªåŒ–èˆ‡æ™ºèƒ½è·¯ç”±ï¼Œé‡é»åˆ†æå¦‚ä½•é€šé HyDEã€Step-Back Prompting ç­‰å…ˆé€²æŠ€è¡“æå‡æª¢ç´¢å“è³ªï¼Œä¸¦è¨­è¨ˆè‡ªé©æ‡‰çš„æŸ¥è©¢è™•ç†ç­–ç•¥ã€‚

---

## åƒè€ƒæ–‡ç»

[^19]: Beyer, K., Goldstein, J., Ramakrishnan, R., & Shaft, U. (1999). "When is 'nearest neighbor' meaningful?" *Database Theoryâ€”ICDT'99*, 217-235.

[^20]: Muennighoff, N., Tazi, N., Magne, L., & Reimers, N. (2022). "MTEB: Massive Text Embedding Benchmark." *arXiv preprint arXiv:2210.07316*.

[^21]: Qdrant Team. (2021). "Qdrant - Vector Database." *GitHub Repository*. https://github.com/qdrant/qdrant

[^22]: Formal, T., Piwowarski, B., & Clinchant, S. (2021). "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking." *SIGIR 2021*, 2288-2292.

---

**èª²ç¨‹è©•ä¼°**: æœ¬ç« å…§å®¹åœ¨æœŸæœ«è€ƒè©¦ä¸­å 25%æ¬Šé‡ï¼Œé‡é»è€ƒæŸ¥å‘é‡æª¢ç´¢ç†è«–å’Œç³»çµ±å¯¦ç¾èƒ½åŠ›ã€‚

**å¯¦é©—è¦æ±‚**: å­¸ç”Ÿéœ€å®Œæˆå‘é‡è³‡æ–™åº«æ€§èƒ½æ¸¬è©¦å’Œæ··åˆæª¢ç´¢ç³»çµ±çš„å®Œæ•´å¯¦ç¾ã€‚