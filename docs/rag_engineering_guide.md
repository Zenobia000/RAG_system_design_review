# 企業級 RAG 系統工程指南：從第一性原理到 2025 SOTA 架構

**一句話結論**：RAG 的致命點不在模型，而在「知識—檢索—上下文組裝」整條鍊路的工程化失真；要贏，就把 KB 治理、檢索質量、重排序與上下文壓縮做成可量化的流水線，必要時用 Graph/Tree-RAG 彎道超車。

---

## 目錄

1. [導論：從 Knowledge Base 出發看 RAG 的真實問題](#一導論從-knowledge-base-出發看-rag-的真實問題)
2. [第一性原理：RAG 為何會失真？](#二第一性原理rag-為何會失真)
3. [RAG 的教科書式演進：從單體腳本到模組化編排](#三rag-的教科書式演進從單體腳本到模組化編排)
4. [2025 最新技術地圖（按流水線分層）](#四2025-最新技術地圖按流水線分層)
5. [Context Engineering 的系統性缺失與對策](#五context-engineering-的系統性缺失與對策)
6. [企業內部知識庫的治理藍圖（RAGOps）](#六企業內部知識庫的治理藍圖ragops)
7. [代理的飛躍：從 RAG 管道到自主知識系統](#七代理的飛躍從-rag-管道到自主知識系統)
8. [戰略「彎道超車」：替代方案與混合未來](#八戰略彎道超車替代方案與混合未來)
9. [評測與 SLO（把「好不好」說清楚）](#九評測與-slo把好不好說清楚)
10. [可落地的工程範式](#十可落地的工程範式)
11. [風險與成本（工程視角）](#十一風險與成本工程視角)
12. [總結與工程對策表](#十二總結與工程對策表)

---

## 一、導論：從 Knowledge Base 出發看 RAG 的真實問題

RAG（Retrieval-Augmented Generation）能把企業內部知識即時「接上」LLM，理論很簡單：查→取→拼→生，但在企業場景常敗在三件事：

1. **知識庫本身髒亂**（無治理、無版本、無權限、無新鮮度規則）
2. **檢索信噪比低**（嵌入、索引、chunk 政策與查詢不匹配）
3. **Context Engineering 失真**（塞了不該塞的段落、關鍵證據被埋在中段、模板與任務不對齊）

長上下文模型仍存在「**中段遺失**」效應：關鍵線索放在 prompt 中間，命中率明顯下降，這是已被系統性量測的現象。

### 1.1 企業現實：「垃圾進，垃圾出」（GIGO）的災難

在學術基準測試之外，企業級 RAG 項目的現實往往是一場數據治理的災難。業界普遍發現，RAG 的技術流程（切割、嵌入、檢索）正迅速「商品化」，真正的挑戰在於底層的知識資產。

企業的「知識庫」——通常是 Confluence、SharePoint、零散的 PDF 和硬碟中的文件——本質上是高度「熵增」的系統。我們面臨的現實是：

- **內容過時**：大量文件停留在 2019 年，早已不反映當前流程
- **資訊矛盾**：不同部門對同一流程的描述相互衝突
- **上下文缺失**：技術文檔中充滿了「部落知識」（tribal knowledge）的假設，RAG 無法理解隱含的上下文
- **治理真空**：內容沒有清晰的所有權或審核週期

RAG 的第一性原理缺陷於此顯現：當 RAG 建立在這樣一個混亂的知識庫之上時，它並不能解決知識問題，反而會加速問題的暴露。它使 LLM 能夠更自信地引用那些過時、矛盾或不相關的來源，從而產生更具欺騙性的「幻覺」。

在許多系統設計中出現的「Guardrail」（護欄）組件，正是對這一第一性原理缺陷的工程性承認。它默認 RAG 系統的輸出是不可靠的，必須在最後一刻進行攔截和審查。

---

## 二、第一性原理：RAG 為何會失真？

### 2.1 成功率分解公式

把 RAG 成功率拆成一個可工程化的期望：

\[
P(\text{正確}) = P(\text{檢索命中} \mid q) \times P(\text{排序置前} \mid \text{命中}) \times P(\text{上下文可用} \mid \text{置前}) \times P(\text{生成忠實} \mid \text{上下文})
\]

其中：
- \(P(\text{檢索命中} \mid q)\)：recall@k、覆蓋率
- \(P(\text{排序置前} \mid \text{命中})\)：rerank、MMR/RRF
- \(P(\text{上下文可用} \mid \text{置前})\)：chunk/壓縮/位置偏置
- \(P(\text{生成忠實} \mid \text{上下文})\)：faithfulness

### 2.2 失真來源與根因

**檢索失真**：嵌入向量表示不穩、索引近似誤差（ANN）、chunk 切割破壞語義邊界。常用 ANN 如 FAISS/HNSW 帶來可接受的近似，但參數沒調好就是 recall 損失。

**排序失真**：只靠向量相似度易被「語義相近但無關」誘導，需 cross-encoder rerank 校正。

**上下文失真**：上下文過長、重複、噪音與位置偏置（lost-in-the-middle）共同造成關鍵證據被淹沒。

**生成失真**：LLM 受自身先驗影響，對檢索證據採納不足（hallucination/over-trust prior），需「忠實度」約束與評測。

### 2.3 上下文工程的系統性失效分析

「上下文工程」是一個比 RAG 更廣泛的概念，它指的是設計和控制 AI 模型在生成回應前所能「看到」的所有資訊的實踐。RAG 只是上下文工程中的一個組件。從企業知識庫的現實出發，RAG 在上下文工程的每一步都面臨系統性的失效級聯。

#### 失效點一：預處理（Preprocessing）—「垃圾進」的源頭

RAG 的失敗始於「攝取」（Ingestion）階段。企業文檔（如 PDF、PPT）的複雜性遠超純文本。當解析器無法正確提取表格、圖表或解析多欄佈局時，關鍵資訊在進入向量數據庫之前就已經「丟失」。此外，缺乏統一的資料治理和元數據（Metadata），使得系統從一開始就無法區分權威性、時效性或準確性。

#### 失效點二：檢索（Retrieval）—「錯誤上下文」問題

這是 RAG 最常被詬病的失敗點。

- **語義失配（Semantic Mismatch）**：使用者提問的措辭與知識庫中的術語不匹配（例如「語言斷層」，language disconnects）
- **特異性失效（Specificity Failure）**：純粹的向量語義搜索無法處理關鍵字、縮寫詞或特定 ID。例如，一個 RAG 系統可能很難區分「CAR」（在腫瘤學中是「嵌合抗原受體」）和「CAR」（在放射學中是「電腦輔助放射學」）。它也幾乎總是在「表 3 的具體劑量是多少？」這類精確查詢上失敗，導致「頂級排名文檔丟失」

#### 失效點三：增強（Augmentation）—「上下文中丟失」問題

這是一個更為根本的、關於 LLM 本身的缺陷。即使 RAG 成功檢索到正確的文檔，LLM 也可能無法在提供的上下文中找到答案。

- **「大海撈針」（NIAH）測試**：這項著名的基準測試專門用於評估這一點：它將一個「針」（事實）隱藏在一個長上下文「草堆」中，然後提問 LLM
- **「中間迷失」（Lost in the Middle）現象**：2023 年的一項關鍵研究（Liu et al.）揭示了 NIAH 測試失敗的原因。LLM 在處理長上下文時，表現出明顯的「U 型」注意力曲線：它們高度關注上下文的開頭和結尾，卻嚴重忽略中間部分的資訊

這一發現具有重大意義：它證明了 RAG 的失敗不僅是檢索問題，也是生成器（LLM）的利用率問題。盲目地檢索更多文檔（增加 Top-K）或使用更長的上下文窗口（如 1M token），反而可能將正確答案「埋」在 LLM 的注意力盲區（即「中間」），從而降低性能。這使得「重新排序」（Reranking）等後處理技術成為必要的工程實踐。

#### 失效點四：生成（Generation）—「不忠實」問題

即使正確的資訊被檢索到並處於 LLM 的注意區域，LLM 仍可能失敗：

- **上下文忽略**：LLM 可能選擇忽略提供的上下文，轉而依賴其內部（可能已過時）的參數化知識
- **綜合失敗（Synthesis Failure）**：當檢索到的文檔相互矛盾時（在混亂的企業知識庫中很常見），LLM 往往無法進行裁決，或者會產生「不完整答案」
- **創造性誤讀**：LLM 可能「誤解」上下文的微妙之處，例如將一個修辭性標題誤讀為事實陳述

---

## 三、RAG 的教科書式演進：從單體腳本到模組化編排

為了應對上述系統性失效，RAG 架構在短短幾年內經歷了三次快速的世代演進。

### 3.1 世代一：Naive RAG（c. 2020-2023）

這是 RAG 的原始範式，由一個簡單、線性的「檢索-生成」（Retrieve-then-Generate）流程定義。

**架構**：使用者查詢 -> 嵌入 -> 向量搜索 (Top-K) -> 上下文增強 -> LLM 生成

**組件**：
- **索引（Indexing）**：將文檔切割成塊（Chunks），通過嵌入模型（Embedding Model）轉換為向量，並存儲在向量數據庫（Vector Database）中
- **檢索與生成（Retrieval & Generation）**：在運行時，將使用者查詢嵌入，執行相似性搜索以獲取最相似的 K 個文本塊，然後將這些文本塊與原始查詢一起作為上下文（"prompt stuffing"）傳遞給 LLM

**工程分析**：這個開創性的架構（現在被其創作者稱為「幾乎只是個基準」）非常脆弱。它直接暴露在所有失效點之下：它對混亂的數據束手無策，容易因語義失配而檢索失敗，並且是「中間迷失」問題的主要受害者。

### 3.2 世代二：Advanced RAG（c. 2023-2024）

業界很快認識到 Naive RAG 的缺陷，開始在核心流程中加入離散的優化步驟，以提高檢索的精確性（Precision）。

**架構**：流程變為一個更長的線性管道：
- **檢索前（Pre-Retrieval）**：查詢重寫（Query Rewriting）/ 查詢擴展（Query Expansion）
- **檢索（Retrieval）**：混合搜索（Hybrid Search）（例如 BM25 + 向量）
- **檢索後（Post-Retrieval）**：重新排序（Reranking）
- **生成（Generation）**：上下文壓縮 / 格式化後再提交給 LLM

**工程分析**：第二代 RAG 代表了從「資訊檢索」到「上下文感知知識綜合」的轉變。它通過增加的步驟直接解決特定的失敗點：
- 混合搜索解決了特異性失效（失效點二）
- 重新排序解決了「中間迷失」問題（失效點三）

然而，它本質上仍然是一個靜態的、一體適用的（one-size-fits-all）線性管道。

### 3.3 世代三：Modular RAG（c. 2024-至今）

這是當前的 SOTA（State-of-the-Art）。其核心認知是：沒有任何一個單一的線性管道可以處理所有類型的查詢。

**架構**：Modular RAG 不再是一個固定的管道，而是一個框架或可組合的工具包。它將 RAG 流程分解為可插拔、可互換的模組，這些模組可以根據查詢的需要被動態調用。

**關鍵組件**：
- **檢索器（Retrievers）**：向量檢索、稀疏檢索（BM25）、圖檢索（Graph）、SQL 檢索器等
- **精煉器（Refiners）**：重新排序器（Rerankers）、總結器（Summarizers）、提取器（Extractors）
- **迴圈（Loops）**：迭代檢索（Iterative Retrieval, 如 FLARE）、自我提問（Self-Ask）
- **路由器（Routers）**：一個基於 LLM 的決策者，它選擇接下來應該運行哪個模組

**從軟體工程演進看 RAG 的典範轉移**：
- Naive RAG 就像一個簡單的、單體的腳本
- Advanced RAG 就像一個更長、更複雜的單體函數
- Modular RAG 則是一次架構上的飛躍，它引入了微服務架構的思想：模組化、可替換性、以及動態路由

這一典範轉移是至關重要的，因為它為「代理」（Agent）的出現奠定了基礎。一個 AI 代理（AI Agent）正是這個模組化系統中的編排者（Orchestrator）或路由器（Router），它根據動態計畫調用這些 RAG「微服務」。

---

## 四、2025 最新技術地圖（按流水線分層）

### 4.1 前檢索（Query Engineering / Router）

#### 多式樣查詢與融合

對同一問題產生多個 query（同義、拆解、多跳），用 **RRF**（Reciprocal Rank Fusion）融合多路檢索：

\[
\text{RRF}(d) = \sum_i \frac{1}{k + \text{rank}_i(d)}
\]

MMR 去冗：

\[
\arg\max_{d \in D} \lambda \cdot \text{sim}(q,d) - (1-\lambda) \max_{d' \in S} \text{sim}(d,d')
\]

實務常設 \(k \approx 60\)、\(\lambda \in [0.3, 0.7]\) 為作業中標準值。

#### 查詢轉換（Query Transformation）

這種 SOTA 技術使用 LLM 在檢索之前重寫使用者的查詢，以克服查詢意圖和文檔措辭之間的鴻溝。

**假設性文檔嵌入（HyDE）**：
1. 使用者提出查詢（Q）
2. LLM 不去檢索，而是先生成一個假設的、完美的答案（A'），這個答案通常是幻覺，但語義上接近真實答案
3. 系統丟棄 Q，轉而對這個假設的答案 A' 進行嵌入
4. 在向量數據庫中搜索與 A' 相似的真實文檔塊

**第一性原理分析**：HyDE 是一個天才的技巧。它解決了「語義失配」問題，因為它不再在「查詢嵌入空間」中搜索，而是在「文檔嵌入空間」中搜索。它尋找的是「答案到答案」的相似性。

**「退一步」提示（"Step-Back Prompting"）**：
1. 使用者提出一個非常具體的查詢（例如：「Thierry Audel 在 2007 年到 2008 年為哪支球隊效力？」）
2. LLM 被提示「退一步」，生成一個更抽象、更高層次的查詢（例如：「Thierry Audel 的職業生涯歷史是什麼？」）
3. RAG 系統檢索這個更廣泛查詢的文檔（這通常更容易檢索到）
4. LLM 使用這個廣泛的上下文來回答原始的具體問題

**工程影響**：該技術對於事實密集型任務非常有效，因為具體的細節可能隱藏得很深，但包含該細節的一般性上下文（例如個人簡歷）卻很容易被高層次查詢找到。

### 4.2 檢索（Retriever / Index）

#### 混合搜索（Hybrid Search）：稀疏 + 密集檢索

這是當前檢索階段的 SOTA 標準。它結合了兩種檢索範式：

- **密集檢索（Dense Retrieval）**：即向量搜索。它理解語義和概念（「關於筆電電池壽命的投訴」）
- **稀疏檢索（Sparse Retrieval）**：即關鍵字搜索（如傳統的 BM25）。它捕捉字面匹配（「SKU-A8B-PXT」）

**必要性**：正如失效點二所分析，單純的密集檢索在處理縮寫詞、ID 和特定術語時會失敗。

**工程實現**：系統並行運行兩個檢索器，然後使用「倒數排名融合」（Reciprocal Rank Fusion, RRF）算法將兩個排名列表合併為一個單一的、更強大的結果列表，然後再將其交給下一階段（重新排序）。

#### 長上下文嵌入與多粒度表示

E5、GTE、BGE-M3 與後續變體在 MTEB 上長期領先；部分型號針對長上下文做延展（如 mGTE 系列）。選型優先看你的語言/任務子榜單，而非總分。

#### 索引選型

HNSW/IVF-PQ 是主力；FAISS GPU 化能在億級向量下保持延遲可控。

### 4.3 結構化擴展：Graph/Tree-RAG（彎道超車）

#### GraphRAG

先用 LLM/IE 建圖（節點=實體/主題，邊=關係/證據），問答時沿子圖檢索→更能處理「模糊/綜觀/橫向關聯」型問題，對企業敘事資料特別有效。

**工程權衡**：業界對 GraphRAG 的懷疑是合理的。它的預處理成本極高（「更多的預處理、更多的成本、更多的活動部件」）。

**SOTA: GNN-RAG**：學術界的 SOTA 甚至開始使用圖神經網絡（GNNs）來學習圖上的最佳檢索路徑，而不僅僅是顯式遍歷。

#### RAPTOR（樹式分層摘要）

先對長文分群/遞迴摘要形成樹，查詢時走樹到葉節點檢索，顯著改善長文/多跳查詢。

### 4.4 重排序（Reranker）

**Cross-Encoder Rerank**（如 Cohere Rerank-3/3.5、BGE-Reranker-v2-M3）顯著提升前段文本的精度，常見做法是 top-k=100→rerank→取 top-n（8~16）進 LLM。雖增加延遲，但在企業問答中是最穩的質量槓桿。

**機制**：這是一個兩階段過程。
- **階段一（檢索）**：速度快，專注召回率（Recall）。例如，混合搜索返回 Top 100 個潛在相關文檔
- **階段二（重排）**：速度慢，成本高，專注精確性（Precision）

**模型（Cross-Encoders）**：Reranker 使用一個更強大（也更慢）的交叉編碼器（Cross-Encoder）模型（如 Cohere、ColBERT 或 bge-reranker）。與嵌入模型（分別計算 Q 和 D）不同，交叉編碼器同時處理 (查詢, 文檔塊) 對，從而能更精確地評估兩者之間的真實相關性。

**工程影響**：Reranking 直接解決了「中間迷失」問題。它允許我們從 100 個檢索結果中，精確找出最相關的 5 個，並將它們放置在上下文窗口的開頭，確保 LLM 的注意力能「看到」它們。

### 4.5 檢索後（Post-Retrieval）：反思與糾錯範式

#### 糾錯檢索增強生成（CRAG）

CRAG 是一種「即插即用」（plug-and-play）的模組，它在檢索和生成之間增加了一個評估步驟。

**核心思想**：承認檢索會失敗，並為此建立一個自動糾錯迴路。

**架構**：
1. 常規檢索
2. 一個輕量級的檢索評估器（Retrieval Evaluator，通常是一個小型的、經過微調的模型）對檢索到的文檔進行評分（判斷其與查詢的相關性）
3. 該分數觸發三個動作之一：
   - **正確（Correct）**：文檔相關。系統對其進行精煉（例如分解為更小的「知識條」）並傳遞給 LLM
   - **錯誤（Incorrect）**：文檔不相關。系統丟棄這些文檔，轉而觸發網路搜索（Web Search）作為後備
   - **模糊（Ambiguous）**：介於兩者之間。系統將精煉後的文檔與網路搜索結果合併

**分析**：CRAG 是應對混亂企業知識庫（靜態、有限且必然會失敗）的完美解決方案。它將程式化的失敗處理引入 RAG 管道，構建了一個簡單、強大的代理迴路（agentic loop）（「if 檢索失敗, then 網路搜索」）。

#### 自我反思 RAG（Self-RAG）

Self-RAG 採取了更激進的方法：它不再依賴外部模組，而是訓練 LLM 本身具備自我糾錯和反思的能力。

**核心思想**：LLM 在生成過程中，會主動生成特殊的「反思令牌」（Reflection Tokens）。

**架構**：在生成的每一步，LLM 都會自我決策：
- `[Retrieve]`：我是否需要檢索資訊來回答這個問題？（是/否）
- `[Relevant]`：（檢索後）這個文檔與問題相關嗎？（相關/不相關）
- `[Support]`：（生成答案後）我的回答是否被文檔支持？（完全支持/部分支持/不支持）
- `[IsUse]`：（生成答案後）這個答案有用嗎？（1-5 分）

**分析**：Self-RAG 是集成度最高的 SOTA。它使 LLM 同時成為了檢索評估器、生成器和護欄。它只在必要時才進行檢索，從而實現了自適應檢索（Adaptive Retrieval）。

**CRAG vs. Self-RAG 的戰略選擇**：
- **CRAG**：模組化的，易於實現，可與任何 LLM（如 GPT-4、Claude）配合使用
- **Self-RAG**：集成化的，其邏輯是學習而非編程的，因此可能更強大，但它要求你必須使用特定的、經過微調的 Self-RAG 模型

### 4.6 上下文工程（Context Engineering）

#### 任務導向模板

把「任務、角色、輸出結構、引用規範」模板化。

#### 上下文壓縮/去重/去干擾

消除重複句、表格標頭、法規 footer 等。

#### 位置偏置緩解

把關鍵證據放開頭或結尾分區（或分兩輪對話逐步餵入），對抗 lost-in-the-middle。

### 4.7 生成（Generation）與安全（Guardrails）

#### 忠實度約束

在回覆中強制「逐句引用來源」或「段落-證據對齊」，能顯著降低幻覺。

#### Guardrails

在生成前後加入策略（越權/PII/機密/合規）檢查與屏蔽。可用現成框架如 NeMo Guardrails 等落地。

這是企業安全層（Enterprise Safety Layer）。這是 CRAG/Self-RAG 中內置檢查的外部化版本。該模組獨立審查 LLM 的最終輸出，檢查：
- 事實一致性（是否幻覺）
- 合規性（是否洩露 PII 或敏感數據）
- 毒性 / 品牌聲譽

---

## 五、Context Engineering 的系統性缺失與對策

### 5.1 故障清單與對策表

| 缺失 | 觀察訊號 | 主要根因 | 一線對策 |
|------|---------|---------|---------|
| 中段遺失 | 引文在 prompt 中段時答錯 | LLM 長上下文位置偏置 | 關鍵證據置頂/置底、分輪饋送、分段思考提示 |
| 題義漂移 | 答案正確但離題 | Query 不穩定 | 多式樣查詢 + RRF 融合、任務限定語境（domain/時間/地點） |
| 檢索回傳噪音 | 前 10 片段半數無關 | 向量/索引/chunk 策略不匹配 | 調整 chunk（語義切分+重疊）、hybrid 檢索、top-k 擴大 + rerank |
| 舊知誤導 | 回答老版本流程 | KB 無新鮮度策略 | 建立 TTL 與版本標籤、CDC（變更捕捉）自動重嵌/重建索引 |
| 多跳失敗 | 需要跨文件關聯時崩潰 | 扁平檢索 | 採 GraphRAG 或 RAPTOR；或在前檢索做子任務拆解 |
| 引用不可驗 | 無法指出原文段落 | 上下文拼裝亂 | 嚴格「段落級引用」與來源 ID；模板強制列出來源→句子對齊 |
| 合規風險 | 洩露內機密 | 權限/屏蔽缺失 | 權限標籤進檢索路由；生成後 Guardrails 再審 |

### 5.2 先進的切割策略（Chunking Strategies）

傳統的固定大小切割（Fixed-Size Chunking）既愚蠢又低效。SOTA 方法專注於語義。

#### 語義切割（Semantic Chunking）

不依賴固定的字元數，而是計算相鄰句子嵌入向量的相似度。當相似度降至某個閾值以下時（即主題發生變化），系統就在此處「切割」。這能確保高度相關的語句（如一個完整的段落）保持在一個 Chunk 中。

#### 命題切割（Propositional Chunking）

這是更先進的基於 LLM 的方法。它使用 LLM 將文檔分解為最小的、原子化的事實陳述或命題（Propositions）。然後，系統對這些命題進行嵌入和檢索。

**工程權衡**：這種方法將 LLM 的昂貴計算成本從運行時（Runtime）轉移到了攝取時（Ingestion）。這是一個明智的權衡：它顯著增加了初始處理成本，但通過提供極度精確和上下文豐富的 Chunk，極大地降低了檢索錯誤並提高了生成品質。

#### 切割方法學比較分析

| 方法 | 機制 | 優點 | 缺點 | 最佳適用場景 |
|------|------|------|------|------------|
| 固定大小（Fixed-Size） | 按字元/Token 數切割 | 簡單、快速、可預測 | 語義割裂；上下文不完整 | 快速原型；結構化文本 |
| 遞歸（Recursive） | 嘗試按段落、句子等遞歸切割 | 結構感知；比固定大小好 | 仍可能割裂語義 | 通用文本；Markdown |
| 語義切割（Semantic） | 按嵌入相似度斷點切割 | 語義連貫性高 | 計算成本中等；對代碼/表格效果不佳 | 敘事性文本；文章 |
| 命題切割（Propositional） | LLM 提取原子事實 | 語義最精確；有利於事實問答 | 計算成本非常高；可能丟失風格 | 事實密集型知識庫；Q&A |

### 5.3 元數據提取與富化（Metadata Extraction）

這是處理企業混亂知識庫最關鍵的步驟之一。它涉及解析非結構化文檔（如 PDF、Word），並為每個 Chunk 附加結構化的元數據（如作者、創建日期、文檔類型、主題、摘要等）。

**工程影響**：這樣做可以實現「元數據感知過濾」（Metadata-Aware Filtering）。RAG 系統不再是盲目地進行向量搜索，而是可以執行一個複合查詢，例如：

```sql
SELECT chunk WHERE (vector_similarity > 0.9) AND (metadata.type == 'Legal') AND (metadata.date > '2024-01-01')
```

這極大地縮小了搜索範圍，從源頭上解決了「內容過時」的問題。

---

## 六、企業內部知識庫的治理藍圖（RAGOps）

### 6.1 七步治理流程

**(1) 來源治理**：定義「權威來源」白名單、版控策略（版本=語義快照）、資料類型（檔案/表格/DB/票據）。

**(2) 擷取與清洗**：OCR/表格結構化、去重（shingling/SimHash）、機敏標註。

**(3) Chunk 政策**：語義斷點切分 + 20–30% 重疊；表格/程式碼用結構化切分；為每個 chunk 附 **metadata**（來源、時間、權限、版本）。

**(4) 嵌入/索引**：語言對應的嵌入模型（觀測 MTEB 分榜），長文採 RAPTOR/章節概括索引，索引採 HNSW/IVF-PQ 混合。

**(5) 檢索/重排**：Hybrid 檢索→Cross-Encoder 重排（top-k→top-n）；對 FAQ 用語義快取（semantic cache）。

**(6) 上下文組裝**：MMR 去冗、RRF 融合、格式模板、引用對齊、壓縮後再餵 LLM。

**(7) 度量/回饋**：線上蒐集 Q/A log→離線用 Ragas/ARES 追蹤 **Context Precision/Recall、Faithfulness**，並回寫訓練樣本做持續改進。

---

## 七、代理的飛躍：從 RAG 管道到自主知識系統

### 7.1 代理的需求：為什麼 Modular RAG 仍嫌不足

第三部分中的先進技術創造了一個高度優化但仍顯靜態的管道。它擅長回答一個問題，但無法處理動態的、多步驟的任務，也無法與真實世界的 API 交互。

一個 AI 代理（AI Agent）是一個使用 LLM 進行推理、規劃和行動的系統。在這種架構中，LLM 的角色從一個被動的生成器轉變為一個主動的編排者（Orchestrator）。

因此，使用者提供的流程圖所展示的並非一個 RAG 管道，而是一個「代理式 RAG 系統」（Agentic RAG System）。它編排多個工具，而 RAG 只是其中之一。

### 7.2 解構「Gen AI 代理系統」流程圖：2025 年的 SOTA 藍圖

使用者提供的流程圖是企業級 AI 應用的 SOTA 工程藍圖。以下我們將結合 SOTA 技術對其進行逐層解構：

#### 1. 查詢路由器（Query Router）

系統的第一步並非 RAG，而是由一個主管代理（Supervisor Agent）或路由器接收查詢。它使用 LLM 進行規劃（Planning），並決策：「根據這個查詢，我應該：
- a) 走 RAG 路徑，從 Vector database 檢索非結構化知識？
- b) 走工具使用（Tool Use）路徑，調用 Function api 獲取即時數據？
- c) 走記憶路徑，檢查 Q/A log Cache DB 中是否已有答案？」

這種基於 LLM 的路由是代理系統的核心定義。2025 年 SOTA 論文 RAGRouter 甚至討論了如何訓練一個「RAG 感知」的專用路由器。

#### 2. 雙重檢索路徑：Vector database vs. Function api & MCP service

這是該架構最精妙之處。企業知識存在於兩種形態：
- **靜態的、非結構化的**（政策、手冊、報告）。由 Vector database（RAG）解決
- **即時的、結構化的**（數據庫、API、當前狀態）。由 Function api（Tool Use）解決

**函數調用（Function Calling）**：這是賦予 LLM「雙手」的機制。路由器 LLM 決定調用一個預定義的函數（如 `get_user_pto_balance(user_id)`）。

**MCP 服務**：圖中的 MCP service（模型上下文協議）是一種更先進、更標準化的協議，用於管理這些函數調用，特別是在安全性、工具的動態發現和上下文管理方面。

#### 3. 增強（Augmentation）步驟

代理（Agent）作為編排者，收集來自所有來源（RAG、函數調用、快取）的上下文，將它們合成到一個最終的「增強提示詞」中，然後將其交給最後的生成 LLM（LLM dense model）。

#### 4. Guardrail -> Answer response

這是企業安全層（Enterprise Safety Layer）。這是 CRAG/Self-RAG 中內置檢查的外部化版本。該模組獨立審查 LLM 的最終輸出，檢查事實一致性、合規性、毒性等。

#### 5. 外部迴路：LLMops 和 NLP AI engineer

這標誌著系統是一個持續演進的產品。Q/A log Cache DB（問答日誌）不僅用於快取，它更是 LLMops 的黃金數據來源。工程師可以審查失敗的案例，創建新的評估集，並持續微調（Fine-tune）系統的各個組件（例如 CRAG 的檢索評估器，或路由器的決策模型）。

### 7.3 實施框架：LangGraph（SOTA 2025）

如何構建這個流程圖？這個圖形包含了分支（RAG vs. API）、迴圈（CRAG 的糾錯）和狀態管理（多步驟任務）。

**LangChain 的局限**：傳統的 LangChain 主要用於構建線性的鏈（Chains）。

**LangGraph 的崛起**：LangGraph 是一個專門用於構建有狀態、多代理系統的 SOTA 框架。它將系統定義為一個圖（Graph），節點（Node）是執行單元（如 RAG、Function Call），邊（Edge）是控制流。

LangGraph 允許我們以工程化的方式實現流程圖中的分支和迴圈。例如，一個 CRAG 迴圈可以被定義為：檢索節點 -> 評估節點 -> (條件分支)：
- if docs_good? -> 生成節點
- if docs_bad? -> 網路搜索節點 -> 生成節點

LangGraph 是實現使用者流程圖中代理架構的 2025 年 SOTA 工程答案。

### 7.4 系統最佳化位點

- **Q/A log & Cache DB**：把成功/失敗對與「檢索證據」一起紀錄，離線餵給 Ragas/ARES；把高頻問題做 semantic cache（向量快取）與 FAQ-style direct answer
- **MCP / Function API**：建立「資料路由表」：純文件→RAG、結構化→SQL/Graph 檢索、即時→工具（API）；Router 規則寫在前檢索層
- **Guardrail 節點**：放在「重寫/模板」與「最終輸出」兩處，對權限、PII、規範做前後雙閘

---

## 八、戰略「彎道超車」：替代方案與混合未來

RAG 並非萬能。針對「如何優化」和「彎道超車的方法」，企業架構師必須在 RAG、微調（Fine-tuning）和長上下文模型（Long-Context Models）之間進行戰略權衡。

### 8.1 RAG vs. 微調 vs. 長上下文的三難困境

| 方法 | 核心機制 | 優點 | 缺點 |
|------|---------|------|------|
| RAG | 在推理時外部提供上下文 | • 知識即時更新<br>• 低幻覺、可溯源<br>• 更新成本低（只需更新 DB） | • GIGO（受知識庫品質上限約束）<br>• 檢索是瓶頸<br>• 存在「中間迷失」問題<br>• 推理延遲較高 |
| 微調 (Fine-Tuning) | 通過訓練調整模型內部權重 | • 教授風格、術語和行為模式<br>• 推理時延遲低、成本低（提示詞短）<br>• 掌握 RAG 無法傳遞的隱性知識 | • 知識是靜態的（訓練截止日期）<br>• 前期訓練成本高<br>• 仍會產生幻覺（只是幻覺更「像」你的領域）<br>• 風險：「災難性遺忘」（忘記通用能力） |
| 長上下文模型 (LCMs) | 在推理時將所有文檔塞入上下文窗口 | • 理論上繞過了「檢索」步驟<br>• 擅長對已知文檔集進行全面綜合 | • 成本和延遲極高<br>• 仍然存在「中間迷失」問題（1M 窗口也沒用）<br>• 無法擴展到「企業級」知識庫（5000 萬文檔） |

### 8.2 2025 SOTA 混合策略：「彎道超車」即「全都要」

戰略性的「彎道超車」不是三選一，而是組合拳。

#### RAG + 微調（RAFT）

這是 SOTA 的最佳實踐。微調的目的不是教會 LLM 知識（這是 RAG 的工作），而是教會 LLM 如何更好地使用 RAG。檢索增強微調（Retrieval-Augmented Fine-Tuning, RAFT）是一種特定方法，它創建 (查詢, 檢索到的文檔, 完美答案) 的訓練集，專門訓練模型處理 RAG 帶來的噪音和干擾，學會忠實地從提供的上下文中提取答案。

#### RAG + 長上下文（LCM）

LCM 的真正價值不是替代 RAG，而是增強 RAG 的最後一步。
1. 使用 SOTA RAG（第三部分）從 5000 萬份文檔中精確檢索出 Top 0.1% 最相關的上下文（例如 50k token）
2. 將這些高度相關的上下文「傾倒」進一個 1M token 的 LCM 中
3. 這為 LLM 提供了一個巨大的「工作區」（Workspace），使其能夠對複雜的、跨文檔的綜合問題給出卓越的答案

### 8.3 GraphRAG：結構化-語義的邊界（另一個「彎道超車」）

對於企業內部知識，尤其是那些關係密集型的知識，還有另一種更根本的飛躍。

#### 向量數據庫 vs. 知識圖譜（KG）

- **向量數據庫**：存儲文本塊。它回答「關於 X 的文檔」
- **知識圖譜**：存儲實體（節點）和關係（邊）。它回答「X 連接到 Y，Y 依賴於 Z」

#### 什麼是 GraphRAG？

GraphRAG 是一種先進的 RAG，它首先使用 LLM 從非結構化文本中構建一個知識圖譜，然後通過遍歷圖（Graph Traversal）來進行檢索。

**工程權衡**：業界對 GraphRAG 的懷疑是合理的。它的預處理成本極高（「更多的預處理、更多的成本、更多的活動部件」）。

**SOTA: GNN-RAG**：學術界的 SOTA 甚至開始使用圖神經網絡（GNNs）來學習圖上的最佳檢索路徑，而不僅僅是顯式遍歷。

#### 使用時機

GraphRAG 不是向量 RAG 的替代品。它們是解決不同問題的專用工具。

| 使用 | Vector RAG（第三部分的所有優化） | GraphRAG（作為代理工具之一） |
|------|--------------------------------|---------------------------|
| 用例 | • 「與你的文檔聊天」<br>• 語義搜索<br>• 針對非結構化文本的問答 | • 複雜、多跳（multi-hop）推理<br>• 「假設分析」<br>• 網路/關係分析（金融、法律、供應鏈） |
| 查詢範例 | 「我們的遠程工作政策是什麼？」 | 「歐盟辦公室中，哪些工程師既具有『RAG』專業知識，又在過去 6 個月參與了『Y 項目』？」 |

### 8.4 SOTA 與變形：何時採用「彎道超車」？

- **資料關聯複雜、問法抽象** → **GraphRAG**：從文件轉知識圖譜，問答沿圖檢索與摘要，對政策、流程、專案脈絡特別有效
- **文件極長**（法務、規範、設計規格）→ **RAPTOR**：先做層級摘要後再檢索，避免把整本書塞進 prompt
- **查準率是命門** → **Cross-encoder Rerank** 常是 CP 值最高的提昇手段
- **人力有限但要穩** → 以 MTEB 分榜挑選嵌入（中文/多語/長文），小改即可獲益

---

## 九、評測與 SLO（把「好不好」說清楚）

### 9.1 離線指標（每天批次）

- **Retrieval**：Recall@k、nDCG、Coverage by source/version
- **Rerank**：MAP@k、Mean Reciprocal Rank（MRR）
- **Augmentation**：Context Precision/Recall（Ragas）、去重率、壓縮比
- **Generation**：Faithfulness/Answer Relevance（ARES）、來源對齊率

### 9.2 線上指標（每請求）

- Evidence-per-Answer（平均引用段數）
- Hallucination 告警率
- 平均延遲（p95/p99）
- 單答 token 成本

### 9.3 Gate 策略

- 「先過檢索，再放生成」：未達 recall@50 ≥ 0.8（離線）則禁止上線變更
- 重要決策問題啟用「兩階段回答」：先出**證據列表**，再出結論（提升可審計性）

---

## 十、可落地的工程範式

### 10.1 檢索與重排（Python 偽碼）

```python
docs = hybrid_retrieve(query, topk=120)          # BM25 + dense
docs = rrf_fuse(multi_queries(query), docs)      # 融合多查詢路徑
reranked = cross_encoder_rerank(query, docs)     # Cohere/BGE reranker
context = mmr_select(reranked, n=12, lambda_=0.5)# 去冗保多樣性
prompt = build_prompt(task_template, context)    # 模板化上下文
answer = llm.generate(prompt, citations=True)    # 強制引用
```

### 10.2 KB 片段（Chunk）策略

```
Rule 1  語義斷點切分（段落/標題/程式碼區塊），重疊 20–30%
Rule 2  表格/程式碼改為結構化片段（保留欄名/行號）
Rule 3  metadata: {source_id, version, timestamp, owner, acl, ttl}
Rule 4  任何變更觸發：re-embed -> reindex（CDC 管道）
```

---

## 十一、風險與成本（工程視角）

- **延遲**：rerank/GraphRAG/RAPTOR 均會增加延遲；以 **分層退火**（先小模型 rerank、命中率不足再上大模型）控時
- **索引成本**：百萬級 chunk 一次重嵌很貴；採「CDC + 分層向量（summary-node + leaf）」減少重嵌面積
- **治理人力**：建 KB ≈ 建數據產品；沒有 owner 與 SLO，品質會回退

---

## 十二、總結與工程對策表

### 12.1 企業 RAG 的成熟度模型

綜合上述分析，我們可以為企業 RAG 的實施繪製一個四級成熟度模型：

**Level 1：Naive RAG**
- 架構：簡單的「檢索 -> 生成」
- 用途：內部原型、簡單演示
- 風險：立即在企業現實的所有指標上失敗

**Level 2：Advanced RAG**
- 架構：線性管道（重寫 -> 混合搜索 -> 重排 -> 生成）
- 用途：針對單一、可控數據源（如產品手冊）的生產級問答
- 風險：缺乏靈活性，無法處理多樣化的企業查詢

**Level 3：Agentic RAG（SOTA 2025）**
- 架構：由路由器（如 LangGraph）編排多個工具（RAG、函數調用、CRAG 迴圈）
- 用途：真正覆蓋全企業的「AI 助手」，能同時處理結構化和非結構化數據
- 基礎：堅實的資料治理和元數據策略

**Level 4：Specialized RAG**
- 架構：Level 3 的系統，並在代理的工具箱中增加了 GraphRAG
- 用途：處理 Level 3 無法解決的、高度複雜的關係型查詢

### 12.2 工程對策表

- **把 KB 當產品**：權威來源、版本、TTL、CDC、自動重嵌、權限標籤
- **把檢索當模型**：嵌入/索引/混檢/重排是可訓、可調、可 A/B 的「模型」
- **把上下文當提示工程**：RRF+MMR、壓縮去重、位置策略、引用對齊
- **該 Graph/Tree 就 Graph/Tree**：GraphRAG/RAPTOR 針對「長、泛、關聯」類問題是捷徑
- **量化一切**：Ragas/ARES + 線上指標閉環迭代

### 12.3 結論：2025 年的 SOTA 是一個被編排的系統

從第一性原理出發，對混亂企業知識庫的分析，在邏輯上必然導向一個複雜、有彈性且自適應的架構。

2025 年的 SOTA（State-of-the-Art）RAG 不再是單一的模型或技術。它是一個動態的、自我糾錯的、由代理編排的系統。該系統的設計哲學是：

1. 假設數據是有缺陷的（通過先進的預處理和元數據來解決）
2. 假設檢索會失敗（通過混合搜索、查詢轉換和 CRAG 式的糾錯來解決）
3. 假設 LLM 會出錯（通過 Reranking 解決「中間迷失」，並通過外部 Guardrails 來解決）
4. 假設任務是複雜的（通過代理路由、函數調用和 GraphRAG 來解決）

### 12.4 心法內化（小學生也能懂）

**「像找書一樣」**：先把圖書館（KB）整理好 → 找書的人（檢索+重排）要會找對書 → 把重點頁貼到筆記本（上下文） → 再寫作業（生成）還要標註頁碼（引用）。每一步都做對，作業就不會寫錯。

### 12.5 口訣記憶（3 點）

1. **先庫後檢，再排後寫**（KB→Retrieve→Rerank→Generate）
2. **短要準，長要分**（短問重排提準；長文先 Graph/Tree 再檢索）
3. **量化閉環**（Ragas/ARES 指標紅就回修：chunk、嵌入、索引、模板）

---

## 參考文獻（精選）

1. **Lost in the Middle**：長上下文位置偏置
2. **GraphRAG（Microsoft Research）**：圖檢索增強
3. **RAPTOR（ICLR 2024）**：樹式遞迴摘要檢索
4. **RAG 綜述**（RA-LLMs & 多篇 Survey）：體系化分類與評估方法
5. **嵌入與評測**（MTEB/E5/GTE/mGTE）：嵌入模型選型與長上下文擴展
6. **FAISS/HNSW**：向量檢索核心工具/索引
7. **Reranker**（Cohere/BGE）與雲端整合案例
8. **RAG 評測**（Ragas/ARES/Survey）
9. **CRAG**：糾錯檢索增強生成
10. **Self-RAG**：自我反思檢索增強生成
11. **LangGraph**：代理編排框架
12. **RAFT**：檢索增強微調

---

**文檔版本**：v1.0  
**最後更新**：2025-01  
**維護者**：RAG System Design Review Team

